[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "INTRODUCTION TO DIGITAL MUSICOLOGY",
    "section": "",
    "text": "Home\n\n\n\n\n\n\n\nWarning\n\n\n\nThese pages are work in progress and will be continuously updated.\n\n\nThis page contains material for the course Introduction to Digital Musicology, held at Julius-Maximilians-Universität, Würzburg (Germany) in Fall 2025.\nThe course takes place on Tuesdays from 10 to 12 AM (c.t.) in Room 107 (CIP-Pool), Domerschulstr. 13.\nEach week treats a different topic and consists of 45 minutes instruction plus 45 minutes hands-on excercises. Homework and reading material complement the lecture.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to refer to these pages, you can cite them as follows:\nMoss, F. C. (2025). Introduction to Digital Musicology. https://fabianmoss.github.io/intro-digimus",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "01_what_is_dm.html",
    "href": "01_what_is_dm.html",
    "title": "1  What is Digital Musicology?",
    "section": "",
    "text": "1.1 Introduction\nVirtual all aspects of our daily lives are increasingly shaped by data and algorithms. This ‘digital turn’ has also not stopped before academia and science, and most fields are more and more engaging with these new methodologies.\nDigital Musicology (DM) is a term that refers to music research that draws on digital data and digital methods to answer its research questions (Pugin, 2015). Stating a concise definition for the field is difficult, and there is no consensus to date. Moreover, a range of similar terms are frequently being used—for example, Computational Musicology—that have slightly different meanings. Faced with the fact that no common definition of these terms exist, Kris Shaffer attempted to describe some of the main aspects associated with Computational and Digital Musicology (Schaffer, 2016). He names the following subfields, which we will briefly discuss in turn (in an adapted order).",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Digital Musicology?</span>"
    ]
  },
  {
    "objectID": "01_what_is_dm.html#music-encoding",
    "href": "01_what_is_dm.html#music-encoding",
    "title": "1  What is Digital Musicology?",
    "section": "1.2 Music Encoding",
    "text": "1.2 Music Encoding\nAt the beginning of any digital approach to music stands the question how music is to be represented digitally. Music encoding in the narrower sense asks, how symbolic musical notations such as Common Western Music Notation (CWMN) can be translated into a computer-readable form. During this course, we will get to know a few fundamental symbolic music encoding formats, namely MIDI, ABC, and MEI. Other important formats, e.g. MusicXML or LilyPond, cannot be covered, unfortunately.\n\n\n\n\n\n\nNote\n\n\n\nI am also teaching a course on music encoding this semester: Digitale Musikkodierung und -edition.\n\n\n\n1.2.1 Corpus Studies\nAlthough corpus studies are considered a modern form of music theory, they can look back an an astonishingly long history. Many consider the first corpus study to be Jeppesen’s study The Style of Palestrina and the Dissonance, where he counts and tabulates occurrences of dissonant vertical intervals in the vocal work of the Renaissance composer (Jeppesen, 1927). Further early examples are the dissertations of Budge (1943) and Norman (1945).\nIn a nutshell, corpus attempt to find regularities in large collections of music data (Shanahan et al., 2022) in order to draw conclusions about the style of a particular composer, period, or genre, for instance. Corpus studies thus naturally make use of digital data and statistical or algorithmic methods for their analysis. Corpus studies often try to generalize music theoretical questions from the analysis of individual pieces to entire corpora. They often report their findings in statistical tables or graphical visualizations.\n\n\n\n\n\n\nNote\n\n\n\nIn the next summer term, I will teach a course on Music Corpus Studies.",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Digital Musicology?</span>"
    ]
  },
  {
    "objectID": "01_what_is_dm.html#music-information-retrieval",
    "href": "01_what_is_dm.html#music-information-retrieval",
    "title": "1  What is Digital Musicology?",
    "section": "1.3 Music Information Retrieval",
    "text": "1.3 Music Information Retrieval\nMusic information retrieval (MIR) is the largest part of DM, and research in this domain is directed both at academic as well as industrial contexts. MIR is closer to computer science, and some of its aspects can be understood as CS applied to music. The primary goal is not always a deeper understanding of music, but for instance improvements of recommendation or classification systems (Burgoyne et al., 2015). Recently, the wave of artificial intelligence (AI) as inspired a renewed interest in music generation, and human-computer co-creativity.",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Digital Musicology?</span>"
    ]
  },
  {
    "objectID": "01_what_is_dm.html#modeling",
    "href": "01_what_is_dm.html#modeling",
    "title": "1  What is Digital Musicology?",
    "section": "1.4 Modeling",
    "text": "1.4 Modeling\n“Modeling” in the scientific sense means to represent some aspect of reality in a simplified manner so that it can be easier studied and understood. If models are specified in such a way that machines can understand them, we speak of formal or computational models. Computational models can be used to perform tasks that are usually done by humans. For example, streaming services such as Spotify use computational models to categorize the songs on their platforms or to automatically create playlists. While models can achieve and even surpass human capabilities in some cases, this does not mean that computational models and humans use the same strategies or mechanisms to achieve certain goals.\nTherefore, computational modeling is also employed in fields such as music perception and cognition, where the focus does not lie on the optimal performance regarding some task, but in the understanding of how human cognition works. Often, however, computational modeling and music cognition go hand in hand because formal models are very convenient to express complex representations or behaviors.\nIn this sense, Computational Musicology is part of the larger movement of Computational Science, an endeavour that embeds computational methods and thinking into domain-specific areas of research (Wing, 2006). If we have this extended view, we also need to include computational models of music perception and cognition, acoustics, and enthnological and anthropological approaches to music as well. However, this would go beyond the scope of this course.\n\nTo sum up, what we understand by the term “Digital Musicology” depends on who you ask. What is common to the approaches above, however, is the development or use of digital methods as research tools.1 The more computational and modeling aspects of digital music research will be covered in other courses. In this course, we focus on the following aspects of Digital Musicology, each of which forms an integral part of its design:\n\nData about music: What is music metadata? How is music organized in different databases? Where is the boundary between music and data?\nMusic as data: How can music, an ephemeral expression of human culture, be captured or represented as digital data on a computer? What kinds of decisions do we have to make? Which musical aspects are retained and which are lost? Is music as data translatable?\nWorking with music data: What kinds of questions can we ask and attempt to answer when we have music data at hand? Which challenges to arise if we read instead of listening to music?\nCritical Digital Musicology: What kinds of ethical and societal issues do we touch upon when doing Digital Musicology? Which important questions do we need to consider before planning or executing a research study? How does Digital Musicology relate\nto other musicological sub-areas?\n\n\n\n\n\n\n\n\n\n\nOrganization notes\n\n\n\nExercises\nEach week concludes with a number of exercises that deepen or expand upon the topics discussed in the seminar. Doing the exercises is essential for a successful completion of the course.\nCredit\n\n50% Final exam\n30% Exercises\n20% Homework\n\n\n\nGo to the Exercise for Week 1\n\n\n\n\nBudge, H. (1943). A Study of Chord Frequencies Based on Music of Representative Composers of the Eighteenth and Nineteenth centuries [PhD thesis]. Columbia University.\n\n\nBurgoyne, J. A., Fujinaga, I., & Downie, J. S. (2015). Music Information Retrieval. In A New Companion to Digital Humanities (pp. 213–228). John Wiley & Sons, Ltd. https://doi.org/10.1002/9781118680605.ch15\n\n\nJeppesen, K. (1927). The Style of Palestrina and the Dissonance (1st ed.). Oxford University Press.\n\n\nNorman, P. B. (1945). A Quantitative Study of Harmonic Similarities in Certain Specified Works of Bach, Beethoven, and Wagner. C. Fischer, Incorporated.\n\n\nPugin, L. (2015). The Challenge of Data in Digital Musicology. Frontiers in Digital Humanities, 2, 1–3. https://doi.org/10.3389/fdigh.2015.00004\n\n\nSchaffer, K. (2016). What is computational musicology? https://medium.com/@krisshaffer/what-is-computational-musicology-f25ee0a65102\n\n\nShanahan, D., Burgoyne, J. A., & Quinn, I. (Eds.). (2022). Oxford Handbook of Music and Corpus Studies. Oxford University Press.\n\n\nWing, J. M. (2006). Computational thinking. Commun. ACM, 49(3), 33–35. https://doi.org/10.1145/1118178.1118215",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Digital Musicology?</span>"
    ]
  },
  {
    "objectID": "01_what_is_dm.html#footnotes",
    "href": "01_what_is_dm.html#footnotes",
    "title": "1  What is Digital Musicology?",
    "section": "",
    "text": "Word processors don’t count, of course.↩︎",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Digital Musicology?</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html",
    "href": "02_dm_today.html",
    "title": "2  Digital Musicology today",
    "section": "",
    "text": "2.1 Institutions",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#institutions",
    "href": "02_dm_today.html#institutions",
    "title": "2  Digital Musicology today",
    "section": "",
    "text": "Warning\n\n\n\nObviously, the following institutional lists are incomplete. Luckily for the field of DM, it has grown so extensively that a complete overview is not possible. The selection below is thus not an evaluation, but rather reflects the familiarity of the author with those institutions and the people there.\n\n\n\n2.1.1 Germany\n\nPaderborn/Detmold\nWürzburg\nErlangen\nMainz\nHalle\n\n\n\n2.1.2 Europe\n\nQueen Mary\nLinz\nBarcelona\nDCML\nAmsterdam\n\n\n\n2.1.3 USA\n\nStanford\nMIT\nGeorgia Tech",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#organizations",
    "href": "02_dm_today.html#organizations",
    "title": "2  Digital Musicology today",
    "section": "2.2 Organizations",
    "text": "2.2 Organizations\n\nISMIR\nIMS Digital Musicology Study group\nFachgruppe Digitale Musikwissenschaft\nMusic Encoding Initiative",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#current-research-topics",
    "href": "02_dm_today.html#current-research-topics",
    "title": "2  Digital Musicology today",
    "section": "2.3 Current research topics",
    "text": "2.3 Current research topics",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#central-journals-and-conferences",
    "href": "02_dm_today.html#central-journals-and-conferences",
    "title": "2  Digital Musicology today",
    "section": "2.4 Central journals and conferences",
    "text": "2.4 Central journals and conferences\n\n2.4.1 Journals\n\nEMR\nMusic Perception\nMusicae Scientiae\nMusic & Science\nMusic Theory Online\nMusic Theory Spectrum\n\n\n\n2.4.2 Conferences\n\nIMS Digital Musicology\nISMIR\nDLfM\nICMPC\nICCCM\nCMMR\nCHR\nSMC\nMEC\nDH Conference",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#important-tools",
    "href": "02_dm_today.html#important-tools",
    "title": "2  Digital Musicology today",
    "section": "2.5 Important tools",
    "text": "2.5 Important tools\n\nRISM Online\nmusic21\nVerovio\n\n\n**Go to the exercises for Week 2.\n\n\n\n\nInskip, C., & Wiering, F. (2015, October). In their own words: Using text analysis to identify musicologists’ attitudes towards technology. Proceedings of the 16th International Society for Music Information Retrieval Conference, Malaga, Spain. (2015).\n\n\nWiering, F., & Inskip, C. (2025). The impact of the pandemic on musicologists’ use of technology. Digital Humanities Quarterly, 019(2). https://dhq.digitalhumanities.org/vol/19/2/000786/000786.html",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "03_history_of_dm.html",
    "href": "03_history_of_dm.html",
    "title": "3  The history of Digital Musicology",
    "section": "",
    "text": "Goal\n\n\n\nKnowing the beginnings and the major stages of DM.",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The history of Digital Musicology</span>"
    ]
  },
  {
    "objectID": "04_RISM.html",
    "href": "04_RISM.html",
    "title": "4  RISM metadata",
    "section": "",
    "text": "Goal\n\n\n\nLearn what metadata are and how to search for music sources on RISM Online.\n\n\n\nWhat is RISM?\nWhat is RISM Online?\n\n\n\n\n\n\n\nExercise\n\n\n\nUnderstand basic SPARQL and design queries via prompting.",
    "crumbs": [
      "DATA ABOUT MUSIC",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>RISM metadata</span>"
    ]
  },
  {
    "objectID": "05_spotify_musicbrainz.html",
    "href": "05_spotify_musicbrainz.html",
    "title": "5  Spotify and MusicBrainz metadata",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand the kind of metadata provided by Spotify vs MusicBrainz.",
    "crumbs": [
      "DATA ABOUT MUSIC",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spotify and MusicBrainz metadata</span>"
    ]
  },
  {
    "objectID": "06_streaming_industry.html",
    "href": "06_streaming_industry.html",
    "title": "6  Music and the streaming industry",
    "section": "",
    "text": "Goal\n\n\n\nGain first insights into the music market and its workings.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWork with sales data.",
    "crumbs": [
      "DATA ABOUT MUSIC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Music and the streaming industry</span>"
    ]
  },
  {
    "objectID": "07_audio.html",
    "href": "07_audio.html",
    "title": "7  Audio",
    "section": "",
    "text": "7.1 pure tones\n\\[x(t) = A \\sin(2\\pi ft + \\phi)\\]",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "07_audio.html#pure-tones",
    "href": "07_audio.html#pure-tones",
    "title": "7  Audio",
    "section": "",
    "text": "amplidute \\(A\\)\nfrequency \\(f\\)\nlimits of hearing, Audible range and volume\nintervals: octave and fifth\nphase \\(\\phi\\)",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "07_audio.html#harmonics",
    "href": "07_audio.html#harmonics",
    "title": "7  Audio",
    "section": "7.2 Harmonics",
    "text": "7.2 Harmonics\nAdding pure tones and decomposition via Fourier (link to 3b1b)\n\nTimbre\nWaveform to spectrogram\nreading melodies from a spectrogram",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "07_audio.html#digital-audio",
    "href": "07_audio.html#digital-audio",
    "title": "7  Audio",
    "section": "7.3 Digital audio",
    "text": "7.3 Digital audio\n\nSampling\n\n\n\n\n\n\n\nFurther reading\n\n\n\nExcellent introductions can be found in Sethares (2005), Müller (2015), and Eerola (2025).\n\n\n\n\n\n\nEerola, T. (2025). Music and science: A guide to empirical music research. Routledge.\n\n\nMüller, M. (2015). Fundamentals of Music Processing: Audio, Analysis, Algorithms, Applications. Springer International Publishing. https://doi.org/10.1007/978-3-319-21945-5\n\n\nSethares, W. A. (2005). Tuning, Timbre, Spectrum, Scale (2nd ed.). Springer.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "08_MIDI.html",
    "href": "08_MIDI.html",
    "title": "8  MIDI",
    "section": "",
    "text": "Goal\n\n\n\nBe able to name use cases for MIDI. Translate MIDI numbers to pitches.\n\n\nWe have seen last week that the fundamental frequency of complex tones roughly corresponds to what we perceive as pitch. For many musical styles, pitch is one of the most fundamental components, as it is used to construct melodies and harmonies.\nGiven the (fundamental) frequency \\(f\\) of some tone, we can calculate its corresponding MIDI pitch number by making use of a reference note. This is usually A4 in Scientific Pitch Notation, and with a frequency of 440 Hz.\nSo the frequency of that A4 is given by \\[f = 440 \\times 2^{(m-69)/12}\\]\nIn Python, we can write a simple calculation to do this for any given MIDI number:\n\ndef MIDI2frequency(m: int, K: float = 440.0) -&gt; int:\n    f = K * 2 ** ((m - 69) / 12)\n    return f\n\nIf we wanted to know the frequency of the note C5 (MIDI pitch 73), we could simply write:\n\nf_C5 = MIDI2frequency(73)\nprint(f_C5)\n\n554.3652619537442\n\n\nWith the invention of electronic instruments in the 20th century, and the possibility of representing continuous sounds as digital signals, it became important for those instruments to interact and communicate. This led to the development of the Musical Instruments Digital Interface (MIDI) format that is still a cornerstone for music information exchange.\nMIDI is relatively simple: it consists of so-called messages that have five components:\n\nTime (in ticks)\nMessage (either NOTE ON or NOTE OFF)\nChannel (up to 16 for classical MIDI)\nNote number (integers between 0 and 127)\nVelocity (simulating the speed of pressing down a key, proxy for volume)\n\nIn that way, MIDI resembles the much older ‘piano roll’ representation for musical notes.\nADD PIANO ROLL IMAGE HERE\nExercise:\n\nTranscribe simple melody to MIDI and plot using pypianoroll.\nDownload multitrack MIDI from the internet and visualize.\nDescribe what can be read from a pianoroll visualization, and what is missing.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MIDI</span>"
    ]
  },
  {
    "objectID": "09_ABC.html",
    "href": "09_ABC.html",
    "title": "9  ABC",
    "section": "",
    "text": "9.1 Pitches",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ABC</span>"
    ]
  },
  {
    "objectID": "09_ABC.html#pitches",
    "href": "09_ABC.html#pitches",
    "title": "9  ABC",
    "section": "",
    "text": "Pitches in the middle octave (C4 – B4) are notated by uppercase letters: C D E F G A B.\nThe octave above is notated by lowercase letters. Together the two octaves from C4 to B5 are: C D E F G A B c d e f g a b.\nThe next upper octave gets an apostrophe: C D E F G A B c d e f g a b c’ d’ e’ f’ g’ a’ b’\nAnd the lower octaves are indicated by commas (a bit like inverted apostrophes). C, D, E, F, G, A, B, C D E F G A B c d e f g a b c’ d’ e’ f’ g’ a’ b’.\nExtending further up or down works by adding commas and apostrophes.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ABC</span>"
    ]
  },
  {
    "objectID": "09_ABC.html#durations",
    "href": "09_ABC.html#durations",
    "title": "9  ABC",
    "section": "9.2 Durations",
    "text": "9.2 Durations\nABC notation consists of two parts: a header with metadata and the body with the melody. We can set the default note duration in the header like so:\nL:1/8\nThis means that the default duration is an eighth note. Other default note duration values can be set analogously. Changing note durations during the course of a melody can be done by multiplying or dividing the default value:\nC/2 C2\nLet’s look at an example. To encode the following melody in the Figure below, we would write:\nX:1\nT:Speed The Plough\nM:4/4\nL:1/8\nN:Simple version\nZ:Steve Mansfield 1/2/2000\nK:G\nGABc dedB | dedB dedB | c2ec B2dB | A2A2 A2 BA|\nGABc dedB | dedB dedB | c2ec B2dB | A2A2 G4 ::\ng2g2 g4 | g2fe dBGB | c2ec B2dB | A2A2 A4 |\ng2g2 g4 | g2fe dBGB | c2ec B2dB | A2A2 G4 :|\nMelodies encoded in ABC notation can be easily rendered visually using the abcjs website.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ABC</span>"
    ]
  },
  {
    "objectID": "09_ABC.html#footnotes",
    "href": "09_ABC.html#footnotes",
    "title": "9  ABC",
    "section": "",
    "text": "https://abcnotation.com↩︎\nThe following is a summary of the tutorial by Steve Mansfield.↩︎",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ABC</span>"
    ]
  },
  {
    "objectID": "10_MEI.html",
    "href": "10_MEI.html",
    "title": "10  MEI",
    "section": "",
    "text": "10.1 XML\nFor a computer, text is merely a string of characters, including ‘invisible’ characters such as spaces between words and line breaks. But texts are usually more structured than that. For instance, they can contain headings, sections, subsections or paragraphs; they can be printed onto different pages, and they can change their appearance by including boldfaced or italicized text, for example. If we want to encode these structural aspects of texts in machine-readable formats, they need to be explicitly marked up.\nThe most popular mark-up language for all sorts of texts is the eXtensible Markup Language (XML). XML documents consist of a so-called XML declaration that tells the computer which structuring elements are available and how to interpret them, as well as several tags and their attributes. This is easily explained with an example:\nThis example contains two types of elements: message and greeting. Elements work a little bit like parentheses ((, )) or brackets ([, ]) because there is (almost) always an opening tag and a closing tag. Closing tags contain a slash (/).\nThe example also shows that tags can be hierarchically nested. If the message consisted of several parts, it could be encoded as follows:\nWe say that greeting and politeness are children of message (they are siblings), and greeting is the parent element.\nElements can contain so-called attributes that modify the element. Attributes are listed only in the opening tag of the element. In the example above, the greeting element contained the attribute @emotion with the value \"joyful\". It is customary to add an @ when talking about an attribute. Attribute values need to be enclosed in quotation marks (\"), see also attributes @version and @encoding of the XML declaration in the first code line.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>MEI</span>"
    ]
  },
  {
    "objectID": "10_MEI.html#xml",
    "href": "10_MEI.html#xml",
    "title": "10  MEI",
    "section": "",
    "text": "&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;message&gt;\n  &lt;greeting&gt;\n    Hello world!\n  &lt;/greeting&gt;\n&lt;/message&gt;\n\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;message&gt;\n  &lt;greeting emotion=\"joyful\"&gt;\n    Hello world!\n  &lt;/greeting&gt;\n  &lt;politeness&gt;\n    Sincerely,\n  &lt;/politeness&gt;\n  &lt;!-- This is a comment that the computer will ignore --&gt;\n&lt;/message&gt;",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>MEI</span>"
    ]
  },
  {
    "objectID": "10_MEI.html#header-and-body",
    "href": "10_MEI.html#header-and-body",
    "title": "10  MEI",
    "section": "10.2 Header and body",
    "text": "10.2 Header and body\nThe basic structure of an MEI document conists of two major parts, the head and the music, roughly corresponding to metadata describing the source and the music ‘itself’, respectively.\n&lt;mei meiversion=\"5.0\"&gt;\n  &lt;meiHead&gt;\n  &lt;!-- metadata goes here --&gt;\n  &lt;/meiHead&gt;\n  &lt;music&gt;\n  &lt;!-- description of musical text goes here --&gt;\n  &lt;/music&gt;\n&lt;/mei&gt;",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>MEI</span>"
    ]
  },
  {
    "objectID": "10_MEI.html#a-minimal-example",
    "href": "10_MEI.html#a-minimal-example",
    "title": "10  MEI",
    "section": "10.3 A minimal example",
    "text": "10.3 A minimal example",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>MEI</span>"
    ]
  },
  {
    "objectID": "10_MEI.html#mei-for-digital-scholarly-editions",
    "href": "10_MEI.html#mei-for-digital-scholarly-editions",
    "title": "10  MEI",
    "section": "10.4 MEI for digital scholarly editions",
    "text": "10.4 MEI for digital scholarly editions\nOne of the main reasons for the development of MEI was that existing music encoding standards did not accomodate all needs of music scholars engaged in editing music. Apart from merely representing what is written or printed in a musical score, musicologists understand that musical texts always stand in certain contexts, and that musical sources need commentaries, corrections, and critical treatment more generally. In short, while CWMN is easily represented by other encoding formats such as MusicXML and LilyPond, MEI is the prime candidate for musicological applications due to this critical layer.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>MEI</span>"
    ]
  },
  {
    "objectID": "10_MEI.html#community",
    "href": "10_MEI.html#community",
    "title": "10  MEI",
    "section": "10.5 Community",
    "text": "10.5 Community\nBesides being a digital format for music encoding, MEI is at the same time a community of scholars and practicioners. In fact, the community aspect is very strong, and people very regularly exchange ideas and issues on their Slack communication channel as well in the annual Music Encoding Conference (MEC).\n\nMuseScore export\nmei friend",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>MEI</span>"
    ]
  },
  {
    "objectID": "billboard_charts.html",
    "href": "billboard_charts.html",
    "title": "11  Analyzing song survival",
    "section": "",
    "text": "In this session, we will analyze songs from the Billboard 100 charts and trace their ‘course of life’ in the charts.\nThe data was obtained from Kaggle, a large community website for data analysis challenges.\nAs before, we first import the pandas library for data analysis and load the data using the read_csv fundtion that takes as its main argument the path to the data file, in our case charts.csv.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"data/charts.csv\")\n\nInspecting the first 5 lines with the .head() method of pandas DataFrames, we obtain an understanding of the structure of the data.\n\ndf.head()\n\n\n\n\n\n\n\n\n\ndate\nrank\nsong\nartist\nlast-week\npeak-rank\nweeks-on-board\n\n\n\n\n0\n2021-11-06\n1\nEasy On Me\nAdele\n1.0\n1\n3\n\n\n1\n2021-11-06\n2\nStay\nThe Kid LAROI & Justin Bieber\n2.0\n1\n16\n\n\n2\n2021-11-06\n3\nIndustry Baby\nLil Nas X & Jack Harlow\n3.0\n1\n14\n\n\n3\n2021-11-06\n4\nFancy Like\nWalker Hayes\n4.0\n3\n19\n\n\n4\n2021-11-06\n5\nBad Habits\nEd Sheeran\n5.0\n2\n18\n\n\n\n\n\n\n\n\nThink: What do the columns represent? Provide verbal descriptions of their meaning and write it down.\nAfter this general overview, we might want to achieve a slightly deeper understanding. For instance, it is not difficult to interpret the date column, but from only the first few entries, we cannot know the temporal extend of our data.\nLet’s find out what the earliest and latest dates are using the .min() and .max() methods, respectively.\n\ndf[\"date\"].min(), df[\"date\"].max()\n\n('1958-08-04', '2021-11-06')\n\n\nThis tells us that the data stored in charts.csv runs from August 1958 to November 2021 and thus allows us to trace the movement of songs in the Billboard charts across more than 60 years.\n\n# Top artists\ndf.artist.value_counts()\n\nartist\nTaylor Swift                     1023\nElton John                        889\nMadonna                           857\nDrake                             787\nKenny Chesney                     769\n                                 ... \nGunna Featuring Playboi Carti       1\nLil Pump Featuring Lil Wayne        1\nBilly Lemmons                       1\nJohnny Caswell                      1\nTammy Montgomery                    1\nName: count, Length: 10205, dtype: int64\n\n\n\n# Longest in charts\ndf.sort_values(by=\"weeks-on-board\", ascending=True).iloc[50_000:]\n\n\n\n\n\n\n\n\n\ndate\nrank\nsong\nartist\nlast-week\npeak-rank\nweeks-on-board\n\n\n\n\n209088\n1981-10-17\n89\nBack To The 60's\nTight Fit\n89.0\n89\n2\n\n\n209064\n1981-10-17\n65\nNever Too Much\nLuther Vandross\n85.0\n65\n2\n\n\n209069\n1981-10-17\n70\nWhen She Dances\nJoey Scarbury\n80.0\n70\n2\n\n\n209072\n1981-10-17\n73\nWired For Sound\nCliff Richard\n83.0\n73\n2\n\n\n209243\n1981-10-03\n44\nEvery Little Thing She Does Is Magic\nThe Police\n66.0\n44\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n39148\n2014-05-10\n49\nRadioactive\nImagine Dragons\n48.0\n3\n87\n\n\n1215\n2021-08-14\n16\nBlinding Lights\nThe Weeknd\n17.0\n1\n87\n\n\n1117\n2021-08-21\n18\nBlinding Lights\nThe Weeknd\n16.0\n1\n88\n\n\n1020\n2021-08-28\n21\nBlinding Lights\nThe Weeknd\n18.0\n1\n89\n\n\n919\n2021-09-04\n20\nBlinding Lights\nThe Weeknd\n21.0\n1\n90\n\n\n\n\n280087 rows × 7 columns\n\n\n\n\n\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n\ndf[df.artist==\"Drake\"].song.value_counts()\n\nsong\nGod's Plan             36\nHotline Bling          36\nControlla              26\nNice For What          25\nFake Love              25\n                       ..\nToo Much                1\nCome Thru               1\nHate Sleeping Alone     1\nShot For Me             1\n9 AM In Dallas          1\nName: count, Length: 108, dtype: int64\n\n\n\ndf[df.artist==\"Elton John\"].song.value_counts()\n\nsong\nCandle In The Wind 1997/Something About The Way You Look Tonight               42\nCan You Feel The Love Tonight (From \"The Lion King\")                           26\nI Guess That's Why They Call It The Blues                                      23\nThe One                                                                        22\nCandle In The Wind                                                             21\nLittle Jeannie                                                                 21\nThe Last Song                                                                  20\nRecover Your Soul                                                              20\nBelieve                                                                        20\nCircle Of Life (From \"The Lion King\")                                          20\nBlessed                                                                        20\nSad Songs (say So Much)                                                        19\nI Don't Wanna Go On With You Like That                                         18\nNikita                                                                         18\nBennie And The Jets                                                            18\nMama Can't Buy You Love                                                        18\nBlue Eyes                                                                      18\nYou Can Make History (Young Again)                                             17\nSacrifice                                                                      17\nEmpty Garden (Hey Hey Johnny)                                                  17\nCrocodile Rock                                                                 17\nGoodbye Yellow Brick Road                                                      17\nI'm Still Standing                                                             16\nClub At The End Of The Street                                                  16\nSimple Life                                                                    16\nSomeday Out Of The Blue                                                        15\nDon't Let The Sun Go Down On Me                                                15\nIsland Girl                                                                    15\nDaniel                                                                         15\nRocket Man                                                                     15\nHealing Hands                                                                  15\nThe Bitch Is Back                                                              14\nWho Wears These Shoes?                                                         14\nWrap Her Up                                                                    14\nSorry Seems To Be The Hardest Word                                             14\nLucy In The Sky With Diamonds                                                  14\nYour Song                                                                      14\nNobody Wins                                                                    13\nA Word In Spanish                                                              13\nSomeone Saved My Life Tonight                                                  13\nYou Gotta Love Someone                                                         13\nIn Neon                                                                        13\nChloe                                                                          13\n(Sartorial Eloquence) Don't Ya Wanna Play This Game No More?                   12\nKiss The Bride                                                                 12\nSaturday Night's Alright For Fighting                                          12\nGrow Some Funk Of Your Own/I Feel Like A Bullet (In The Gun Of Robert Ford)    11\nMade In England                                                                10\nLevon                                                                          10\nVictim Of Love                                                                 10\nPart-Time Love                                                                 10\nHonky Cat                                                                      10\nFriends                                                                         9\nHeartache All Over The World                                                    8\nEgo                                                                             8\nTiny Dancer                                                                     7\nBite Your Lip (Get up and dance!)                                               6\nBorder Song                                                                     5\nName: count, dtype: int64\n\n\n\ndef chart_performance(artist, song):\n    data = df[(df[\"artist\"] == artist) & (df[\"song\"] == song)]\n    data = data.sort_values(by=\"date\").reset_index(drop=True)\n    data[\"date_rel\"] = pd.to_timedelta(data[\"date\"] - data[\"date\"][0]).dt.days\n    return data\n\n\ntest_cases = {\n    \"Taylor Swift\": \"You Belong With Me\",\n    \"Drake\": \"God's Plan\",\n    \"Elton John\": \"Candle In The Wind 1997/Something About The Way You Look Tonight\",\n    \"The Weeknd\": \"Blinding Lights\",\n    \"Elvis Presley\": \"Please Don't Stop Loving Me\"\n}\n\n\ntaylor = chart_performance(\"Taylor Swift\", \"You Belong With Me\")\ndrake = chart_performance(\"Drake\", \"God's Plan\")\nelton = chart_performance(\"Elton John\", \"Candle In The Wind 1997/Something About The Way You Look Tonight\")\nweeknd = chart_performance(\"The Weeknd\", \"Blinding Lights\")\nelvis = chart_performance(\"Elvis Presley\", \"Please Don't Stop Loving Me\")\n\n\n_, ax = plt.subplots(figsize=(15,4))\n\nfor artist, song in test_cases.items():\n    data = chart_performance(artist, song)\n    x = data[\"date\"].values\n    y = data[\"rank\"].values\n\n    ax.plot(x, y, marker=\".\", label=f\"{song} ({artist})\")\n\nplt.gca().invert_yaxis()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n_, ax = plt.subplots(figsize=(15,4))\n\nfor artist, song in test_cases.items():\n    data = chart_performance(artist, song)\n    x = data[\"date_rel\"].values\n    y = data[\"rank\"].values\n\n    ax.plot(x, y, marker=\".\", label=f\"{song} ({artist})\")\n\nplt.gca().invert_yaxis()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# TODO: Add case study on Mariah Carey's \"Fantasy\", \n# 1st female to start on 1 in charts (first ever: Michael Jackson, \n# You Are Not Alone)\n# Fantasy remained 8 weeks on no. 1, while YANA only one\n# see: https://www.forbes.com/sites/hughmcintyre/2025/09/30/how-mariah-carey-followed-michael-jackson-and-made-chart-history/\n\n\n# TODO: remove lines for missing weeks (gaps in curves)\n# add two cases:\n#  - short duration but high peak\n#  - long duration but low peak\n\n\n# Q: can we predict a song's survival using the features given in the data?\n# --&gt; at least introduce notion of training/test data and discuss the epistemological problem of using 'all' historical \n# sources for explanation\n\n\n# Try other data: https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n\n\ndf_charts = pd.read_csv(\"data/Hot Stuff.csv\", index_col=0)\ndf_charts[\"WeekID\"] = pd.to_datetime(df_charts[\"WeekID\"])\n\n\ndf_charts.head()\n\n\n\n\n\n\n\n\n\nurl\nWeekID\nWeek Position\nSong\nPerformer\nSongID\nInstance\nPrevious Week Position\nPeak Position\nWeeks on Chart\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-07-17\n34\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n45.0\n34\n4\n\n\n1\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-07-24\n22\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n34.0\n22\n5\n\n\n2\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-07-31\n14\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n22.0\n14\n6\n\n\n3\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-08-07\n10\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n14.0\n10\n7\n\n\n4\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-08-14\n8\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n10.0\n8\n8\n\n\n\n\n\n\n\n\n\ndf_audio = pd.read_csv(\"data/Hot 100 Audio Features.csv\", index_col=0)\n\n\ndf_audio.head()\n\n\n\n\n\n\n\n\n\nSongID\nPerformer\nSong\nspotify_genre\nspotify_track_id\nspotify_track_preview_url\nspotify_track_duration_ms\nspotify_track_explicit\nspotify_track_album\ndanceability\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nspotify_track_popularity\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n-twistin'-White Silver SandsBill Black's Combo\nBill Black's Combo\n-twistin'-White Silver Sands\n[]\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n¿Dònde Està Santa Claus? (Where Is Santa Claus...\nAugie Rios\n¿Dònde Està Santa Claus? (Where Is Santa Claus?)\n['novelty']\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n......And Roses And RosesAndy Williams\nAndy Williams\n......And Roses And Roses\n['adult standards', 'brill building pop', 'eas...\n3tvqPPpXyIgKrm4PR9HCf0\nhttps://p.scdn.co/mp3-preview/cef4883cfd1e0e53...\n166106.0\nFalse\nThe Essential Andy Williams\n0.154\n...\n-14.063\n1.0\n0.0315\n0.91100\n0.000267\n0.112\n0.150\n83.969\n4.0\n38.0\n\n\n3\n...And Then There Were DrumsSandy Nelson\nSandy Nelson\n...And Then There Were Drums\n['rock-and-roll', 'space age pop', 'surf music']\n1fHHq3qHU8wpRKHzhojZ4a\nNaN\n172066.0\nFalse\nCompelling Percussion\n0.588\n...\n-17.278\n0.0\n0.0361\n0.00256\n0.745000\n0.145\n0.801\n121.962\n4.0\n11.0\n\n\n4\n...Baby One More TimeBritney Spears\nBritney Spears\n...Baby One More Time\n['dance pop', 'pop', 'post-teen pop']\n3MjUtNVVq3C8Fn0MP3zhXa\nhttps://p.scdn.co/mp3-preview/da2134a161f1cb34...\n211066.0\nFalse\n...Baby One More Time (Digital Deluxe Version)\n0.759\n...\n-5.745\n0.0\n0.0307\n0.20200\n0.000131\n0.443\n0.907\n92.960\n4.0\n77.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nd = df_charts.merge(df_audio)\n\n\nd.shape\n\n(330208, 29)\n\n\n\nd[\"WeekID\"] = pd.to_datetime(d[\"WeekID\"])\n\n\nd.sample(10)\n\n\n\n\n\n\n\n\n\nurl\nWeekID\nWeek Position\nSong\nPerformer\nSongID\nInstance\nPrevious Week Position\nPeak Position\nWeeks on Chart\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nspotify_track_popularity\n\n\n\n\n80076\nhttp://www.billboard.com/charts/hot-100/2017-0...\n2017-07-08\n20\nBad Liar\nSelena Gomez\nBad LiarSelena Gomez\n1\n23.0\n20\n6\n...\n-6.408\n1.0\n0.0713\n0.1890\n0.00000\n0.0767\n0.728\n121.032\n4.0\n75.0\n\n\n296574\nhttp://www.billboard.com/charts/hot-100/1963-0...\n1963-07-06\n11\nYou Can't Sit Down\nThe Dovells\nYou Can't Sit DownThe Dovells\n1\n8.0\n3\n11\n...\n-5.596\n1.0\n0.2390\n0.1190\n0.00000\n0.3190\n0.966\n164.777\n4.0\n25.0\n\n\n191090\nhttp://www.billboard.com/charts/hot-100/2007-1...\n2007-12-29\n85\nNothin' Better To Do\nLeAnn Rimes\nNothin' Better To DoLeAnn Rimes\n1\n81.0\n73\n12\n...\n-3.668\n0.0\n0.1430\n0.2540\n0.00000\n0.1700\n0.455\n198.075\n4.0\n36.0\n\n\n161426\nhttps://www.billboard.com/charts/hot-100/2019-...\n2019-07-27\n78\nRaised On Country\nChris Young\nRaised On CountryChris Young\n1\n80.0\n78\n7\n...\n-3.224\n1.0\n0.0455\n0.3950\n0.00000\n0.2300\n0.790\n148.095\n4.0\n67.0\n\n\n6906\nhttp://www.billboard.com/charts/hot-100/1989-1...\n1989-11-04\n49\nTalk To Myself\nChristopher Williams\nTalk To MyselfChristopher Williams\n1\n49.0\n49\n9\n...\n-9.256\n1.0\n0.0572\n0.0539\n0.00000\n0.1410\n0.815\n112.777\n4.0\n16.0\n\n\n297464\nhttp://www.billboard.com/charts/hot-100/1987-0...\n1987-02-28\n5\nYou Got It All\nThe Jets\nYou Got It AllThe Jets\n1\n6.0\n5\n16\n...\n-11.184\n0.0\n0.0287\n0.2550\n0.00000\n0.0999\n0.625\n166.904\n4.0\n40.0\n\n\n223258\nhttp://www.billboard.com/charts/hot-100/1960-0...\n1960-04-02\n39\nEl Matador\nThe Kingston Trio\nEl MatadorThe Kingston Trio\n1\n36.0\n32\n7\n...\n-14.485\n0.0\n0.0274\n0.7870\n0.00115\n0.3230\n0.420\n81.931\n4.0\n16.0\n\n\n286408\nhttp://www.billboard.com/charts/hot-100/2016-0...\n2016-05-28\n87\nMoolah\nYoung Greatness\nMoolahYoung Greatness\n3\nNaN\n87\n5\n...\n-5.732\n1.0\n0.2410\n0.0264\n0.00000\n0.3020\n0.230\n62.912\n4.0\n56.0\n\n\n88718\nhttp://www.billboard.com/charts/hot-100/1983-0...\n1983-07-23\n58\nTry Again\nChampaign\nTry AgainChampaign\n1\n47.0\n23\n17\n...\n-11.477\n1.0\n0.0417\n0.5890\n0.00001\n0.1460\n0.712\n128.214\n4.0\n37.0\n\n\n70300\nhttps://www.billboard.com/charts/hot-100/2020-...\n2020-07-25\n14\nTitanic\nJuice WRLD\nTitanicJuice WRLD\n1\nNaN\n14\n1\n...\n-6.668\n0.0\n0.0448\n0.1680\n0.00000\n0.1120\n0.234\n174.146\n4.0\n73.0\n\n\n\n\n10 rows × 29 columns\n\n\n\n\n\n## BOOTSTRAP!\n\n# d = d.sample(500_000, replace=True)\n\n\nd.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 330208 entries, 0 to 330207\nData columns (total 29 columns):\n #   Column                     Non-Null Count   Dtype         \n---  ------                     --------------   -----         \n 0   url                        330208 non-null  object        \n 1   WeekID                     330208 non-null  datetime64[ns]\n 2   Week Position              330208 non-null  int64         \n 3   Song                       330208 non-null  object        \n 4   Performer                  330208 non-null  object        \n 5   SongID                     330208 non-null  object        \n 6   Instance                   330208 non-null  int64         \n 7   Previous Week Position     298048 non-null  float64       \n 8   Peak Position              330208 non-null  int64         \n 9   Weeks on Chart             330208 non-null  int64         \n 10  spotify_genre              315700 non-null  object        \n 11  spotify_track_id           287066 non-null  object        \n 12  spotify_track_preview_url  169915 non-null  object        \n 13  spotify_track_duration_ms  287066 non-null  float64       \n 14  spotify_track_explicit     287066 non-null  object        \n 15  spotify_track_album        287004 non-null  object        \n 16  danceability               286508 non-null  float64       \n 17  energy                     286508 non-null  float64       \n 18  key                        286508 non-null  float64       \n 19  loudness                   286508 non-null  float64       \n 20  mode                       286508 non-null  float64       \n 21  speechiness                286508 non-null  float64       \n 22  acousticness               286508 non-null  float64       \n 23  instrumentalness           286508 non-null  float64       \n 24  liveness                   286508 non-null  float64       \n 25  valence                    286508 non-null  float64       \n 26  tempo                      286508 non-null  float64       \n 27  time_signature             286508 non-null  float64       \n 28  spotify_track_popularity   287066 non-null  float64       \ndtypes: datetime64[ns](1), float64(15), int64(4), object(9)\nmemory usage: 73.1+ MB\n\n\n\nfrom IPython.display import Audio, HTML\n\n\nAudio(url=d.loc[1000,\"spotify_track_preview_url\"])\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef curves(performer, song):\n    data = d[(d.Performer == performer) & (d.Song == song)].sort_values(by=\"WeekID\").reset_index(drop=True)\n    data[\"date_rel\"] = pd.to_timedelta(data[\"WeekID\"] - data[\"WeekID\"][0]).dt.days\n    x = data[\"date_rel\"].values # or date_rel or WeekID\n    y = data[\"Week Position\"].values\n    return x,y\n\n\ntest_cases2 = {\n    \"Patty Duke\": \"Don't Just Stand There\",\n    \"Ace Of Base\": \"Don't Turn Around\",\n    \"Dan + Shay\": \"Speechless\",\n    \"YoungBloodZ Featuring Lil Jon\": \"Damn!\",\n    \"K-Ci & JoJo\": \"All My Life\",\n    \"Trevor Daniel\": \"Falling\"\n}\n\n\n_, ax = plt.subplots(figsize=(15,4))\n\nfor performer, song in test_cases2.items():\n    x,y = curves(performer, song)\n    ax.plot(x, y, marker=\".\", label=f\"{song} ({performer})\")\n\nplt.gca().invert_yaxis()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nModeling the life of a song in the Top 100:\nWe assume that once a song has left the Top 100, it is impossible to re-enter (even though that does happen, of course)\n\nEach song has a starting rank \\(r_0\\).\nFor each following week, there is a bernoulli dropout probability \\(\\theta\\) that determines whether a song remains in the charts.\n\n\n\n# O\n\n\nentrances = []\npeaks = []\nexits = []\n\nfor _, group in d.groupby(\"SongID\"):\n    weeks = group.sort_values(by=\"WeekID\")[\"Week Position\"].values\n    entrances.append(weeks[0])\n    peaks.append(weeks.min())\n    exits.append(weeks[-1])\n\n\nimport numpy as np\n\n\n# from matplotlib.collections import LineCollection\n\n\n_, ax = plt.subplots(figsize=(10,4))\n\nK = len(entrances) + 1\n\nfor a, b, c in zip(entrances[:K], peaks[:K], exits[:K]):\n    if a != b != c: # remove constants\n        ax.plot([0, 1, 2], [a, b, c], c=\"k\", lw=.5, alpha=.01)\n\nplt.xlim(0,2)\nplt.ylim(0,100)\nplt.gca().invert_yaxis() # smaller is better\nplt.savefig(\"img/rise-decline.png\", dpi=600)\nplt.show()\n\n\n\n\n\n\n\n\nOBSERVATION: At least 3 types:\n\nconstants\nlow in, peak, low out\nlow in, peak, mid out\n\nTry to disentangle what causes the difference",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analyzing song survival</span>"
    ]
  },
  {
    "objectID": "11_dma_harmony.html",
    "href": "11_dma_harmony.html",
    "title": "12  Digital music analysis: harmony",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand what labeling is and why labels can be useful.\n\n\n\nfurther MuseScore practice\nsegmentation and labeling\nCounting chords, finding cadences",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Digital music analysis: harmony</span>"
    ]
  },
  {
    "objectID": "12_dma_melody.html",
    "href": "12_dma_melody.html",
    "title": "13  Digital music analysis: melody",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand how melodic pattern matching works in principle.\n\n\n\nPattern finding in melodies (Non-Western)\n\nGo to the Exercise for Week X",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Digital music analysis: melody</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html",
    "href": "04_jazz_solos.html",
    "title": "14  Solos in the Weimar Jazz Database",
    "section": "",
    "text": "14.1 Melodic arc?\nDoes the melodic arc also appear in the Jazz solos?\ndef notelist(melid):\n    \n    solo = solos[solos[\"melid\"] == melid]\n    \n    solo = solo[[\"pitch\", \"duration\"]]\n    solo[\"onset\"] = solo[\"duration\"].cumsum()\n    return solo\nnotelist(1)\n\n\n\n\n\n\n\n\n\npitch\nduration\nonset\n\n\n\n\n0\n65.0\n0.138776\n0.138776\n\n\n1\n63.0\n0.171247\n0.310023\n\n\n2\n58.0\n0.081270\n0.391293\n\n\n3\n61.0\n0.235102\n0.626395\n\n\n4\n63.0\n0.130612\n0.757007\n\n\n...\n...\n...\n...\n\n\n525\n66.0\n0.137143\n80.645238\n\n\n526\n65.0\n0.101587\n80.746825\n\n\n527\n63.0\n0.104490\n80.851315\n\n\n528\n62.0\n0.110295\n80.961610\n\n\n529\n70.0\n0.187211\n81.148821\n\n\n\n\n530 rows × 3 columns\ndef plot_melodic_profile(notelist, ax=None, c=None, mean=False, Z=False, sections=False, standardized=False):\n    \n    if ax == None:\n        ax = plt.gca()\n    \n    if standardized:\n        x = notelist[\"Rel. Onset\"]\n        y = notelist[\"Rel. MIDI Pitch\"]\n    else:\n        x = notelist[\"onset\"]\n        y = notelist[\"pitch\"]\n    \n    ax.step(x,y, color=c)\n    \n    if mean:\n        ax.axhline(y.mean(), color=\"gray\", linestyle=\"--\")\n        \n    if sections:\n        for l in [ x.max() * i for i in [ 1/4, 1/2, 3/4] ]:\n            ax.axvline(l, color=\"gray\", linewidth=1, linestyle=\"--\")\nfig, axes = plt.subplots(2,2, figsize=(20,9))\naxes = axes.flatten()\n\nplot_melodic_profile(notelist(1), ax=axes[0], mean=True)\nplot_melodic_profile(notelist(77), ax=axes[1], mean=True)\nplot_melodic_profile(notelist(50), ax=axes[2], mean=True)\nplot_melodic_profile(notelist(233), ax=axes[3], mean=True)\ndef standardize(notelist):\n    \"\"\"\n    Takes a notelist as input and returns a standardized version.\n    \"\"\"\n    \n    notelist[\"Rel. MIDI Pitch\"] = (notelist[\"pitch\"] - notelist[\"pitch\"].mean()) / notelist[\"pitch\"].std()\n    notelist[\"Rel. Duration\"] = notelist[\"duration\"] / notelist[\"duration\"].sum()\n    notelist[\"Rel. Onset\"] = notelist[\"onset\"] / notelist[\"onset\"].max()\n    \n    return notelist\nstandardize(notelist(1))\n\n\n\n\n\n\n\n\n\npitch\nduration\nonset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\n\n\n\n\n0\n65.0\n0.138776\n0.138776\n-0.460594\n0.001710\n0.001710\n\n\n1\n63.0\n0.171247\n0.310023\n-0.697714\n0.002110\n0.003820\n\n\n2\n58.0\n0.081270\n0.391293\n-1.290513\n0.001001\n0.004822\n\n\n3\n61.0\n0.235102\n0.626395\n-0.934833\n0.002897\n0.007719\n\n\n4\n63.0\n0.130612\n0.757007\n-0.697714\n0.001610\n0.009329\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n525\n66.0\n0.137143\n80.645238\n-0.342034\n0.001690\n0.993794\n\n\n526\n65.0\n0.101587\n80.746825\n-0.460594\n0.001252\n0.995046\n\n\n527\n63.0\n0.104490\n80.851315\n-0.697714\n0.001288\n0.996334\n\n\n528\n62.0\n0.110295\n80.961610\n-0.816274\n0.001359\n0.997693\n\n\n529\n70.0\n0.187211\n81.148821\n0.132205\n0.002307\n1.000000\n\n\n\n\n530 rows × 6 columns\nfig, ax = plt.subplots(figsize=(20,5))\n\nfor i in range(4):\n    plot_melodic_profile(standardize(notelist(i)), \n                         mean=True, \n                         standardized=True)\nplt.xlim(0,1)\nplt.show()\nbig_df = pd.concat([standardize(notelist(i)) for i in range(solos_meta.shape[0])])\nsolos\n\n\n\n\n\n\n\n\n\neventid\nmelid\nonset\npitch\nduration\nperiod\ndivision\nbar\nbeat\ntatum\n...\nloud_max\nloud_med\nloud_sd\nloud_relpos\nloud_cent\nloud_s2b\nf0_range\nf0_freq_hz\nf0_med_dev\nperformer\n\n\n\n\n0\n1\n1\n10.343492\n65.0\n0.138776\n4\n1\n0\n1\n1\n...\n0.126209\n66.526087\n5.541147\n0.307692\n0.389466\n1.056169\n37.794261\n12.932532\n-0.328442\nArt Pepper\n\n\n1\n2\n1\n10.637642\n63.0\n0.171247\n4\n4\n0\n2\n1\n...\n0.349751\n69.133321\n2.912412\n0.250000\n0.468687\n1.120317\n6.365930\n6.956935\n11.135423\nArt Pepper\n\n\n2\n3\n1\n10.843719\n58.0\n0.081270\n4\n4\n0\n2\n4\n...\n0.094051\n66.352130\n3.564563\n0.428571\n0.531354\n1.310389\n68.010392\nNaN\n32.366787\nArt Pepper\n\n\n3\n4\n1\n10.948209\n61.0\n0.235102\n4\n1\n0\n3\n1\n...\n0.521187\n66.484173\n2.414298\n0.818182\n0.559333\n0.984047\n15.443906\n5.867151\n-3.374696\nArt Pepper\n\n\n4\n5\n1\n11.232653\n63.0\n0.130612\n4\n1\n0\n4\n1\n...\n0.560737\n71.699054\n2.185794\n0.166667\n0.438973\n1.061262\n11.444363\n8.329975\n6.377737\nArt Pepper\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n200804\n200805\n456\n63.135057\n57.0\n0.168345\n4\n2\n53\n4\n2\n...\n1.113380\n72.169552\n6.896394\n0.687500\n0.581956\n1.271747\n191.074095\n10.966972\n-11.891698\nZoot Sims\n\n\n200805\n200806\n456\n63.303401\n55.0\n0.087075\n4\n3\n54\n1\n1\n...\n0.491496\n69.732265\n1.814723\n0.500000\n0.595212\n1.339060\n40.375449\nNaN\n-99.173779\nZoot Sims\n\n\n200806\n200807\n456\n63.390476\n57.0\n0.191565\n4\n3\n54\n1\n2\n...\n1.187058\n76.628621\n2.628726\n0.411765\n0.590950\n1.432802\n104.823845\n11.148561\n-2.911604\nZoot Sims\n\n\n200807\n200808\n456\n63.640091\n59.0\n0.406349\n4\n1\n54\n2\n1\n...\n0.972676\n66.042058\n3.690577\n0.000000\n0.334937\n1.082549\n165.810976\n2.659723\n14.311001\nZoot Sims\n\n\n200808\n200809\n456\n64.058050\n52.0\n1.433832\n4\n2\n54\n3\n2\n...\n0.368321\n58.174931\n9.418678\n0.053030\n0.400571\n1.278890\n66.932198\n2.153916\n-9.381310\nZoot Sims\n\n\n\n\n200809 rows × 27 columns\nbig_df\n\n\n\n\n\n\n\n\n\npitch\nduration\nonset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\n\n\n\n\n0\n65.0\n0.138776\n0.138776\n-0.460594\n0.001710\n0.001710\n\n\n1\n63.0\n0.171247\n0.310023\n-0.697714\n0.002110\n0.003820\n\n\n2\n58.0\n0.081270\n0.391293\n-1.290513\n0.001001\n0.004822\n\n\n3\n61.0\n0.235102\n0.626395\n-0.934833\n0.002897\n0.007719\n\n\n4\n63.0\n0.130612\n0.757007\n-0.697714\n0.001610\n0.009329\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n200585\n62.0\n0.870748\n68.588934\n0.014206\n0.012540\n0.987794\n\n\n200586\n57.0\n0.133515\n68.722449\n-0.896471\n0.001923\n0.989717\n\n\n200587\n62.0\n0.139320\n68.861769\n0.014206\n0.002006\n0.991723\n\n\n200588\n61.0\n0.133515\n68.995283\n-0.167930\n0.001923\n0.993646\n\n\n200589\n60.0\n0.441179\n69.436463\n-0.350065\n0.006354\n1.000000\n\n\n\n\n200590 rows × 6 columns\nsolos_meta[\"performer\"].unique()\n\narray(['Art Pepper', 'Benny Carter', 'Benny Goodman', 'Ben Webster',\n       'Bix Beiderbecke', 'Bob Berg', 'Branford Marsalis', 'Buck Clayton',\n       'Cannonball Adderley', 'Charlie Parker', 'Charlie Shavers',\n       'Chet Baker', 'Chris Potter', 'Chu Berry', 'Clifford Brown',\n       'Coleman Hawkins', 'Curtis Fuller', 'David Liebman',\n       'David Murray', 'Dexter Gordon', 'Dickie Wells', 'Dizzy Gillespie',\n       'Don Byas', 'Don Ellis', 'Eric Dolphy', 'Fats Navarro',\n       'Freddie Hubbard', 'George Coleman', 'Gerry Mulligan',\n       'Hank Mobley', 'Harry Edison', 'Henry Allen', 'Herbie Hancock',\n       'J.C. Higginbotham', 'J.J. Johnson', 'Joe Henderson', 'Joe Lovano',\n       'John Abercrombie', 'John Coltrane', 'Johnny Dodds',\n       'Johnny Hodges', 'Joshua Redman', 'Kai Winding', 'Kenny Dorham',\n       'Kenny Garrett', 'Kenny Wheeler', 'Kid Ory', 'Lee Konitz',\n       'Lee Morgan', 'Lester Young', 'Lionel Hampton', 'Louis Armstrong',\n       'Michael Brecker', 'Miles Davis', 'Milt Jackson', 'Nat Adderley',\n       'Ornette Coleman', 'Pat Martino', 'Pat Metheny', 'Paul Desmond',\n       'Pepper Adams', 'Phil Woods', 'Red Garland', 'Rex Stewart',\n       'Roy Eldridge', 'Sidney Bechet', 'Sonny Rollins', 'Sonny Stitt',\n       'Stan Getz', 'Steve Coleman', 'Steve Lacy', 'Steve Turre',\n       'Von Freeman', 'Warne Marsh', 'Wayne Shorter', 'Woody Shaw',\n       'Wynton Marsalis', 'Zoot Sims'], dtype=object)\n%%time\n\nfig, ax = plt.subplots(figsize=(12,8))\n\nartists = [\"Louis Armstrong\"]\n\n# for i, (artist, group) in enumerate(solos.groupby(\"performer\")):\n#     if artist in artists:\n#         for j, group in group.groupby(\"melid\"):\n#             solo = standardize(notelist(j))\n#             x = solo[\"Rel. Onset\"]\n#             y = solo[\"Rel. MIDI Pitch\"]\n#             ax. plot(x,y, lw=.5, c=\"tab:red\", alpha=.5)\n\nfor ID in range(solos_meta.shape[0]):\n    solo = standardize(notelist(ID))\n    x = solo[\"Rel. Onset\"]\n    y = solo[\"Rel. MIDI Pitch\"]\n    ax. plot(x,y, lw=.5, c=\"tab:red\", alpha=.05)\n        \nax.axvline(.25, lw=2, ls=\"--\", c=\"gray\")\nax.axvline(.5, lw=2, ls=\"--\", c=\"gray\")\nax.axvline(.75, lw=2, ls=\"--\", c=\"gray\")\nax.axhline(0, lw=2, ls=\"--\", c=\"gray\")\n\nlowess = sm.nonparametric.lowess\nbig_x = big_df[\"Rel. Onset\"]\nbig_y = big_df[\"Rel. MIDI Pitch\"]\nbig_z = lowess(big_y, big_x, frac=1/10, delta=1/20)\nax.plot(big_z[:,0], big_z[:,1], c=\"black\", lw=3)\n\nplt.title(\"Solo wave\")\nplt.xlabel(\"Relative onset\")\nplt.ylabel(\"Pitch deviation\")\nplt.xticks(np.linspace(0,1,5))\nplt.yticks(np.linspace(-5,5,11))\nplt.xlim(0,1)\n\nplt.tight_layout()\nplt.savefig(\"img/jazz_melodic_arc.png\")\nplt.show()\n\n\n\n\n\n\n\n\nCPU times: user 3.97 s, sys: 9.36 ms, total: 3.98 s\nWall time: 4 s",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html#pitch-vs-loudness",
    "href": "04_jazz_solos.html#pitch-vs-loudness",
    "title": "14  Solos in the Weimar Jazz Database",
    "section": "14.2 Pitch vs loudness",
    "text": "14.2 Pitch vs loudness\nAbove we have already analyzed some melodic profiles and seen that, on average, the Jazz solos tend not to follow the melodic arch on a global scale. Now, we ask whether the pitch of the notes in the solos are related to another important feature of performance: loudness. The WJazzD contains several measures for loudness (compare the columns in the solos DataFrame). Here, we focus on the “Median loudness” which is stored in the loud_med column.\nLet us look at an example.\n\nexample_solo = solos[ solos[\"melid\"] == 233 ][[\"pitch\", \"loud_med\"]]\n\n\nexample_solo\n\n\n\n\n\n\n\n\n\npitch\nloud_med\n\n\n\n\n115075\n62.0\n67.082700\n\n\n115076\n65.0\n65.345677\n\n\n115077\n67.0\n66.323539\n\n\n115078\n69.0\n69.204257\n\n\n115079\n62.0\n69.059581\n\n\n...\n...\n...\n\n\n115549\n59.0\n61.083907\n\n\n115550\n57.0\n58.345887\n\n\n115551\n55.0\n65.132786\n\n\n115552\n54.0\n59.595735\n\n\n115553\n53.0\n58.390132\n\n\n\n\n479 rows × 2 columns\n\n\n\n\nWe can get a visual impression of whether there might be a direct relation between the two features by plotting it and drawing a regression line. For this, the regplot() function of the seaborn library is well-suited.\n\nsns.regplot(data=example_solo, x=\"pitch\", y=\"loud_med\", line_kws={\"color\":\"black\"});\n\n\n\n\n\n\n\n\nThere seems to be no clear relation; no matter how high the pitch, the loudness stays more or less the same. Let’s look at another example!\n\nexample_solo2 = solos[ solos[\"melid\"] == 333 ][[\"pitch\", \"loud_med\"]]\n\nsns.regplot(data=example_solo2, x=\"pitch\", y=\"loud_med\", line_kws={\"color\":\"black\"});\n\n\n\n\n\n\n\n\nIn this case, there is a positive trend. The higher the pitch, the louder the performer plays. Since we have now two different examples - in one case no relation, in the other case a positive correlation - we should now look at whether there is a trend emerging from all solos taken together.",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html#the-rain-cloud-of-jazz-solos",
    "href": "04_jazz_solos.html#the-rain-cloud-of-jazz-solos",
    "title": "14  Solos in the Weimar Jazz Database",
    "section": "14.3 The “rain cloud” of Jazz solos",
    "text": "14.3 The “rain cloud” of Jazz solos\nWe now take all 200’809 notes from all solos and look at the relation between their pitch and their median loudness.\n\nX = solos[[\"pitch\", \"loud_med\"]].values\nx = X[:,0]\ny = X[:,1]\n\nfig, ax = plt.subplots(figsize=(16,9))\nax.scatter(x,y, alpha=0.05)\n\nplt.xlabel(\"Pitch\")\nplt.ylabel(\"Median loudness [dB]\")\nplt.show()\n\n\n\n\n\n\n\n\nThe visual impression is that of a cloud from which rain drops down and forms a puddle. Which trends can we observe?",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html#comparing-performers",
    "href": "04_jazz_solos.html#comparing-performers",
    "title": "14  Solos in the Weimar Jazz Database",
    "section": "14.4 Comparing performers",
    "text": "14.4 Comparing performers\nTaking all pieces together was not really informative. Maybe a somewhat closer look brings more to the front. Let us some specific performers whose solos we want to compare.\n\nselected_performers = [\"Charlie Parker\", \"Miles Davis\", \"Louis Armstrong\", \"Herbie Hancock\", \"Von Freeman\", \"Red Garland\"]\n\n\ngrouped_df = solos.groupby(\"performer\")\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor performer, df in grouped_df:\n    if performer in selected_performers:\n        sns.regplot(\n            data=df, \n            x=\"pitch\", \n            y=\"loud_med\", \n            x_jitter=.1, \n            y_jitter=.1, \n            scatter_kws={\"alpha\":.01, \"color\":\"grey\"}, \n            line_kws={\"lw\":2},\n            label=performer,\n            scatter=False,\n            ax=ax\n        )\n        \nplt.xlabel(\"MIDI Pitch\")\nplt.ylabel(\"Median loudness [dB]\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nObservations:\n\nMost performers increase loudness with increasing pitch.\nCharlie Parker (sax) and Louis Armstrong (t) show very similar patterns but Armstrong is generally higher.\nMiles Davis (t) is similar to the two but plays generally softer than both.\nVon Freeman (sax) strongly and Herbie Hancock (p) weakly decrease loudness with increasing pitch (almost all other performers show positive correlations).\nRed Garland (p) plays generally lower than Herbie Hancock (p) but does show a positive correlation between pitch and loudness (NB: there is only one solo in the database).\n\nDoes this tell us something about performer styles or about instruments?",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html",
    "href": "05_data-driven_music_history.html",
    "title": "15  Data-Driven Music History",
    "section": "",
    "text": "15.1 Research Questions",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#research-questions",
    "href": "05_data-driven_music_history.html#research-questions",
    "title": "15  Data-Driven Music History",
    "section": "",
    "text": "General: How can we study historical changes quantitatively?\nSpecific: What can we say about the history of tonality based on a dataset of musical pieces?",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#a-bit-of-theory",
    "href": "05_data-driven_music_history.html#a-bit-of-theory",
    "title": "15  Data-Driven Music History",
    "section": "15.2 A bit of theory",
    "text": "15.2 A bit of theory\n\nnote_names = list(\"FCGDAEB\") # diatonic note names in fifths ordering\nnote_names\n\n['F', 'C', 'G', 'D', 'A', 'E', 'B']\n\n\n\naccidentals = [\"bb\", \"b\", \"\", \"#\", \"##\"] # up to two accidentals is suffient here\naccidentals\n\n['bb', 'b', '', '#', '##']\n\n\n\nlof = [ n + a for a in accidentals for n in note_names ] # lof = \"Line of Fifths\"\nprint(lof)\n\n['Fbb', 'Cbb', 'Gbb', 'Dbb', 'Abb', 'Ebb', 'Bbb', 'Fb', 'Cb', 'Gb', 'Db', 'Ab', 'Eb', 'Bb', 'F', 'C', 'G', 'D', 'A', 'E', 'B', 'F#', 'C#', 'G#', 'D#', 'A#', 'E#', 'B#', 'F##', 'C##', 'G##', 'D##', 'A##', 'E##', 'B##']\n\n\n\nlen(lof) # how long is this line-of-fifths segment?\n\n35\n\n\nWe call the elements on the line of fifths tonal pitch-classes",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#data",
    "href": "05_data-driven_music_history.html#data",
    "title": "15  Data-Driven Music History",
    "section": "15.3 Data",
    "text": "15.3 Data\n\n15.3.1 A (kind of) large corpus: TP3C\nHere, we use a dataset that was specifically compiled for this kind of analysis, the Tonal pitch-class counts corpus (TP3C) (Moss, Neuwirth, Rohrmeier, 2020)\n\n2,012 pieces\n75 composers\napprox. spans 600 years of music history\ndoes not contain complete pieces but only counts of tonal pitch-classes\n\n\nimport pandas as pd # to work with tabular data\n\nurl = \"https://raw.githubusercontent.com/DCMLab/TP3C/master/tp3c.tsv\"\ndata = pd.read_table(url)\n\ndata.sample(10)\n\n\n\n\n\n\n\n\n\ncomposer\ncomposer_first\nwork_group\nwork_catalogue\nopus\nno\nmov\ntitle\ncomposition\npublication\nsource\ndisplay_year\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\n\n\n\n\n522\nBrahms\nJohannes\nSechs Klavierstücke\nOp.\n118\n2\nNaN\nIntermezzo\n1893.0\nNaN\nMS\n1893.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n2\n2\n25\n50\n24\n166\n358\n243\n177\n226\n192\n127\n57\n31\n29\n11\n0\n0\n0\n0\n0\n0\n0\n\n\n514\nBrahms\nJohannes\n7 Fantasien\nOp.\n116\n4\nNaN\nIntermezzo\n1892.0\nNaN\nDCML\n1892.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n22\n7\n21\n106\n186\n134\n107\n109\n134\n89\n27\n4\n15\n11\n0\n0\n0\n0\n0\n0\n\n\n606\nMachaut\nGuillaume de\nNaN\nNaN\nNaN\nNaN\nNaN\nDe toutes flours\n1377.0\nNaN\nELVIS\n1377.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n25\n25\n26\n99\n68\n62\n44\n24\n23\n19\n3\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n817\nDesprez\nJosquin\nMissa Ave Maris Stella\nNaN\nNaN\nNaN\nNaN\nBenedictus\n1486.0\nNaN\nELVIS\n1486.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n16\n12\n11\n12\n22\n24\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n907\nAlkan\nCharles Valentin\nNaN\nOp.\n50b\nNaN\nNaN\nLe Tambour Bat aux Champs\nNaN\n1859.0\nMS\n1859.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n14\n44\n67\n59\n78\n150\n171\n324\n231\n253\n628\n450\n250\n128\n120\n173\n72\n4\n1\n3\n4\n0\n0\n0\n0\n\n\n211\nLiszt\nFranz\n12 Transcendental Etudes\nS.\n139\n10\nNaN\nAllegro Agitato Molto\n1851.0\nNaN\nMS\n1851.0\n0\n0\n0\n0\n4\n4\n14\n32\n45\n149\n426\n429\n287\n341\n602\n498\n282\n160\n142\n226\n259\n56\n26\n66\n40\n14\n20\n11\n10\n7\n0\n0\n0\n0\n0\n\n\n1969\nMozart\nWolfgang Amadeus\nSonaten\nKV\n310\n9\n1.0\nNaN\n1777.0\nNaN\nCCARH\n1777.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n7\n37\n272\n374\n394\n290\n184\n286\n203\n60\n28\n11\n10\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n24\nBeethoven\nLudwig van\nPiano Sonatas\nOp.\n54\nNaN\n2.0\nPiano Sonata No. 22\n1804.0\nNaN\nMS\n1804.0\n0\n0\n0\n0\n0\n3\n22\n8\n19\n49\n113\n133\n110\n197\n402\n389\n297\n194\n228\n210\n96\n37\n45\n28\n17\n3\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n812\nDufay\nGuillaume\nNaN\nNaN\nNaN\nNaN\nNaN\nJe ne suy plus teil que soloye\n1474.0\nNaN\nELVIS\n1474.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n24\n28\n43\n20\n15\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1112\nReichardt\nLouise\nSechs Lieder von Novalis\nOp.\n4\n3b\nNaN\nGeistliches Lied\nNaN\nNaN\nOSLC\n1802.5\n0\n0\n0\n0\n0\n0\n0\n1\n0\n9\n33\n39\n22\n14\n21\n15\n6\n2\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\ndata[\"display_year\"].plot(kind=\"hist\", bins=50, figsize=(15,6)); # historical overview\n\n\n\n\n\n\n\n\n\nit can be seen that there are large gaps and that some historical periods are underrepresented\nhowever, it is not so obvious how to fix that\ndo we want a uniform distribution over time?\ndo we want a “historically accurate” distribution?\ndo we want to remove geographical/gender/class/instrument/etc. biases?\non one hand, balanced datasets are likely not to reflect historical realities\non the other hand, such datasets rather represent the “canon”, that is a contemporary selection of “valuable” compositions that may differ greatly from what was considered relevant at the time\n\n–&gt; There is no unique objective answer to these questions. It is important to be aware of these limitations and take them into account when interpreting the results\nFor this workshop we ignore all the metadata about the pieces (titles, composer names etc.) but only focus on their tonal material. Therefore, we don’t need all the columns of the table.\n\ntpc_counts = data.loc[:, lof] # select all rows (\":\") and the lof columns\ntpc_counts.sample(20)\n\n\n\n\n\n\n\n\n\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\n\n\n\n\n64\n0\n0\n0\n0\n0\n0\n0\n1\n11\n68\n94\n128\n361\n664\n866\n455\n271\n399\n337\n95\n80\n29\n42\n14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1721\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n13\n0\n15\n18\n20\n6\n2\n0\n6\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1742\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n33\n75\n47\n70\n107\n108\n56\n7\n8\n24\n3\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1534\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n6\n20\n2\n20\n155\n260\n124\n254\n423\n544\n342\n143\n148\n207\n119\n21\n20\n37\n2\n0\n2\n1\n0\n0\n0\n\n\n856\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n22\n36\n32\n32\n18\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1661\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n43\n46\n48\n43\n28\n50\n20\n10\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1982\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n19\n15\n78\n136\n201\n200\n144\n106\n120\n66\n22\n6\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1238\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n16\n132\n193\n80\n157\n289\n315\n207\n27\n51\n87\n29\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1411\n0\n0\n0\n0\n0\n0\n4\n4\n4\n60\n57\n61\n65\n54\n66\n61\n9\n7\n3\n20\n22\n14\n0\n9\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n417\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n60\n38\n40\n109\n50\n70\n24\n0\n9\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1313\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n2\n48\n110\n22\n83\n124\n90\n63\n1\n3\n28\n4\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1237\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n21\n13\n69\n93\n218\n55\n41\n93\n47\n4\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n415\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n23\n177\n379\n352\n314\n166\n240\n141\n19\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1330\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n21\n92\n95\n101\n106\n75\n86\n38\n17\n10\n10\n3\n2\n0\n0\n0\n0\n0\n0\n\n\n1214\n0\n0\n0\n0\n0\n0\n25\n51\n0\n55\n242\n322\n361\n134\n131\n200\n78\n6\n70\n45\n3\n11\n25\n5\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1124\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n52\n119\n188\n90\n23\n79\n44\n17\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n422\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n26\n29\n30\n36\n28\n33\n15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n458\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n28\n90\n109\n88\n129\n150\n104\n62\n24\n12\n11\n8\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n678\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n9\n18\n48\n73\n42\n81\n145\n174\n76\n15\n39\n41\n22\n0\n0\n2\n0\n0\n0\n\n\n1100\n0\n0\n0\n0\n0\n0\n0\n29\n3\n24\n49\n36\n67\n76\n180\n102\n123\n197\n195\n54\n128\n54\n9\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\npiece = tpc_counts.iloc[10]\n\nfig, axes = plt.subplots(2, 1, figsize=(20,10))\n\naxes[0].bar(piece.sort_values(ascending=False).index, piece.sort_values(ascending=False))\naxes[0].set_title(\"'without theory'\")\n\naxes[1].bar(piece.index, piece)\naxes[1].set_title(\"'with theory'\")\n\n# plt.savefig(\"img/random_piece.png\")\n# plt.show()\n\nText(0.5, 1.0, \"'with theory'\")\n\n\n\n\n\n\n\n\n\nLet us have an overview of the note counts in these pieces!\nIf we would just look at the raw counts of the tonal pitch-classe, we could not learn much from it. Using a theoretical model (the line of fifths) shows that the notes in pieces are usually come from few adjacent keys (you don’t say!).\nWe probably have very long pieces (sonatas) and very short pieces (songs) in the dataset. Since we don’t want length (or the absolute number of notes in a piece) to have an effect, we rather consider tonal pitch-class distributions instead counts, by normalizing all pieces to sum to one.\n\ntpc_dists = tpc_counts.div(tpc_counts.sum(axis=1), axis=0)\ntpc_dists.sample(20)\n\n\n\n\n\n\n\n\n\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\n\n\n\n\n945\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.003953\n0.015152\n0.028327\n0.015152\n0.024374\n0.140975\n0.245718\n0.191700\n0.046772\n0.094203\n0.092885\n0.062582\n0.032938\n0.001976\n0.000000\n0.003294\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1715\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.108262\n0.176638\n0.207977\n0.131054\n0.099715\n0.159544\n0.091168\n0.017094\n0.000000\n0.008547\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1501\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.013699\n0.092797\n0.213875\n0.190897\n0.100751\n0.107380\n0.171012\n0.099867\n0.009722\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1616\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.047619\n0.134921\n0.297619\n0.166667\n0.059524\n0.099206\n0.142857\n0.051587\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n234\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.020126\n0.108176\n0.113208\n0.075472\n0.132075\n0.184906\n0.130818\n0.100629\n0.023899\n0.033962\n0.054088\n0.021384\n0.001258\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1764\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.003413\n0.054608\n0.177474\n0.085324\n0.112628\n0.187713\n0.211604\n0.095563\n0.027304\n0.000000\n0.037543\n0.006826\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1075\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00572\n0.002080\n0.016121\n0.098284\n0.121685\n0.065523\n0.108684\n0.159646\n0.156006\n0.085283\n0.027041\n0.042122\n0.072283\n0.028081\n0.010400\n0.001040\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n601\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.003367\n0.097643\n0.185185\n0.188552\n0.171717\n0.094276\n0.148148\n0.097643\n0.013468\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n693\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.082255\n0.149723\n0.164510\n0.151571\n0.138632\n0.151571\n0.112754\n0.048059\n0.000000\n0.000000\n0.000924\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n919\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000202\n0.005060\n0.035013\n0.050799\n0.040680\n0.069217\n0.132969\n0.111516\n0.110099\n0.032382\n0.127505\n0.076098\n0.040478\n0.018620\n0.046752\n0.030763\n0.028942\n0.012143\n0.010727\n0.010322\n0.007084\n0.002631\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1573\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.002868\n0.000000\n0.006692\n0.016252\n0.065010\n0.067878\n0.024857\n0.051625\n0.177820\n0.173040\n0.092734\n0.054493\n0.103250\n0.065966\n0.034417\n0.019120\n0.028681\n0.009560\n0.005736\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1979\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.090909\n0.223140\n0.245868\n0.136364\n0.037190\n0.177686\n0.076446\n0.008264\n0.000000\n0.002066\n0.002066\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1188\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.010582\n0.037037\n0.066138\n0.206349\n0.124339\n0.195767\n0.119048\n0.153439\n0.052910\n0.034392\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1710\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.048417\n0.048417\n0.040968\n0.007449\n0.111732\n0.199255\n0.193669\n0.081937\n0.111732\n0.072626\n0.068901\n0.011173\n0.000000\n0.003724\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n88\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.008895\n0.070521\n0.127700\n0.088310\n0.122618\n0.153113\n0.140407\n0.132147\n0.056544\n0.020330\n0.054003\n0.024778\n0.000635\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1670\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000847\n0.005085\n0.100000\n0.182203\n0.189831\n0.141525\n0.120339\n0.135593\n0.104237\n0.017797\n0.000000\n0.002542\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.025424\n0.135593\n0.101695\n0.076271\n0.144068\n0.135593\n0.211864\n0.135593\n0.000000\n0.016949\n0.016949\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1293\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.013793\n0.000000\n0.004138\n0.095172\n0.180690\n0.256552\n0.115862\n0.081379\n0.118621\n0.085517\n0.017931\n0.006897\n0.023448\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n243\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00120\n0.004802\n0.000000\n0.015606\n0.026411\n0.088836\n0.141657\n0.038415\n0.085234\n0.176471\n0.152461\n0.110444\n0.034814\n0.018007\n0.074430\n0.025210\n0.003601\n0.000000\n0.000000\n0.002401\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1751\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.057692\n0.132212\n0.109375\n0.116587\n0.159856\n0.223558\n0.125000\n0.028846\n0.009615\n0.028846\n0.008413\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nFor further numerical analysis, we extract the data from this table and assign it to a variable X.\n\n# extract values of table to matrix\nX = tpc_dists.values\n\nX.shape # shows (#rows, #columns) of X\n\n(2012, 35)\n\n\nNow, X is a 2012 \\(\\times\\) 35 matrix where the rows represent the pieces and the columns (also called “features” or “dimensions”) represent the relative frequency of tonal pitch-classes.\nThinking in 35 dimensions is quite difficult for most people. Without trying to imagine what this would look like, what can we already say about this data?\nSince each piece is a point in this 35-D space and pieces are represented as vectors, pieces that have similar tonal pitch-class distributions must be close in this space (whatever this looks like).\nWhat groups of pieces that cluster together? Maybe pieces of the same composer are similar to each other? Maybe pieces from a similar time? Maybe pieces for the same instruments?\nIf we find clusters, these would still be in 35-D and thus difficult to interpret. Luckily, there are a range of so-called dimensionality reduction methods that transform the data into lower-dimensional spaces so that we actually can look at them.\nA very common dimensionality reduction method is Principal Components Analysis (PCA).\nThe basic idea of PCA is:\n\nfind dimensions in the data that maximize the variance in this direction\nthese dimensions have to be orthogonal to each other (mutually independent)\nthese dimensions are called the principal components\neach principal component is associated with how much of the data variance it explains\n\n\nimport numpy as np # for numerical computations\nimport sklearn\nfrom sklearn.decomposition import PCA # for dimensionality reduction\n\npca = sklearn.decomposition.PCA(n_components=35) # initialize PCA with 35 dimensions\npca.fit(X) # apply it to the data\nvariance = pca.explained_variance_ratio_ # assign explained variance to variable\n\n\nfig, ax = plt.subplots(figsize=(14,5))\nx = np.arange(35)\nax.plot(x, variance, label=\"relative\", marker=\"o\")\nax.plot(x, variance.cumsum(), label=\"cumulative\", marker=\"o\")\nax.set_xlim(-0.5, 35)\nax.set_ylim(-0.1, 1.1)\nax.set_xlabel(\"Principal Components\")\nax.set_ylabel(\"Explained variance\")\nplt.xticks(np.arange(len(lof)), np.arange(len(lof)) + 1) # because Pyhon starts counting at 0\n\nplt.legend(loc=\"center right\")\nplt.tight_layout()\n# plt.savefig(\"img/explained_variance.png\")\n# plt.show()\n\n\n\n\n\n\n\n\n\nvariance[:5]\n\narray([0.41144591, 0.23410347, 0.09063507, 0.07574242, 0.04436989])\n\n\nThe first principal component explains 41.1% of the variance of the data, the second explains 23.4% and the third 9%. Together, this amounts to 73.6%.\nAlmost three quarters of the variance in the dataset is retained by reducing the dimensionality from 35 to 3 dimensions (8.6%)! If we reduce the data to two dimensions, we still can explain \\(\\approx\\) 65% of the variance.\nThis is great because it means that we can look at the data in 2 or 3 dimensions without loosing too much information.",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#recovering-the-line-of-fifths-from-data",
    "href": "05_data-driven_music_history.html#recovering-the-line-of-fifths-from-data",
    "title": "15  Data-Driven Music History",
    "section": "15.4 Recovering the line of fifths from data",
    "text": "15.4 Recovering the line of fifths from data\n\npca3d = PCA(n_components=3)\npca3d.fit(X)\n\nX_ = pca3d.transform(X)\nX_.shape\n\n(2012, 3)\n\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(6,6))\n\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_[:,0], X_[:,1], X_[:,2], s=50, alpha=.25) # c=cs,\nax.set_xlabel(\"PC 1\", labelpad=30)\nax.set_ylabel(\"PC 2\", labelpad=30)\nax.set_zlabel(\"PC 3\", labelpad=30)\n\nplt.tight_layout()\n# plt.savefig(\"img/3d_scatter.png\")\n# plt.show()\n\n\n\n\n\n\n\n\nEach piece in this plot is represented by a point in 3-D space. But remember that this location represents ~75% of the information contained in the full tonal pitch-class distribution. In 35-D space each dimension corresponded to the relative frequency of a tonal pitch-class in a piece.\n\nWhat do these three dimensions signify?\nHow can we interpret them?\n\nFortunately, we can inspect them individually and try to interpret what we see.\n\nfrom itertools import combinations\n\nfig, axes = plt.subplots(1,3, sharey=True, figsize=(24,8))\n\nfor k, (i, j) in enumerate(combinations(range(3), 2)):\n\n    axes[k].scatter(X_[:,i], X_[:,j], s=50, alpha=.25, edgecolor=None)\n    axes[k].set_xlabel(f\"PC {i+1}\")\n    axes[k].set_ylabel(f\"PC {j+1}\")\n    axes[k].set_aspect(\"equal\")\n\nplt.tight_layout()\n# plt.savefig(\"img/3d_dimension_pairs.png\")\n# plt.show()\n\n\n\n\n\n\n\n\nClearly, looking at two principal components at a time shows that there is some latent structure in the data. How can we understand it better?\nOne way to see whether the pieces are clustered together systematically be coloring them according to some criterion.\nAs always, many different options are available. For the present purpose we will use the most simple summary of the piece: its most frequent note (which is the mode of its pitch-class distribution in statistical terms) and call this note its tonal center.\nThis will also allow to map the tonal pitch-classes on the line of fifths to colors.\n\ntpc_dists[\"tonal_center\"] = tpc_dists.apply(lambda piece: np.argmax(piece[lof].values) - 15, axis=1)\ntpc_dists.sample(10)\n\n\n\n\n\n\n\n\n\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\ntonal_center\n\n\n\n\n75\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000899\n0.006890\n0.015578\n0.014979\n0.091372\n0.158179\n0.121031\n0.152786\n0.147693\n0.108448\n0.086878\n0.046435\n0.023068\n0.015878\n0.008089\n0.001797\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-3\n\n\n1093\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.003185\n0.015924\n0.149682\n0.136943\n0.117834\n0.194268\n0.210191\n0.101911\n0.057325\n0.000000\n0.006369\n0.006369\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n\n\n1870\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.098081\n0.174840\n0.171286\n0.095238\n0.121535\n0.192608\n0.110163\n0.017768\n0.002843\n0.008529\n0.007107\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4\n\n\n930\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.005489\n0.002994\n0.000499\n0.056387\n0.163174\n0.237525\n0.046407\n0.053393\n0.089820\n0.060878\n0.004990\n0.004491\n0.038423\n0.051896\n0.038922\n0.017465\n0.054391\n0.039920\n0.015968\n0.003992\n0.008483\n0.003992\n0.000499\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-4\n\n\n1734\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.002967\n0.100890\n0.181009\n0.210682\n0.148368\n0.112760\n0.106825\n0.094955\n0.032641\n0.005935\n0.002967\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n\n\n844\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.035477\n0.002217\n0.000000\n0.035477\n0.170732\n0.101996\n0.099778\n0.199557\n0.221729\n0.099778\n0.022173\n0.011086\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n1786\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.043956\n0.183150\n0.208791\n0.139194\n0.146520\n0.109890\n0.124542\n0.032967\n0.000000\n0.010989\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4\n\n\n960\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.016345\n0.156018\n0.141159\n0.135215\n0.121842\n0.219911\n0.120357\n0.075780\n0.002972\n0.002972\n0.007429\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n\n\n1719\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078947\n0.131579\n0.078947\n0.105263\n0.179825\n0.245614\n0.083333\n0.008772\n0.017544\n0.061404\n0.008772\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4\n\n\n1939\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000513\n0.001026\n0.015906\n0.089790\n0.191893\n0.143150\n0.129810\n0.119548\n0.126219\n0.102617\n0.037455\n0.010775\n0.025141\n0.003592\n0.002565\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-2\n\n\n\n\n\n\n\n\n\nfrom matplotlib import cm\nfrom matplotlib.colors import Normalize\n\n#normalize item number values to colormap\nnorm = Normalize(vmin=-15, vmax=20)\n\n# cs = [ cm.seismic(norm(c)) for c in data[\"tonal_center\"]]\ncs = [ cm.seismic(norm(c)) for c in tpc_dists[\"tonal_center\"]]\n\n\nfrom itertools import combinations\n\nfig, axes = plt.subplots(1,3, sharey=True, figsize=(24,8))\n\nfor k, (i, j) in enumerate(combinations(range(3), 2)):\n\n    axes[k].scatter(X_[:,i], X_[:,j], s=50, c=[ np.abs(c) for c in cs], edgecolor=None)\n    axes[k].set_xlabel(f\"PC {i}\")\n    axes[k].set_ylabel(f\"PC {j}\")\n    axes[k].set_aspect(\"equal\")\n\nplt.tight_layout()\n# plt.savefig(\"img/3d_dimension_pairs_colored.png\")\n# plt.show()",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#historical-development-of-tonality",
    "href": "05_data-driven_music_history.html#historical-development-of-tonality",
    "title": "15  Data-Driven Music History",
    "section": "15.5 Historical development of tonality",
    "text": "15.5 Historical development of tonality\nThe line of fifths is an important underlying structure for pitch-class distributions in tonal compositions\nBut we have treated all pieces in our dataset as synchronic and have not yet taken their historical location into account.\nLet’s assume the pitch-class content of a piece spreads on the line of fifths from F to A\\(\\sharp\\). This means, its range on the line of fifths is \\(10 - (-1) = 11\\). The piece covers eleven consecutive fifths on the lof.\nWe can generalize this calculation and write a function that calculates the range for each piece in the dataset.\n\ndef lof_range(piece):\n    l = [i for i, v in enumerate(piece) if v!=0]\n    return max(l) - min(l)\n\n\ndata[\"lof_range\"] = data.loc[:, lof].apply(lof_range, axis=1) # create a new column\ndata.sample(20)\n\n\n\n\n\n\n\n\n\ncomposer\ncomposer_first\nwork_group\nwork_catalogue\nopus\nno\nmov\ntitle\ncomposition\npublication\nsource\ndisplay_year\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\nlof_range\n\n\n\n\n607\nDebussy\nClaude\nPréludes I\nNaN\nNaN\nNaN\nNaN\nThe girl with the flaxen hair\n1909.0\nNaN\nMS\n1909.0\n0\n0\n0\n0\n0\n0\n0\n5\n58\n136\n110\n57\n121\n121\n33\n17\n10\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n305\nChopin\nFrédéric\nNocturnes\nOp.\n9\n2\nNaN\n3 Nocturnes\n1833.0\nNaN\nMS\n1833.0\n0\n0\n0\n0\n0\n0\n4\n0\n46\n8\n19\n86\n272\n179\n178\n118\n214\n93\n46\n39\n16\n2\n5\n2\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n\n\n5\nMendelssohn\nFelix\nString Quartet\nOp.\n80\nNaN\nNaN\nAllegro vivace assai\n1847.0\n1850.0\nMS\n1847.0\n0\n0\n0\n0\n0\n0\n3\n24\n7\n46\n339\n611\n369\n641\n804\n860\n524\n197\n163\n256\n43\n24\n25\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n\n\n1530\nAlkan\nCharles Valentin\nTrois morceaux dans le genre pathétique\nOp.\n15\n3\nNaN\nMorte\nNaN\n1837.0\nMS\n1837.0\n0\n10\n0\n30\n102\n110\n209\n340\n858\n1384\n934\n951\n1327\n1309\n733\n122\n115\n533\n147\n32\n7\n5\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n21\n\n\n942\nMahler\nGustav\nLieder eines fahrenden Gesellen\nNaN\nNaN\n1\nNaN\nWenn mein Schatz heut Hochzeit macht\nNaN\n1897.0\nOSLC\n1897.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n27\n98\n192\n138\n102\n149\n241\n140\n53\n7\n10\n6\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n\n\n962\nDufay\nGuillaume\nMissa Ava Maria coelorum\nNaN\nNaN\nNaN\nNaN\nKyrie\n1474.0\nNaN\nELVIS\n1474.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n10\n24\n294\n668\n531\n366\n304\n355\n274\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n333\nBach\nJohann Sebastian\nWohltemperiertes Klavier I\nBWV\n862\n2\nNaN\nNaN\n1722.0\nNaN\nMS\n1722.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n18\n105\n137\n137\n127\n141\n122\n101\n20\n9\n14\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n\n\n1127\nReichardt\nLouise\nZwölf Deutsche und Italiänische Romantische Ge...\nOp.\nNaN\n9\nNaN\nAus Ariels Offenbarungen\nNaN\nNaN\nOSLC\n1802.5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n2\n31\n66\n63\n38\n12\n35\n23\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n\n\n545\nStrauss\nRichard\nLieder\nOp.\n27\n2\nNaN\nCäcilie\n1894.0\nNaN\nMS\n1894.0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n19\n40\n79\n96\n74\n96\n214\n172\n73\n79\n121\n53\n34\n9\n19\n2\n5\n0\n0\n1\n0\n0\n0\n0\n0\n0\n21\n\n\n1549\nWebern\nAnton\nVariationen für Klavier\nOp.\n27\n3\nNaN\nNaN\n1936.0\nNaN\nMS\n1936.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n5\n11\n26\n32\n30\n30\n35\n31\n28\n31\n32\n31\n30\n22\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n15\n\n\n1386\nScriabin\nAlexander\nPréludes\nOp.\n22\n4\nNaN\n4 Preludes\n1897.0\nNaN\nDCML\n1897.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n5\n6\n51\n64\n25\n45\n85\n80\n32\n0\n1\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n\n\n1096\nLasso\nOrlando di\nNaN\nNaN\nNaN\nNaN\nNaN\nQui sequitur me\nNaN\n1577.0\nELVIS\n1577.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n40\n45\n35\n33\n35\n58\n26\n1\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9\n\n\n164\nAlkan\nCharles Valentin\nEsquisses\nOp.\n63\n25\nNaN\nNaN\n1861.0\nNaN\nMS\n1861.0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n2\n6\n13\n5\n26\n74\n130\n70\n99\n87\n100\n57\n15\n5\n16\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n19\n\n\n461\nBach\nJohann Sebastian\nInventions and Sinfonias\nBWV\n772a\nNaN\nNaN\nNaN\nNaN\n1723.0\nMS\n1723.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9\n57\n75\n77\n80\n75\n82\n65\n19\n4\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n1980\nMozart\nWolfgang Amadeus\nSonaten\nKV\n331\n11\n1.0\nVar. 5\n1783.0\nNaN\nCCARH\n1783.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n5\n72\n128\n163\n74\n52\n95\n45\n18\n10\n6\n7\n1\n0\n0\n0\n0\n0\n0\n13\n\n\n546\nChaminade\nCécile\nAlbum des enfants – Première serie\nOp.\n123\n1\nNaN\nNaN\nNaN\n1906.0\nMS\n1906.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n33\n65\n94\n47\n42\n54\n50\n11\n3\n0\n1\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n\n\n1714\nGrieg\nEdvard\nLyrical Pieces\nOp.\n12\n8\nNaN\nNaN\n1866.0\n1867.0\nDCML\n1866.0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n10\n27\n66\n65\n41\n26\n63\n29\n7\n6\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n\n\n1852\nCorelli\nArcangelo\n12 Violin Sonatas\nOp.\n5\n8\n4.0\nNaN\nNaN\n1700.0\nCCARH\n1700.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n21\n62\n27\n55\n65\n67\n44\n6\n5\n16\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n\n\n1995\nMozart\nWolfgang Amadeus\nSonaten\nKV\n533\n15\n3.0\nNaN\n1788.0\nNaN\nCCARH\n1788.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n5\n83\n137\n114\n110\n94\n177\n96\n10\n13\n23\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n\n\n1483\nTchaikovsky\nPyotr\nThe Seasons\nOp.\n37a\n8\nNaN\nAugust: The Harvest\n1876.0\nNaN\nDCML\n1876.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n6\n4\n36\n71\n174\n362\n147\n257\n516\n442\n214\n65\n22\n155\n42\n0\n2\n0\n0\n0\n0\n0\n0\n17\n\n\n\n\n\n\n\n\nThis allows us now to take the display_year (composition or publication) and lof_range (range on the line of fifths) features to observe historical changes.\n\nfig, ax = plt.subplots(figsize=(18,9))\nax.scatter(data[\"display_year\"].values, data[\"lof_range\"].values, alpha=.5, s=50)\nax.set_ylim(0,35)\nax.set_xlabel(\"year\")\nax.set_ylabel(\"line-of-fifths range\");\n# plt.savefig(\"img/hist_scatter.png\");\n\n\n\n\n\n\n\n\nWe could try to fit a line to this data to see whether there is a trend (kinda obvious here).\n\ng = sns.lmplot(\n    data=data, \n    x=\"display_year\", \n    y=\"lof_range\", \n    line_kws={\"color\":\"k\"},\n    scatter_kws={\"alpha\":.5},\n#     lowess=True,\n    height=8,\n    aspect=2\n);\n# g.savefig(\"img/hist_scatter_line.png\");\n\n\n\n\n\n\n\n\nBut actually, this is not the best idea. Why should any historical process be linear? More complex models might make more sense.\nA more versatile technique is Locally Weighted Scatterplot Smoothing (LOWESS) that locally fits a polynomial. Using this method, we see that a non-linear process is displayed.\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nx = data.display_year\ny = data.lof_range\nl = lowess(y,x)\n\nfig, ax = plt.subplots(figsize=(15,10))\n\nax.scatter(x,y, s=50)\nax.plot(l[:,0], l[:,1], c=\"k\")\nax.set_ylabel(\"line-of-fifths range\");\n# plt.savefig(\"img/hist_scatter_lowess.png\")\n# plt.show()",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#if-there-is-time-some-more-advanced-stuff",
    "href": "05_data-driven_music_history.html#if-there-is-time-some-more-advanced-stuff",
    "title": "15  Data-Driven Music History",
    "section": "15.6 If there is time: some more advanced stuff",
    "text": "15.6 If there is time: some more advanced stuff\n\nB = 200\ndelta = 1/10 \n\nfig, ax = plt.subplots(figsize=(16,9))\n\nx = data.display_year\ny = data.lof_range\nl = lowess(y,x, frac=delta)\n\nax.scatter(x,y, s=50, alpha=.25)\n\nfor _ in range(B):\n    resampled = data.sample(data.shape[0], replace=True)\n    \n    xx = resampled.display_year\n    yy = resampled.lof_range\n    ll = lowess(yy,xx, frac=delta)\n    \n    ax.plot(ll[:,0], ll[:,1], c=\"k\", alpha=.05)\n    \nax.plot(l[:,0], l[:,1], c=\"yellow\")\n\n## REGIONS\nfrom matplotlib.patches import Rectangle\n\ntext_kws = {\n    \"rotation\" : 90,\n    \"fontsize\" : 16,\n    \"bbox\" : dict(\n        facecolor=\"white\", \n        boxstyle=\"round\"\n    ),\n    \"horizontalalignment\" : \"center\",\n    \"verticalalignment\" : \"center\"\n}\n\nrect_props = {\n    \"width\" : 40,\n    \"zorder\" : -1,\n    \"alpha\" : 1.\n}\n\nstylecolors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nax.text(1980, 3, \"diatonic\", **text_kws)\nax.axhline(6.5, c=\"gray\", linestyle=\"--\", lw=2) # dia / chrom.\nax.add_patch(Rectangle((1960,0), height=6.5, facecolor=stylecolors[0], **rect_props))\n\nax.text(1980, 9.5, \"chromatic\", **text_kws)\nax.axhline(12.5, c=\"gray\", linestyle=\"--\", lw=2) # chr. / enh.\nax.add_patch(Rectangle((1960,6.5), height=6, facecolor=stylecolors[1], **rect_props))\n\nax.text(1980, 23.5, \"enharmonic\", **text_kws)\nax.add_patch(Rectangle((1960,12.5), height=28, facecolor=stylecolors[2], **rect_props))\n\nax.set_ylim(0,35)\nax.set_xlim(1300,2000)\n\nax.set_ylabel(\"line-of-fifths range\");\n# plt.savefig(\"img/final.png\", dpi=300)\n# plt.show()\n\n\n\n\n\n\n\n\nUsung bootstrap sampling we achieve an estimation of the local varience of the data and thus of the diversity in the note usage of the musical pieces.\nWe also can distinguish three regions in terms of line-of-fifth range: diatonic, chromatic, and enharmonic.\nGrouping the data together in these three regions, we see a clear change from diatonic and chromatic to chromatic and enharmonic pieces over the course of history.\n\nepochs = {\n    \"Renaissance\" : [1300, 1549],\n    \"Baroque\" : [1550, 1649],\n    \"Classical\" : [1650, 1749],\n    \"Early\\nRomantic\" : [1750, 1819],\n    \"Late Romantic/\\nModern\" : [1820, 2000]\n}   \n\nstrata = [\n    \"diatonic\",\n    \"chromatic\",\n    \"enharmonic\"\n]\n\nwidths = data[[\"display_year\", \"lof_range\"]].sort_values(by=\"display_year\").reset_index(drop=True)\n\ndf = pd.concat(\n    [\n        widths[ \n            (widths.display_year &gt;= epochs[e][0]) & (widths.display_year &lt;= epochs[e][1]) \n        ][\"lof_range\"].value_counts(normalize=True).sort_index().groupby( \n            lambda x: strata[0] if x &lt;= 6 else strata[1] if x &lt;= 12 else strata[2]\n        ).sum() for e in epochs\n    ], axis=1, sort=True\n)\n\ndf.columns = epochs.keys()\ndf = df.reindex(strata)\ndf.T.plot(kind=\"bar\", stacked=True, figsize=(12,5))\n# plt.title(\"Epochs\")\nplt.legend(bbox_to_anchor=(1.3,0.75))\nplt.gca().set_xticklabels(epochs.keys(), rotation=\"horizontal\")\nplt.tight_layout()\n# plt.savefig(\"img/epochs_regions.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nRenaissance: largest diatonic proportion overall but mostly chromatic\nBaroque: alost completely chromatic\nClassical: enharmonic proportion increases -&gt; more distant modulations\nThis trend continues through the Romantic eras",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#summary",
    "href": "05_data-driven_music_history.html#summary",
    "title": "15  Data-Driven Music History",
    "section": "15.7 Summary",
    "text": "15.7 Summary\n\nWe have analyzed a very specific aspect of Western classical music.\nWe have used a large(-ish) corpus to answer our research question.\nWe have operationalized musical pieces as vectors that represent distributions of tonal pitch-classes.\nWe have used the dimensionality-reduction technique Principal Component Analysis (PCA) in order to visually inspect the distribution of the data in 2 and 3 dimensions.\nWe have used music-theoretical domain knowledge to find meaningful structure in this space.\nWe have seen that pieces are largely distributed along the line of fifths.\nWe have used Locally Weighted Scatterplot Smoothing (LOWESS) to estimate the variance in this historical process.\nWe have seen that, historically, composers explore ever larger regions on this line and that the variance also increases.",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "13_copyright.html",
    "href": "13_copyright.html",
    "title": "16  Copyright",
    "section": "",
    "text": "Goal\n\n\n\nKnow a few famous copyright infringement cases and why data analysis is important here.\n\n\n\nPlagiarism cases and copyright",
    "crumbs": [
      "CRITICAL DIGITAL MUSICOLOGY",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Copyright</span>"
    ]
  },
  {
    "objectID": "14_representation.html",
    "href": "14_representation.html",
    "title": "17  Representation and representativeness",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand the difference between representativeness and representation. Obtain a critical understanding of biases relevant for data selection.\n\n\n\nRepresentation and the canon\nRepresenting means modeling means abstraction (what is “music” in “music encoding”?)\nbiases: how to recognize them, how to deal with them, and when biases are a good thing.\nFAIR and CARE",
    "crumbs": [
      "CRITICAL DIGITAL MUSICOLOGY",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Representation and representativeness</span>"
    ]
  },
  {
    "objectID": "15_discussion.html",
    "href": "15_discussion.html",
    "title": "18  Discussion",
    "section": "",
    "text": "Goal",
    "crumbs": [
      "CRITICAL DIGITAL MUSICOLOGY",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "ex/01_what_is_dm.html",
    "href": "ex/01_what_is_dm.html",
    "title": "Exercise for Week 1",
    "section": "",
    "text": "In the remainder of the course, we will engage with data and computer code. While this is probably new for most of you, there are luckily many tools that can help us to do so more easily.\nThe instructions below are fairly general. Learning how to find answers online is an important skill for anyone engaging with digital methods.\n\nCreate a directory/folder named intro-digimus for this course (note: no spaces!). You can use any location on your personal computer or on your J: drive provided by the university.\nAs code editor we will use Microsoft Visual Studio Code. Install the program on your machine.\nOpen the directory you just created and install the Jupyter Extension.\nCreate a new Jupyter notebook named test.ipynb and write the following in the first cell: print(\"Hello world!\"). Congratulations, you wrote your first computer program!\n\n\n\n\n\n\n\nHomework\n\n\n\n\nRead Schaffer (2016) and Pugin (2015).\nDiscuss with your peers unclear terms and find answers online.\nFind aspects of Computational Musicology that you think are underrepresented in Schaffer’s list.\nCollaboratively write up a definition for Digital Musicology.\n\n\n\n\n\n\n\nPugin, L. (2015). The Challenge of Data in Digital Musicology. Frontiers in Digital Humanities, 2, 1–3. https://doi.org/10.3389/fdigh.2015.00004\n\n\nSchaffer, K. (2016). What is computational musicology? https://medium.com/@krisshaffer/what-is-computational-musicology-f25ee0a65102",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week 1"
    ]
  },
  {
    "objectID": "ex/02_python_primer.html",
    "href": "ex/02_python_primer.html",
    "title": "Exercise for Week 2",
    "section": "",
    "text": "Variables and types\nVariable assignment in Python is straight-forward. You choose a name for the variable, and assign a value to it using the = operator, for example:\nx = 100\nassigns the value 100 to the variable x. If we call the variable now, we can see that it has the value we assigned to it:\nx\n\n100\nOf course, we can also assign things other than numbers, for example:\nname = \"Fabian\"\nWhat we assigned to the variable name is called a string, it has the value \"Fabian\". Strings are sequences of characters.\nWe can also assign a list of things to a variable:\nmylist = [1, 2, 3, \"A\", \"B\", \"C\"]\nLists are enclosed by square brackets. As you can see, Python allows you to store any kind of data in lists (here: integer numbers and character strings). However, it is good practice to include only items of the same type in lists—you’ll understand later why.\nAnother structured data type in python are dictionaries. Dictionaries are collections of key-value pairs. For example, a dictionary addresses could store the email addresses of certain people:\naddresses = {\n    \"Andrew\" : \"andrew@example.com\",\n    \"Zoe\" : \"zoe@example.com\"\n}\nNow, if we wanted to look up Zoe’s email address, we could to so with:\naddresses[\"Zoe\"]\n\n'zoe@example.com'",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week 2"
    ]
  },
  {
    "objectID": "ex/02_python_primer.html#variables-and-types",
    "href": "ex/02_python_primer.html#variables-and-types",
    "title": "Exercise for Week 2",
    "section": "",
    "text": "Tip\n\n\n\nNote that \"Fabian\" is enclosed by double-quotes. Why is this the case? Why could we not just type name = Fabian?",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week 2"
    ]
  },
  {
    "objectID": "ex/02_python_primer.html#on-repeat",
    "href": "ex/02_python_primer.html#on-repeat",
    "title": "Exercise for Week 2",
    "section": "On repeat",
    "text": "On repeat\nCoding something is only useful if you can’t do the job as fast or as efficient by yourself. Especially when it comes to repeating the same little thing many, many times, knowing how to code comes in handy.\nAs a simple example, imagine you want to write down all numbers from 1 to 10, or from 1 to 100, or… you get the idea. In Python, you would do it as follows:\n\nfor i in range(10):\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nYou see that this is not exactly what we wanted. We’re seeing numbers from 0 to 9, each one being printed on a new line. But what we wanted was all numbers from 1 to 10. Before we fix the code to produce the desired result, let’s explain the bits and pieces of the code above.\nWhat we just did was to use a so-called for-loop, probably the most common way to repeat things in Python. First we create an iterator variable i (we could have named any other variable name as well), which takes its value from the list of numbers specified by range(10). If only one number n is provided to range(n), it will enumerate all numbers from 0 to n-1. If instead two arguments are provided, the first one determines the starting number, and the second one stands for the terminating number minus one—confusing, right?\nSo, in order to enumerate all numbers from 1 to 10, we have to write range(1,11). Additionally, we can use the end keyword of the print function that allows us to print all numbers in one line, separated only by a single white space instead of each one on a new line.\n\nfor i in range(1,11):\n    print(i, end=\" \")\n\n1 2 3 4 5 6 7 8 9 10 \n\n\nVoilà!",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week 2"
    ]
  },
  {
    "objectID": "ex/02_python_primer.html#just-in-case",
    "href": "ex/02_python_primer.html#just-in-case",
    "title": "Exercise for Week 2",
    "section": "Just in case",
    "text": "Just in case\nOften we encounter a situation where we would execute some code only if certain conditions are met. In python, this is done with the if statement. For example, if we want to only print the even numbers in the range from 1 to 10, we could adapt the code from above as follows:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n\n2 is even\n4 is even\n6 is even\n8 is even\n10 is even\n\n\nYou can read this as “if the remainder of dividing i by 2 is zero, then print ‘i is even’”.\nNow, we could also want to print a similar statement in the case that i is odd:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nAnd, finally, we could also have more than just one condition. An if-statement allows for arbitrary many if-else clauses, with which we can formulate several different conditions by writing elif (shorthand for ‘or else if’):\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    elif i % 3 == 0:\n        print(i, \"is divisible by 3\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is divisible by 3\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is divisible by 3\n10 is even\n\n\nWe now know when a number is even and when it is divisible by 3. But what about numbers that are both even and divisible by 3? We just add another condition to the elif statement and enclose each condition in parentheses, so that Python knows how things group together:\n\nfor i in range(1,11):\n    if i % 2 == 0:\n        print(i, \"is even\")\n    elif (i % 2 == 0) and (i % 3 == 0):\n        print(i, \"is even and divisible by 3\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nWhy did this not work? The number 6 is even and divisible by 3! The reason is that the three statements (if, elif, and else) are being executed in the order that we wrote them down. That means, that Python will first check for each number whether it is even (and nothing more), and if it is, it will follow the instruction to print it and go on to the next number. So, once we arrived at 6, the programm will only check if the number is even. That is not the desired result and we have to make a little change to it. We will switch the conditions in the if and elif statements:\n\nfor i in range(1,11):\n    if (i % 2 == 0) and (i % 3 == 0):\n        print(i, \"is even and divisible by 3\")\n    elif i % 2 == 0:\n        print(i, \"is even\")\n    else:\n        print(i, \"is odd\")\n\n1 is odd\n2 is even\n3 is odd\n4 is even\n5 is odd\n6 is even and divisible by 3\n7 is odd\n8 is even\n9 is odd\n10 is even\n\n\nNow it works! Note that new never specified any conditions for the else statement. This is because whatever follows it will be executed in case none of the conditions in if or elif are met.",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week 2"
    ]
  },
  {
    "objectID": "ex/02_python_primer.html#functions",
    "href": "ex/02_python_primer.html#functions",
    "title": "Exercise for Week 2",
    "section": "Functions",
    "text": "Functions\nWith more and more experience in programming, it is likely that your code will become more and more complex. That means that it will become harder to keep track of what every piece of it is supposed to do. A good strategy to deal with this is to aim for writing code that is modular: it can be broken down into smaller units (modules) that are easier to understand. Moreover, it is sometimes necessary to reuse the same code several times. It would, however, be inefficient to write the same lines over and over again. With your code being modular you can wrap the pieces that you need in several places into a function.\nLet’s look at an example! Assume, your (fairly) complex code involves calculating the sum of the squares of two numbers. In Python, we use the + operator to calculate sums and the ** operator to raise a number to a certain power (**2 for the square of a number).\n\nx = 3\ny = 5\n\nsum_of_squares = x**2 + y**2\n\nThe variable sum_of_squares now contains the sum of squares of x=3 and y=5. We can inspect the result by calling the variable:\n\nsum_of_squares \n\n34\n\n\nNow, imagine that you would have to do the same calculation several times for different combinations of values for x and y (and always keeping in mind that this stands in for much more complex examples with several lines of code). We can code this in a function:\n\ndef func_sum_of_squares(x, y):\n    return x**2 + y**2\n\nNow, each time we want to calculate a sum of squares, we can do so by simply invoking\n\nfunc_sum_of_squares(5,4)\n\n41\n\n\nAnd, of course, we could chose a shorter name for the function as well, although I would recommend to always use function names that make clear what the function does:\n\nf = func_sum_of_squares\n\nf(5,4)\n\n41",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week 2"
    ]
  },
  {
    "objectID": "ex/02_python_primer.html#libraries-youll-love",
    "href": "ex/02_python_primer.html#libraries-youll-love",
    "title": "Exercise for Week 2",
    "section": "Libraries you’ll love",
    "text": "Libraries you’ll love\nLuckily, you don’t have to programm all functions by yourself. There is a huge community of Python programmers out there that works and collaborates on several libraries. A library is (more or less) simply a collection of certain functions (and some more, but we don’t get into this here). This means, instead of writing a function yourself, you can rely on functions that someone else has programmed.\n\n\n\n\n\n\nCaution\n\n\n\nWhether a Python library or function does actually do what it promises is another story. Popular libraries with tens of thousands of users are very trust-worthy because you can be almost sure that someone would have noticed erroneous behavior. But it is certainly possible that badly-maintained libraries contain errors. So be prudent when using the code of others.\n\n\n\nNumPy\nOne of the most popular Python libaries is NumPy for numerical computations. We will rely a lot on the functions in this library, especially in order to draw random samples—more on this later! To use the functions or variables from this library, they have to be imported so that you can use them. There are several ways to do this. For example, you can import the libary entirely:\n\nimport numpy\n\nNow, you can use the (approximated) value of \\(\\pi\\) stored in this library by typing\n\nnumpy.pi\n\n3.141592653589793\n\n\nA different way is to just import everything from the library by writing\n\nfrom numpy import * \n\nHere, the * stands for ‘everything’. Now, to use the value of \\(\\pi\\) we could simply type\n\npi\n\n3.141592653589793\n\n\nThis is, however discouraged for the following reason: imagine we had another library, numpy2 that also stores the value of \\(\\pi\\), but less precisely (e.g. only as 3.14). If we write\n\nfrom numpy import * \nfrom numpy2 import * \n\nWe would have imported the variables holding the value of \\(\\pi\\) from both libraries. But, because they have the same name pi. In this case, pi would equal 3.14 because we imported numpy2 last. This is confusing and shouldn’t be so! To avoid this, it is better to keep references to imported libraries explicit. In order not to have to type too much (we’re all lazy, after all), we can define an alias for the library.\n\nimport numpy as np\nnp.pi\n\n3.141592653589793\n\n\nAll functions of NumPy are now accessible with the prefix np..\n\n\nPandas\n[TODO]\n\n\nMatplotlib\n[TODO]\n\n\nSummary\nYou can choose any alias when importing a library (it can even by longer than the library name) but certain conventions have emerged that you’re encouraged to follow. Importing the most commonly used Python libraries for data-science tasks (“The data science triad”), use the following:\n\nimport numpy as np # for numerical computations\nimport pandas as pd # for tabular data \nimport matplotlib.pyplot as plt # for data visualization\n\nWe will use all three of them in the following chapters and you’ll learn to love them.\n\n\n\n\n\n\nConcepts covered\n\n\n\n\nvariables\ntypes (integers, strings, lists, dictionaries)\nfunctions\nlibraries, importing and aliases",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week 2"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html",
    "href": "ex/03_melody_I.html",
    "title": "Exercise for Week X",
    "section": "",
    "text": "Finding Similarities and Differences in Folk Melodies with Python and Pandas\nIn this exercise we will have a look at the Essen Folksong Collection (EFD). It is a database of Folksongs from all around the world gathered by the ethnomusicologist Helmut Schaffrath (1995). The collection can be downloaded from this website. It comes in the **kern format that is a table of note events where the rows correspond to event time.\nWe will answer a very specific question:\nWhat can we learn about musical scales when we look at the notes in a musical piece?\nFor this purpose and also due to limited time, we need to make some simplifications. We will only consider note counts. That means, for this excercise we just count all the notes in a piece but do not care how long the notes are. Another way to say it is that we are just interested in the pitch dimension.\nNotes in the EFD come as spelled pitches. Spelled pitches have three parts: 1. the diatonic step (C, D, E, F, G, A, or B) 2. possibly one or two accidentals (# or b) 3. the octave in which the note sounds\nMoreover, we will reduce the pitches in a melody to pitch classe. This means we do not differentiate between the same pitches in different octaves, e.g. C4 and C5, nor will we distinguish between enharmonically equivalent pitches, such as F#3 and Gb3.\nThis way, each piece can be represented as a list of pitch classes that can then be counted.\nBecause it is not straight-forward to work with **kern scores in Python, the data was already transformed into DataFrame format and exported to data.csv.\nFirst, we need to import sum libraries that contain the functions that we will use for the data analysis. If you do not have one or more of the packages installed, run conda install packages-names in a command-line tool (as adminstrator).\nimport numpy as np # for numerical computation (we need it to transpose melodies)\nimport pandas as pd # to organize our data in tabular format\n\nimport matplotlib.pyplot as plt # to visualize results \nimport seaborn as sns # more advanced visualization tools\n# directly show the plots in the notebook\n%matplotlib inline \n\nimport re # regular expressions: for pattern finding in strings",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html#finding-similarities-and-differences-in-folk-melodies-with-python-and-pandas",
    "href": "ex/03_melody_I.html#finding-similarities-and-differences-in-folk-melodies-with-python-and-pandas",
    "title": "Exercise for Week X",
    "section": "",
    "text": "Step 1: Preprocessing\n\nRead the data and create new column that contains a list of the pitches of the melody\n\ndata = pd.read_csv(\"../data/essen_data.csv\", sep='\\t', index_col=0)\ndata.head()\n\n\n\n\n\n\n\n\n\nregion\ntitle\nkey\nDGIs\nspelled_pitches\n\n\n\n\n0\nafrica\nMuwaschah Lamma Bada\ng minor\n['P4', 'M2', 'm2', 'M2', 'M-2', 'P1', 'm-2', '...\n['D5', 'G5', 'A5', 'B-5', 'C6', 'B-5', 'B-5', ...\n\n\n1\nmexico\nCUCA 1\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\n['C4', 'C4', 'C4', 'F4', 'A4', 'C4', 'C4', 'C4...\n\n\n2\nmexico\nCUCA 2\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\n['C4', 'C4', 'C4', 'F4', 'A4', 'C4', 'C4', 'C4...\n\n\n3\nmexico\nCUCA 1\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\n['C4', 'C4', 'C4', 'F4', 'A4', 'C4', 'C4', 'C4...\n\n\n4\nmexico\nCUCA 2\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\n['C4', 'C4', 'C4', 'F4', 'A4', 'C4', 'C4', 'C4...\n\n\n\n\n\n\n\n\nWee see that the (preprocessed) data comes with the features region, title, and key, and is represended as directed generic intervals (DGIs) and spelled_pitches.\nFor this excercise we will only work with the data in the key and spelled_pitches columns. The spelled_pitches entries look like a list of pitches but they are actually strings ('[', ',' and whitespaces are characters). Therefore we need to first transform it into a representation that we can work with. We will not go into details here but basically we just remove everything that we don’t need from the string until only the spelled pitches are left.\n\ndata['spelled_pitches'] = \\\n    data['spelled_pitches'].str.replace(\"', '\", \" \").str.replace(\"['\", \"\").str.replace(\"']\", \"\").str.strip()\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\nregion\ntitle\nkey\nDGIs\nspelled_pitches\n\n\n\n\n0\nafrica\nMuwaschah Lamma Bada\ng minor\n['P4', 'M2', 'm2', 'M2', 'M-2', 'P1', 'm-2', '...\nD5 G5 A5 B-5 C6 B-5 B-5 A5 A5 G5 G5 F#5 G5 A5 ...\n\n\n1\nmexico\nCUCA 1\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n2\nmexico\nCUCA 2\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n3\nmexico\nCUCA 1\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n4\nmexico\nCUCA 2\nF major\n['P1', 'P1', 'P4', 'M3', 'M-6', 'P1', 'P1', 'P...\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n\n\n\n\n\n\nLet’s also drop the columns that we don’t need for this excercise.\n\ndel data['region']\ndel data['DGIs']\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\ntitle\nkey\nspelled_pitches\n\n\n\n\n0\nMuwaschah Lamma Bada\ng minor\nD5 G5 A5 B-5 C6 B-5 B-5 A5 A5 G5 G5 F#5 G5 A5 ...\n\n\n1\nCUCA 1\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n2\nCUCA 2\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n3\nCUCA 1\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n4\nCUCA 2\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\n\n\n\n\n\n\n\n\nLet us inspect the data bit further with the describe method.\n\ndata.describe()\n\n\n\n\n\n\n\n\n\ntitle\nkey\nspelled_pitches\n\n\n\n\ncount\n9373\n9372\n9373\n\n\nunique\n7783\n22\n9015\n\n\ntop\nMelodia instrumentalna\nG major\nB3 E4 F#4 G4 F#4 E4 B4 F#4 A4 G4 F#4 E4 D#4 E4...\n\n\nfreq\n84\n2831\n4\n\n\n\n\n\n\n\n\nIt seems that we have some duplicates with different titles in the dataset. Normally, we should deal with this issue. For now we will treat them as separate songs. Also we can see that apparently there is missing key information for one song. Let’s see where this happens.\n\ndata[ data['key'].isnull() ]\n\n\n\n\n\n\n\n\n\ntitle\nkey\nspelled_pitches\n\n\n\n\n1028\nYidui gezi xukongli fei\nNaN\n[]\n\n\n\n\n\n\n\n\nSince we have only the title for this song, it doesn’t make much sense to include it into our analysis. We will exclude it and drop this row from the DataFrame.\n\ndata.drop(1028, inplace=True)\n\n\ndata = data.reset_index(drop=True)\n\n\ndata.shape\n\n(9372, 3)",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html#step-2-extract-the-root-and-mode-of-the-pieces-and-write-them-in-new-columns",
    "href": "ex/03_melody_I.html#step-2-extract-the-root-and-mode-of-the-pieces-and-write-them-in-new-columns",
    "title": "Exercise for Week X",
    "section": "Step 2: Extract the root and mode of the pieces and write them in new columns",
    "text": "Step 2: Extract the root and mode of the pieces and write them in new columns\nTranslate the keys into modes and pitch classes of roots. The easiest way might be to write a dictionary by hand that does the job.\n\ndata[['root', 'mode']] = data['key'].str.split(\" \", expand=True)\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\ntitle\nkey\nspelled_pitches\nroot\nmode\n\n\n\n\n0\nMuwaschah Lamma Bada\ng minor\nD5 G5 A5 B-5 C6 B-5 B-5 A5 A5 G5 G5 F#5 G5 A5 ...\ng\nminor\n\n\n1\nCUCA 1\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\nF\nmajor\n\n\n2\nCUCA 2\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\nF\nmajor\n\n\n3\nCUCA 1\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\nF\nmajor\n\n\n4\nCUCA 2\nF major\nC4 C4 C4 F4 A4 C4 C4 C4 F4 A4 F4 F4 E4 E4 D4 D...\nF\nmajor\n\n\n\n\n\n\n\n\nIt is always a good idea to inspect the data to understand it better. What are the proportions of major and minor pieces in this dataset?\n\ndata['mode'].value_counts()\n\nmode\nmajor    8388\nminor     984\nName: count, dtype: int64\n\n\n\ndata['mode'].value_counts() / len(data)\n\nmode\nmajor    0.895006\nminor    0.104994\nName: count, dtype: float64\n\n\n\ndata['root'].value_counts()\n\nroot\nG     2831\nF     1878\nC     1309\nB-     664\nD      648\nA      553\ng      370\nE-     289\na      284\nd      170\nE      150\ne      109\nA-      56\nf       19\nc       17\nb-       8\nD-       6\nB        4\nd-       2\nf#       2\nb        2\ne-       1\nName: count, dtype: int64\n\n\nWe don’t want to distinguish between the roots of major and minor keys, so we just write all the roots as uppercase letters.\n\ndata['root'] = data['root'].str.upper()\n\n\ndata['root'].value_counts()\n\nroot\nG     3201\nF     1897\nC     1326\nA      837\nD      818\nB-     672\nE-     290\nE      259\nA-      56\nD-       8\nB        6\nF#       2\nName: count, dtype: int64\n\n\nIn order to transpose all the melodies to the same key, we need to know the pitch-class of each root. We will use a pragmatic approach and just explicitly state the information in a dictionary. It is common to define 'C' as pitch class 0.\n\nroots_dict = { \n    'G':7,\n    'F':5,\n    'C':0,\n    'A':9,\n    'D':2,\n    'B-':10,\n    'E-':3,\n    'E':4,\n    'A-':8,\n    'D-':1,\n    'B':11,\n    'F#':6\n    }\n\nNo we can translate the roots to pitch classes.\n\ndata['root'] = data['root'].map(roots_dict)\n\n\ndata.tail()\n\n\n\n\n\n\n\n\n\ntitle\nkey\nspelled_pitches\nroot\nmode\n\n\n\n\n9367\nDie schoene Magdalena Was geschah an einem Mon...\nG major\nD4 D4 G4 G4 G4 E5 D5 B4 G4 D5 D5 C5 A4 F#4 D5 ...\n7\nmajor\n\n\n9368\nDas Maedchen und der Faehnrich 'Ach Tochter, l...\nG major\nD4 G4 A4 B4 C5 D5 B4 G4 G4 E5 E5 G5 E5 E5 D5 D...\n7\nmajor\n\n\n9369\nDas Maedchen und der Faehnrich Es war ein reic...\nG major\nB4 D5 B4 D5 E5 D5 C5 B4 C5 D5 B4 A4 A4 G4 A4 D...\n7\nmajor\n\n\n9370\nDer schwatzhafte Junggeselle Es waren drei Ges...\nF major\nC4 F4 E4 D4 C4 A4 G4 F4 C4 F4 E4 D4 C4 A4 G4 F...\n5\nmajor\n\n\n9371\nVerschlafener Jaeger Es wollt ein Jaeger frueh...\nA major\nE4 A4 E4 E4 F#4 G#4 A4 F#4 F#4 F#4 F#4 A4 G#4 ...\n9\nmajor",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html#step-3-create-one-new-column-for-each-pitch-class-in-order-to-extract-pitch-class-counts",
    "href": "ex/03_melody_I.html#step-3-create-one-new-column-for-each-pitch-class-in-order-to-extract-pitch-class-counts",
    "title": "Exercise for Week X",
    "section": "Step 3: Create one new column for each pitch class in order to extract pitch-class counts",
    "text": "Step 3: Create one new column for each pitch class in order to extract pitch-class counts\nFirst, we transform the melody into a list of spelled pitches.\n\ndata['spelled_pitches'] = data['spelled_pitches'].str.split()\n\n\ndata.head()\n\n\n\n\n\n\n\n\n\ntitle\nkey\nspelled_pitches\nroot\nmode\n\n\n\n\n0\nMuwaschah Lamma Bada\ng minor\n[D5, G5, A5, B-5, C6, B-5, B-5, A5, A5, G5, G5...\n7\nminor\n\n\n1\nCUCA 1\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n\n\n2\nCUCA 2\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n\n\n3\nCUCA 1\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n\n\n4\nCUCA 2\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n\n\n\n\n\n\n\n\nNext, we need a way to transform each pitch to a pitch class. To that end we define a function that takes a spelled pitch (a symbol such as B-5) and returns it’s pitch class as a number between 0 and 12.\n\ndef spelled_pitch_to_pitch_class(spelled_pitch):\n    \"\"\"\n    This function transforms a spelled pitch, such as, `B-5` \n    into a pitch class, a number between 0 and 12.\n    \n    A spelled pitch consists of three parts:\n    1. Its diatonic step (C, D, E, F, G, A, or B)\n    2. Potentially one or two accidentals (# or b)\n    3. Its octave as a number.\n    \"\"\"\n    \n    # Remove octave by removing the last character in the string\n    spelled_pitch_class = spelled_pitch[:-1]\n    \n    # Extract the diatonic step\n    # First, we define a dictionary that associates \n    # each diatonic step with a pitch class\n    \n    pitch_classes = {\n        'C':0,\n        'D':2,\n        'E':4,\n        'F':5,\n        'G':7,\n        'A':9,\n        'B':11\n    }\n    \n    # Extract accidentals\n    # We define a regular expression that finds the three parts \n    # of a spelled pitch class. \n    match = re.match(r'(\\w)(\\#*)(-*)', spelled_pitch_class).group(1,2,3)\n    \n    # If we find a match, we get a tripel (step, sharps, flats)\n    if match:\n        step = pitch_classes[match[0]]\n        sharps = len(match[1])\n        flats = len(match[2])\n    \n    # The only thing left to do is to take the pitch class of the diatonic step,\n    # add the number of sharps and subtract the number of flats\n    # Finally, since pitch classes are always between 0 and 11, we take this number mod 12.\n    return (step + sharps - flats) % 12\n\nIn the previous step we set up a function that converts spelled pitches into pitch classes. Now we can use it to count all the notes in a piece.\n\n# First we set up an empty list that will later contain dictionaries of pitch-class counts for each song. \ncountdicts = []\n\n# Then we loop over all the rows (pieces) in our dataframe \nfor index, row in data.iterrows():\n    \n    # We replace the spelled pitches with pitch classes\n    row['spelled_pitches'] = [spelled_pitch_to_pitch_class(pitch) for pitch in row['spelled_pitches']]\n    \n    # Then we count the occurences of each pitch class in this list\n    # We create an empty dictionary that will contain the pitch-class counts for the current piece\n    intcounts = {}\n    \n    # We iterate over all pitch classes and see if it is already in the `intcounts` dictionary\n    for pitch_class in row['spelled_pitches']:\n        # if not, set the count to 1\n        if pitch_class not in intcounts.keys():\n            intcounts[pitch_class] = 1\n        # if yes, increment the count by 1\n        else:\n            intcounts[pitch_class] += 1\n    # Finally, add the pitch-class counts dictionary to our list of pitch-class count dictionaries\n    countdicts.append(intcounts)\n\nThis is what the first 10 entries in the list of pitch-class count dictionaries looks like:\n\ncountdicts[:10]\n\n[{2: 17, 7: 32, 9: 29, 10: 23, 0: 11, 6: 12, 3: 8, 5: 2, 4: 1},\n {0: 25, 5: 7, 9: 9, 4: 6, 2: 5, 7: 8, 10: 5},\n {0: 25, 5: 8, 9: 11, 4: 6, 2: 5, 7: 10, 10: 5},\n {0: 25, 5: 7, 9: 9, 4: 6, 2: 5, 7: 8, 10: 5},\n {0: 25, 5: 8, 9: 11, 4: 6, 2: 5, 7: 10, 10: 5},\n {0: 4, 5: 3, 9: 1, 2: 4, 7: 3, 4: 1},\n {5: 11, 0: 5, 7: 6, 9: 10, 10: 1},\n {2: 10, 11: 15, 9: 12, 7: 7, 6: 2, 0: 2},\n {7: 9, 11: 8, 9: 11, 2: 8, 0: 4, 6: 2, 4: 2},\n {7: 8, 9: 10, 11: 17, 0: 8, 2: 6, 4: 2}]\n\n\nThis is not really convenient. To handle it easier, we transform it to a DataFrame object and set all pitch classes to 0 if they do not occur in a piece.\n\ncounts = pd.DataFrame(countdicts).fillna(0)\n\n\ncounts.head(10)\n\n\n\n\n\n\n\n\n\n2\n7\n9\n10\n0\n6\n3\n5\n4\n11\n8\n1\n\n\n\n\n0\n17.0\n32.0\n29.0\n23.0\n11.0\n12.0\n8.0\n2.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n5.0\n8.0\n9.0\n5.0\n25.0\n0.0\n0.0\n7.0\n6.0\n0.0\n0.0\n0.0\n\n\n2\n5.0\n10.0\n11.0\n5.0\n25.0\n0.0\n0.0\n8.0\n6.0\n0.0\n0.0\n0.0\n\n\n3\n5.0\n8.0\n9.0\n5.0\n25.0\n0.0\n0.0\n7.0\n6.0\n0.0\n0.0\n0.0\n\n\n4\n5.0\n10.0\n11.0\n5.0\n25.0\n0.0\n0.0\n8.0\n6.0\n0.0\n0.0\n0.0\n\n\n5\n4.0\n3.0\n1.0\n0.0\n4.0\n0.0\n0.0\n3.0\n1.0\n0.0\n0.0\n0.0\n\n\n6\n0.0\n6.0\n10.0\n1.0\n5.0\n0.0\n0.0\n11.0\n0.0\n0.0\n0.0\n0.0\n\n\n7\n10.0\n7.0\n12.0\n0.0\n2.0\n2.0\n0.0\n0.0\n0.0\n15.0\n0.0\n0.0\n\n\n8\n8.0\n9.0\n11.0\n0.0\n4.0\n2.0\n0.0\n0.0\n2.0\n8.0\n0.0\n0.0\n\n\n9\n6.0\n8.0\n10.0\n0.0\n8.0\n0.0\n0.0\n0.0\n2.0\n17.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nThe DataFrame counts contains now the pitch-class counts for all pieces. We can see if the dimensions of counts and data fit.\n\ncounts.shape, data.shape\n\n((9372, 12), (9372, 5))\n\n\nBut now, longer pieces weight more just because they contain more notes. To avoid that we have to normalize the DataFrame to get relative frequencies.\n\nnormalized = counts.div(counts.sum(axis=1), axis=0)\nnormalized.head(10)\n\n\n\n\n\n\n\n\n\n2\n7\n9\n10\n0\n6\n3\n5\n4\n11\n8\n1\n\n\n\n\n0\n0.125926\n0.237037\n0.214815\n0.170370\n0.081481\n0.088889\n0.059259\n0.014815\n0.007407\n0.000000\n0.0\n0.0\n\n\n1\n0.076923\n0.123077\n0.138462\n0.076923\n0.384615\n0.000000\n0.000000\n0.107692\n0.092308\n0.000000\n0.0\n0.0\n\n\n2\n0.071429\n0.142857\n0.157143\n0.071429\n0.357143\n0.000000\n0.000000\n0.114286\n0.085714\n0.000000\n0.0\n0.0\n\n\n3\n0.076923\n0.123077\n0.138462\n0.076923\n0.384615\n0.000000\n0.000000\n0.107692\n0.092308\n0.000000\n0.0\n0.0\n\n\n4\n0.071429\n0.142857\n0.157143\n0.071429\n0.357143\n0.000000\n0.000000\n0.114286\n0.085714\n0.000000\n0.0\n0.0\n\n\n5\n0.250000\n0.187500\n0.062500\n0.000000\n0.250000\n0.000000\n0.000000\n0.187500\n0.062500\n0.000000\n0.0\n0.0\n\n\n6\n0.000000\n0.181818\n0.303030\n0.030303\n0.151515\n0.000000\n0.000000\n0.333333\n0.000000\n0.000000\n0.0\n0.0\n\n\n7\n0.208333\n0.145833\n0.250000\n0.000000\n0.041667\n0.041667\n0.000000\n0.000000\n0.000000\n0.312500\n0.0\n0.0\n\n\n8\n0.181818\n0.204545\n0.250000\n0.000000\n0.090909\n0.045455\n0.000000\n0.000000\n0.045455\n0.181818\n0.0\n0.0\n\n\n9\n0.117647\n0.156863\n0.196078\n0.000000\n0.156863\n0.000000\n0.000000\n0.000000\n0.039216\n0.333333\n0.0\n0.0\n\n\n\n\n\n\n\n\nNow we are almoste done with transforming the data in order to answer our question. We still need to transpose all songs into the same key so that we can compare their pitch-class distributions. Let’s think a moment about how this can be done. We have the pitch-class distribution of each song in counts, and we have key, root, and mode in data.\nLet’s say that we want to transpose all songs to the root C. C major and C minor pieces do not have to change. A piece in G major, for instance, has the root 7 and needs to be transposed to the root 0. A piece in Bb minor has the root 10 and needs do be transposed to the root 0. The easiest way to do this is to ‘rotate’ the pitch-class distributions by the negative amount of the root. Luckily, numpy provides the roll function to do exactly that. It takes an array (a vector) and rolls it by the specified amount. We do this for all songs in data and save the result in a new DataFrame transposed.\n\ntransposed = pd.DataFrame(\n    [ np.roll( normalized.iloc[i,:], -data['root'][i] ) for i in range(len(data)) ]\n    )\n\ntransposed.head(10)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n0\n0.014815\n0.007407\n0.000000\n0.000000\n0.0\n0.125926\n0.237037\n0.214815\n0.170370\n0.081481\n0.088889\n0.059259\n\n\n1\n0.000000\n0.000000\n0.107692\n0.092308\n0.0\n0.000000\n0.000000\n0.076923\n0.123077\n0.138462\n0.076923\n0.384615\n\n\n2\n0.000000\n0.000000\n0.114286\n0.085714\n0.0\n0.000000\n0.000000\n0.071429\n0.142857\n0.157143\n0.071429\n0.357143\n\n\n3\n0.000000\n0.000000\n0.107692\n0.092308\n0.0\n0.000000\n0.000000\n0.076923\n0.123077\n0.138462\n0.076923\n0.384615\n\n\n4\n0.000000\n0.000000\n0.114286\n0.085714\n0.0\n0.000000\n0.000000\n0.071429\n0.142857\n0.157143\n0.071429\n0.357143\n\n\n5\n0.000000\n0.000000\n0.187500\n0.062500\n0.0\n0.000000\n0.000000\n0.250000\n0.187500\n0.062500\n0.000000\n0.250000\n\n\n6\n0.000000\n0.000000\n0.333333\n0.000000\n0.0\n0.000000\n0.000000\n0.000000\n0.181818\n0.303030\n0.030303\n0.151515\n\n\n7\n0.000000\n0.000000\n0.312500\n0.000000\n0.0\n0.208333\n0.145833\n0.250000\n0.000000\n0.041667\n0.041667\n0.000000\n\n\n8\n0.000000\n0.045455\n0.181818\n0.000000\n0.0\n0.181818\n0.204545\n0.250000\n0.000000\n0.090909\n0.045455\n0.000000\n\n\n9\n0.000000\n0.039216\n0.333333\n0.000000\n0.0\n0.117647\n0.156863\n0.196078\n0.000000\n0.156863\n0.000000\n0.000000\n\n\n\n\n\n\n\n\nWe can already observe that none of the first 10 songs in the dataset has the tritone (pitch class 6), and only one has a minor third (pitch class 3) or a minor seventh (pitch class 10).\nIt would be nice not having to work with to DataFrames, data and transposed, so we combine (concatenate) them in a new one, just called df.\n\ndf = pd.concat([data, transposed], axis=1)\n\n\ndf.shape\n\n(9372, 17)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\ntitle\nkey\nspelled_pitches\nroot\nmode\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n0\nMuwaschah Lamma Bada\ng minor\n[D5, G5, A5, B-5, C6, B-5, B-5, A5, A5, G5, G5...\n7\nminor\n0.014815\n0.007407\n0.000000\n0.000000\n0.0\n0.125926\n0.237037\n0.214815\n0.170370\n0.081481\n0.088889\n0.059259\n\n\n1\nCUCA 1\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n0.000000\n0.000000\n0.107692\n0.092308\n0.0\n0.000000\n0.000000\n0.076923\n0.123077\n0.138462\n0.076923\n0.384615\n\n\n2\nCUCA 2\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n0.000000\n0.000000\n0.114286\n0.085714\n0.0\n0.000000\n0.000000\n0.071429\n0.142857\n0.157143\n0.071429\n0.357143\n\n\n3\nCUCA 1\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n0.000000\n0.000000\n0.107692\n0.092308\n0.0\n0.000000\n0.000000\n0.076923\n0.123077\n0.138462\n0.076923\n0.384615\n\n\n4\nCUCA 2\nF major\n[C4, C4, C4, F4, A4, C4, C4, C4, F4, A4, F4, F...\n5\nmajor\n0.000000\n0.000000\n0.114286\n0.085714\n0.0\n0.000000\n0.000000\n0.071429\n0.142857\n0.157143\n0.071429\n0.357143",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html#step-4-plot-your-first-pitch-class-histogram-and-pitch-class-distribution",
    "href": "ex/03_melody_I.html#step-4-plot-your-first-pitch-class-histogram-and-pitch-class-distribution",
    "title": "Exercise for Week X",
    "section": "Step 4: Plot your first pitch class histogram and pitch class distribution",
    "text": "Step 4: Plot your first pitch class histogram and pitch class distribution\n\nChoose an example piece.\nWhat do you expect to see?\nPlot a pitch class histogram in chromatic order.\n\n\npiece = df.iloc[0,-11:]\npiece.plot.bar(rot=0);",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html#step-5-plot-the-averaged-pitch-class-distribution-for-the-major-and-the-minor-mode",
    "href": "ex/03_melody_I.html#step-5-plot-the-averaged-pitch-class-distribution-for-the-major-and-the-minor-mode",
    "title": "Exercise for Week X",
    "section": "Step 5: Plot the averaged pitch class distribution for the major and the minor mode",
    "text": "Step 5: Plot the averaged pitch class distribution for the major and the minor mode\n\nPlot the averaged distributions.\nCan you show everything in one figure?\nWould it also make sense to plot averaged pitch class histograms?\n\n\nmelted = df.melt(id_vars='mode',\n                 value_vars=[0,1,2,3,4,5,6,7,8,9,10,11],\n                 var_name='pitch_classes',\n                 value_name='relative_frequencies'\n                )\n\n\nmelted.head()\n\n\n\n\n\n\n\n\n\nmode\npitch_classes\nrelative_frequencies\n\n\n\n\n0\nminor\n0\n0.014815\n\n\n1\nmajor\n0\n0.000000\n\n\n2\nmajor\n0\n0.000000\n\n\n3\nmajor\n0\n0.000000\n\n\n4\nmajor\n0\n0.000000\n\n\n\n\n\n\n\n\n\nmelted.shape\n\n(112464, 3)\n\n\n\nsns.catplot(data=melted, \n               x='pitch_classes', \n               y='relative_frequencies', \n               hue='mode',\n               kind='bar',\n               aspect=2.5\n              );\n\nplt.show()",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html#step-6-plot-the-averaged-distributions-in-fifths-ordering",
    "href": "ex/03_melody_I.html#step-6-plot-the-averaged-distributions-in-fifths-ordering",
    "title": "Exercise for Week X",
    "section": "Step 6: Plot the averaged distributions in fifths ordering",
    "text": "Step 6: Plot the averaged distributions in fifths ordering\n\nCreate the plot.\nWhat do you see?\n\n\nmelted['fifths'] = melted['pitch_classes'] * 7 % 12\n\n\nsns.catplot(data=melted, \n               x='fifths', \n               y='relative_frequencies', \n               hue='mode',\n               kind='bar',\n               aspect=2.5\n              );",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "ex/03_melody_I.html#step-7-extend-the-plot-above-to-show-the-diffusion-of-each-pitch-class",
    "href": "ex/03_melody_I.html#step-7-extend-the-plot-above-to-show-the-diffusion-of-each-pitch-class",
    "title": "Exercise for Week X",
    "section": "Step 7: Extend the plot above to show the diffusion of each pitch class",
    "text": "Step 7: Extend the plot above to show the diffusion of each pitch class\n\nDecide to either use error bars, boxplots, or violin plots. What is the difference between them? Violin plots are of cause the most fancy figures…\nDescribe what you see.\n\n\nplt.figure(figsize=(12,10))\nsns.boxplot(\n    data=melted,\n    x='pitch_classes',\n    y='relative_frequencies',\n    hue='mode',\n    fliersize=2.5\n);\n\n\n\n\n\n\n\n\nThese boxplots show already much more! For example, they reveal that there are many outliers which we can’t see in the bar plot. https://www.autodeskresearch.com/publications/samestats\n\nplt.figure(figsize=(12,10))\nsns.violinplot(\n    data=melted,\n    x='pitch_classes',\n    y='relative_frequencies',\n    hue='mode',\n    inner='quart',\n    split=True\n);",
    "crumbs": [
      "EXERCISES",
      "Exercise for Week X"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "REFERENCES",
    "section": "",
    "text": "Budge, H. (1943). A Study of Chord Frequencies\nBased on Music of Representative\nComposers of the Eighteenth and\nNineteenth centuries [PhD thesis]. Columbia\nUniversity.\n\n\nBurgoyne, J. A., Fujinaga, I., & Downie, J. S. (2015). Music\nInformation Retrieval. In A New Companion\nto Digital Humanities (pp. 213–228). John Wiley &\nSons, Ltd. https://doi.org/10.1002/9781118680605.ch15\n\n\nEerola, T. (2025). Music and science: A guide to empirical music\nresearch. Routledge.\n\n\nInskip, C., & Wiering, F. (2015, October). In their own words: Using\ntext analysis to identify musicologists’ attitudes towards technology.\nProceedings of the 16th International Society for\nMusic Information Retrieval Conference,\nMalaga, Spain. (2015).\n\n\nJeppesen, K. (1927). The Style of\nPalestrina and the Dissonance (1st ed.).\nOxford University Press.\n\n\nMüller, M. (2015). Fundamentals of Music Processing:\nAudio, Analysis, Algorithms,\nApplications. Springer International Publishing. https://doi.org/10.1007/978-3-319-21945-5\n\n\nNorman, P. B. (1945). A Quantitative Study of\nHarmonic Similarities in Certain Specified\nWorks of Bach, Beethoven, and\nWagner. C. Fischer, Incorporated.\n\n\nPugin, L. (2015). The Challenge of Data in\nDigital Musicology. Frontiers in Digital\nHumanities, 2, 1–3. https://doi.org/10.3389/fdigh.2015.00004\n\n\nSchaffer, K. (2016). What is computational musicology? https://medium.com/@krisshaffer/what-is-computational-musicology-f25ee0a65102\n\n\nSethares, W. A. (2005). Tuning, Timbre,\nSpectrum, Scale (2nd ed.). Springer.\n\n\nShanahan, D., Burgoyne, J. A., & Quinn, I. (Eds.). (2022).\nOxford Handbook of Music and Corpus\nStudies. Oxford University Press.\n\n\nWiering, F., & Inskip, C. (2025). The impact of the pandemic on\nmusicologists’ use of technology. Digital Humanities Quarterly,\n019(2). https://dhq.digitalhumanities.org/vol/19/2/000786/000786.html\n\n\nWing, J. M. (2006). Computational thinking. Commun. ACM,\n49(3), 33–35. https://doi.org/10.1145/1118178.1118215",
    "crumbs": [
      "REFERENCES"
    ]
  }
]