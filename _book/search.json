[
  {
    "objectID": "billboard_charts.html",
    "href": "billboard_charts.html",
    "title": "7  Analyzing song survival",
    "section": "",
    "text": "In this session, we will analyze songs from the Billboard 100 charts and trace their ‘course of life’ in the charts.\nThe data was obtained from Kaggle, a large community website for data analysis challenges.\nAs before, we first import the pandas library for data analysis and load the data using the read_csv fundtion that takes as its main argument the path to the data file, in our case charts.csv.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"data/charts.csv\")\n\nInspecting the first 5 lines with the .head() method of pandas DataFrames, we obtain an understanding of the structure of the data.\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nrank\nsong\nartist\nlast-week\npeak-rank\nweeks-on-board\n\n\n\n\n0\n2021-11-06\n1\nEasy On Me\nAdele\n1.0\n1\n3\n\n\n1\n2021-11-06\n2\nStay\nThe Kid LAROI & Justin Bieber\n2.0\n1\n16\n\n\n2\n2021-11-06\n3\nIndustry Baby\nLil Nas X & Jack Harlow\n3.0\n1\n14\n\n\n3\n2021-11-06\n4\nFancy Like\nWalker Hayes\n4.0\n3\n19\n\n\n4\n2021-11-06\n5\nBad Habits\nEd Sheeran\n5.0\n2\n18\n\n\n\n\n\n\n\nThink: What do the columns represent? Provide verbal descriptions of their meaning and write it down.\nAfter this general overview, we might want to achieve a slightly deeper understanding. For instance, it is not difficult to interpret the date column, but from only the first few entries, we cannot know the temporal extend of our data.\nLet’s find out what the earliest and latest dates are using the .min() and .max() methods, respectively.\n\ndf[\"date\"].min(), df[\"date\"].max()\n\n('1958-08-04', '2021-11-06')\n\n\nThis tells us that the data stored in charts.csv runs from August 1958 to November 2021 and thus allows us to trace the movement of songs in the Billboard charts across more than 60 years.\n\n# Top artists\ndf.artist.value_counts()\n\nartist\nTaylor Swift                                                    1023\nElton John                                                       889\nMadonna                                                          857\nDrake                                                            787\nKenny Chesney                                                    769\n                                                                ... \nYoungBoy Never Broke Again Featuring Sherhonda Gaulden             1\nDrake Featuring Chris Brown                                        1\nKehlani Featuring Jhene Aiko                                       1\nDaBaby Featuring A Boogie Wit da Hoodie & London On Da Track       1\nThe Shins                                                          1\nName: count, Length: 10205, dtype: int64\n\n\n\n# Longest in charts\ndf.sort_values(by=\"weeks-on-board\", ascending=True).iloc[50_000:]\n\n\n\n\n\n\n\n\ndate\nrank\nsong\nartist\nlast-week\npeak-rank\nweeks-on-board\n\n\n\n\n213768\n1980-11-22\n69\nTurn And Walk Away\nThe Babys\n79.0\n69\n2\n\n\n106440\n2001-06-16\n41\nFill Me In\nCraig David\n69.0\n41\n2\n\n\n106443\n2001-06-16\n44\nBootylicious\nDestiny's Child\n66.0\n44\n2\n\n\n106448\n2001-06-16\n49\nAll Or Nothing\nO-Town\n60.0\n49\n2\n\n\n213674\n1980-11-29\n75\nMy Mother's Eyes\nBette Midler\n85.0\n75\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n39148\n2014-05-10\n49\nRadioactive\nImagine Dragons\n48.0\n3\n87\n\n\n1215\n2021-08-14\n16\nBlinding Lights\nThe Weeknd\n17.0\n1\n87\n\n\n1117\n2021-08-21\n18\nBlinding Lights\nThe Weeknd\n16.0\n1\n88\n\n\n1020\n2021-08-28\n21\nBlinding Lights\nThe Weeknd\n18.0\n1\n89\n\n\n919\n2021-09-04\n20\nBlinding Lights\nThe Weeknd\n21.0\n1\n90\n\n\n\n\n280087 rows × 7 columns\n\n\n\n\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n\ndf[df.artist==\"Drake\"].song.value_counts()\n\nsong\nHotline Bling     36\nGod's Plan        36\nControlla         26\nFake Love         25\nNice For What     25\n                  ..\nTrust Issues       1\nToo Much           1\nOwn It             1\nTuscan Leather     1\nCome Thru          1\nName: count, Length: 108, dtype: int64\n\n\n\ndf[df.artist==\"Elton John\"].song.value_counts()\n\nsong\nCandle In The Wind 1997/Something About The Way You Look Tonight               42\nCan You Feel The Love Tonight (From \"The Lion King\")                           26\nI Guess That's Why They Call It The Blues                                      23\nThe One                                                                        22\nCandle In The Wind                                                             21\nLittle Jeannie                                                                 21\nThe Last Song                                                                  20\nRecover Your Soul                                                              20\nBelieve                                                                        20\nCircle Of Life (From \"The Lion King\")                                          20\nBlessed                                                                        20\nSad Songs (say So Much)                                                        19\nI Don't Wanna Go On With You Like That                                         18\nNikita                                                                         18\nBennie And The Jets                                                            18\nMama Can't Buy You Love                                                        18\nBlue Eyes                                                                      18\nYou Can Make History (Young Again)                                             17\nSacrifice                                                                      17\nEmpty Garden (Hey Hey Johnny)                                                  17\nCrocodile Rock                                                                 17\nGoodbye Yellow Brick Road                                                      17\nI'm Still Standing                                                             16\nClub At The End Of The Street                                                  16\nSimple Life                                                                    16\nSomeday Out Of The Blue                                                        15\nDon't Let The Sun Go Down On Me                                                15\nIsland Girl                                                                    15\nDaniel                                                                         15\nRocket Man                                                                     15\nHealing Hands                                                                  15\nThe Bitch Is Back                                                              14\nWho Wears These Shoes?                                                         14\nWrap Her Up                                                                    14\nSorry Seems To Be The Hardest Word                                             14\nLucy In The Sky With Diamonds                                                  14\nYour Song                                                                      14\nNobody Wins                                                                    13\nA Word In Spanish                                                              13\nSomeone Saved My Life Tonight                                                  13\nYou Gotta Love Someone                                                         13\nIn Neon                                                                        13\nChloe                                                                          13\n(Sartorial Eloquence) Don't Ya Wanna Play This Game No More?                   12\nKiss The Bride                                                                 12\nSaturday Night's Alright For Fighting                                          12\nGrow Some Funk Of Your Own/I Feel Like A Bullet (In The Gun Of Robert Ford)    11\nMade In England                                                                10\nLevon                                                                          10\nVictim Of Love                                                                 10\nPart-Time Love                                                                 10\nHonky Cat                                                                      10\nFriends                                                                         9\nHeartache All Over The World                                                    8\nEgo                                                                             8\nTiny Dancer                                                                     7\nBite Your Lip (Get up and dance!)                                               6\nBorder Song                                                                     5\nName: count, dtype: int64\n\n\n\ndef chart_performance(artist, song):\n    data = df[(df[\"artist\"] == artist) & (df[\"song\"] == song)]\n    data = data.sort_values(by=\"date\").reset_index(drop=True)\n    data[\"date_rel\"] = pd.to_timedelta(data[\"date\"] - data[\"date\"][0]).dt.days\n    return data\n\n\ntest_cases = {\n    \"Taylor Swift\": \"You Belong With Me\",\n    \"Drake\": \"God's Plan\",\n    \"Elton John\": \"Candle In The Wind 1997/Something About The Way You Look Tonight\",\n    \"The Weeknd\": \"Blinding Lights\",\n    \"Elvis Presley\": \"Please Don't Stop Loving Me\"\n}\n\n\ntaylor = chart_performance(\"Taylor Swift\", \"You Belong With Me\")\ndrake = chart_performance(\"Drake\", \"God's Plan\")\nelton = chart_performance(\"Elton John\", \"Candle In The Wind 1997/Something About The Way You Look Tonight\")\nweeknd = chart_performance(\"The Weeknd\", \"Blinding Lights\")\nelvis = chart_performance(\"Elvis Presley\", \"Please Don't Stop Loving Me\")\n\n\n_, ax = plt.subplots(figsize=(15,4))\n\nfor artist, song in test_cases.items():\n    data = chart_performance(artist, song)\n    x = data[\"date\"].values\n    y = data[\"rank\"].values\n\n    ax.plot(x, y, marker=\".\", label=f\"{song} ({artist})\")\n\nplt.gca().invert_yaxis()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n_, ax = plt.subplots(figsize=(15,4))\n\nfor artist, song in test_cases.items():\n    data = chart_performance(artist, song)\n    x = data[\"date_rel\"].values\n    y = data[\"rank\"].values\n\n    ax.plot(x, y, marker=\".\", label=f\"{song} ({artist})\")\n\nplt.gca().invert_yaxis()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# TODO: remove lines for missing weeks (gaps in curves)\n# add two cases:\n#  - short duration but high peak\n#  - long duration but low peak\n\n\n# Q: can we predict a song's survival using the features given in the data?\n# --&gt; at least introduce notion of training/test data and discuss the epistemological problem of using 'all' historical \n# sources for explanation\n\n\n# Try other data: https://www.kaggle.com/datasets/thedevastator/billboard-hot-100-audio-features\n\n\ndf_charts = pd.read_csv(\"Hot Stuff.csv\", index_col=0)\ndf_charts[\"WeekID\"] = pd.to_datetime(df_charts[\"WeekID\"])\n\n\ndf_charts.head()\n\n\n\n\n\n\n\n\nurl\nWeekID\nWeek Position\nSong\nPerformer\nSongID\nInstance\nPrevious Week Position\nPeak Position\nWeeks on Chart\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-07-17\n34\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n45.0\n34\n4\n\n\n1\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-07-24\n22\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n34.0\n22\n5\n\n\n2\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-07-31\n14\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n22.0\n14\n6\n\n\n3\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-08-07\n10\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n14.0\n10\n7\n\n\n4\nhttp://www.billboard.com/charts/hot-100/1965-0...\n1965-08-14\n8\nDon't Just Stand There\nPatty Duke\nDon't Just Stand TherePatty Duke\n1\n10.0\n8\n8\n\n\n\n\n\n\n\n\ndf_audio = pd.read_csv(\"Hot 100 Audio Features.csv\", index_col=0)\n\n\ndf_audio.head()\n\n\n\n\n\n\n\n\nSongID\nPerformer\nSong\nspotify_genre\nspotify_track_id\nspotify_track_preview_url\nspotify_track_duration_ms\nspotify_track_explicit\nspotify_track_album\ndanceability\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nspotify_track_popularity\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n-twistin'-White Silver SandsBill Black's Combo\nBill Black's Combo\n-twistin'-White Silver Sands\n[]\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n¿Dònde Està Santa Claus? (Where Is Santa Claus...\nAugie Rios\n¿Dònde Està Santa Claus? (Where Is Santa Claus?)\n['novelty']\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n......And Roses And RosesAndy Williams\nAndy Williams\n......And Roses And Roses\n['adult standards', 'brill building pop', 'eas...\n3tvqPPpXyIgKrm4PR9HCf0\nhttps://p.scdn.co/mp3-preview/cef4883cfd1e0e53...\n166106.0\nFalse\nThe Essential Andy Williams\n0.154\n...\n-14.063\n1.0\n0.0315\n0.91100\n0.000267\n0.112\n0.150\n83.969\n4.0\n38.0\n\n\n3\n...And Then There Were DrumsSandy Nelson\nSandy Nelson\n...And Then There Were Drums\n['rock-and-roll', 'space age pop', 'surf music']\n1fHHq3qHU8wpRKHzhojZ4a\nNaN\n172066.0\nFalse\nCompelling Percussion\n0.588\n...\n-17.278\n0.0\n0.0361\n0.00256\n0.745000\n0.145\n0.801\n121.962\n4.0\n11.0\n\n\n4\n...Baby One More TimeBritney Spears\nBritney Spears\n...Baby One More Time\n['dance pop', 'pop', 'post-teen pop']\n3MjUtNVVq3C8Fn0MP3zhXa\nhttps://p.scdn.co/mp3-preview/da2134a161f1cb34...\n211066.0\nFalse\n...Baby One More Time (Digital Deluxe Version)\n0.759\n...\n-5.745\n0.0\n0.0307\n0.20200\n0.000131\n0.443\n0.907\n92.960\n4.0\n77.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\nd = df_charts.merge(df_audio)\n\n\nd.shape\n\n(330208, 29)\n\n\n\nd[\"WeekID\"] = pd.to_datetime(d[\"WeekID\"])\n\n\nd.sample(10)\n\n\n\n\n\n\n\n\nurl\nWeekID\nWeek Position\nSong\nPerformer\nSongID\nInstance\nPrevious Week Position\nPeak Position\nWeeks on Chart\n...\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nspotify_track_popularity\n\n\n\n\n275610\nhttp://www.billboard.com/charts/hot-100/1964-0...\n1964-06-20\n85\nMy Dreams\nBrenda Lee\nMy DreamsBrenda Lee\n1\n96.0\n85\n3\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n189145\nhttp://www.billboard.com/charts/hot-100/2014-0...\n2014-02-08\n97\nRadio\nDarius Rucker\nRadioDarius Rucker\n1\n71.0\n65\n15\n...\n-6.836\n1.0\n0.0833\n0.11900\n0.000000\n0.0606\n0.965\n185.939\n4.0\n48.0\n\n\n245340\nhttp://www.billboard.com/charts/hot-100/1969-0...\n1969-08-02\n92\nLet's Call It A Day Girl\nBobby Vee\nLet's Call It A Day GirlBobby Vee\n1\nNaN\n92\n1\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n141579\nhttp://www.billboard.com/charts/hot-100/2009-0...\n2009-08-08\n3\nKnock You Down\nKeri Hilson Featuring Kanye West & Ne-Yo\nKnock You DownKeri Hilson Featuring Kanye West...\n1\n4.0\n3\n18\n...\n-4.781\n1.0\n0.1860\n0.01240\n0.000000\n0.1770\n0.671\n155.171\n4.0\n66.0\n\n\n117570\nhttp://www.billboard.com/charts/hot-100/1960-0...\n1960-02-20\n93\nSleepy Lagoon\nThe Platters\nSleepy LagoonThe Platters\n1\nNaN\n93\n1\n...\n-15.519\n1.0\n0.0302\n0.89400\n0.002600\n0.1160\n0.541\n68.994\n4.0\n33.0\n\n\n150750\nhttp://www.billboard.com/charts/hot-100/1966-0...\n1966-09-17\n36\nFlamingo\nHerb Alpert & The Tijuana Brass\nFlamingoHerb Alpert & The Tijuana Brass\n1\n46.0\n36\n3\n...\n-9.728\n1.0\n0.0303\n0.52000\n0.765000\n0.0745\n0.877\n142.111\n4.0\n15.0\n\n\n23582\nhttp://www.billboard.com/charts/hot-100/1985-1...\n1985-10-05\n63\nSoul Kiss\nOlivia Newton-John\nSoul KissOlivia Newton-John\n1\nNaN\n63\n1\n...\n-15.599\n1.0\n0.0298\n0.00708\n0.001260\n0.1130\n0.830\n103.222\n4.0\n22.0\n\n\n21493\nhttp://www.billboard.com/charts/hot-100/2003-0...\n2003-04-12\n21\nRock Your Body\nJustin Timberlake\nRock Your BodyJustin Timberlake\n1\n28.0\n21\n4\n...\n-6.055\n0.0\n0.1400\n0.20200\n0.000234\n0.0521\n0.818\n100.972\n4.0\n73.0\n\n\n71236\nhttp://www.billboard.com/charts/hot-100/2008-1...\n2008-12-13\n52\nMy Life\nThe Game Featuring Lil Wayne\nMy LifeThe Game Featuring Lil Wayne\n1\n46.0\n21\n17\n...\n-5.093\n0.0\n0.3560\n0.07730\n0.000000\n0.0877\n0.382\n148.110\n4.0\n62.0\n\n\n208184\nhttp://www.billboard.com/charts/hot-100/1984-1...\n1984-12-15\n60\nMissing You\nDiana Ross\nMissing YouDiana Ross\n1\n72.0\n60\n3\n...\n-11.410\n0.0\n0.0366\n0.65800\n0.001160\n0.0485\n0.187\n86.822\n4.0\n36.0\n\n\n\n\n10 rows × 29 columns\n\n\n\n\n## BOOTSTRAP!\n\n# d = d.sample(500_000, replace=True)\n\n\nd.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 330208 entries, 0 to 330207\nData columns (total 29 columns):\n #   Column                     Non-Null Count   Dtype         \n---  ------                     --------------   -----         \n 0   url                        330208 non-null  object        \n 1   WeekID                     330208 non-null  datetime64[ns]\n 2   Week Position              330208 non-null  int64         \n 3   Song                       330208 non-null  object        \n 4   Performer                  330208 non-null  object        \n 5   SongID                     330208 non-null  object        \n 6   Instance                   330208 non-null  int64         \n 7   Previous Week Position     298048 non-null  float64       \n 8   Peak Position              330208 non-null  int64         \n 9   Weeks on Chart             330208 non-null  int64         \n 10  spotify_genre              315700 non-null  object        \n 11  spotify_track_id           287066 non-null  object        \n 12  spotify_track_preview_url  169915 non-null  object        \n 13  spotify_track_duration_ms  287066 non-null  float64       \n 14  spotify_track_explicit     287066 non-null  object        \n 15  spotify_track_album        287004 non-null  object        \n 16  danceability               286508 non-null  float64       \n 17  energy                     286508 non-null  float64       \n 18  key                        286508 non-null  float64       \n 19  loudness                   286508 non-null  float64       \n 20  mode                       286508 non-null  float64       \n 21  speechiness                286508 non-null  float64       \n 22  acousticness               286508 non-null  float64       \n 23  instrumentalness           286508 non-null  float64       \n 24  liveness                   286508 non-null  float64       \n 25  valence                    286508 non-null  float64       \n 26  tempo                      286508 non-null  float64       \n 27  time_signature             286508 non-null  float64       \n 28  spotify_track_popularity   287066 non-null  float64       \ndtypes: datetime64[ns](1), float64(15), int64(4), object(9)\nmemory usage: 73.1+ MB\n\n\n\nfrom IPython.display import Audio, HTML\n\n\nAudio(url=d.loc[1000,\"spotify_track_preview_url\"])\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef curves(performer, song):\n    data = d[(d.Performer == performer) & (d.Song == song)].sort_values(by=\"WeekID\").reset_index(drop=True)\n    data[\"date_rel\"] = pd.to_timedelta(data[\"WeekID\"] - data[\"WeekID\"][0]).dt.days\n    x = data[\"date_rel\"].values # or date_rel or WeekID\n    y = data[\"Week Position\"].values\n    return x,y\n\n\ntest_cases2 = {\n    \"Patty Duke\": \"Don't Just Stand There\",\n    \"Ace Of Base\": \"Don't Turn Around\",\n    \"Dan + Shay\": \"Speechless\",\n    \"YoungBloodZ Featuring Lil Jon\": \"Damn!\",\n    \"K-Ci & JoJo\": \"All My Life\",\n    \"Trevor Daniel\": \"Falling\"\n}\n\n\n_, ax = plt.subplots(figsize=(15,4))\n\nfor performer, song in test_cases2.items():\n    x,y = curves(performer, song)\n    ax.plot(x, y, marker=\".\", label=f\"{song} ({performer})\")\n\nplt.gca().invert_yaxis()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nModeling the life of a song in the Top 100:\nWe assume that once a song has left the Top 100, it is impossible to re-enter (even though that does happen, of course)\n\nEach song has a starting rank \\(r_0\\).\nFor each following week, there is a bernoulli dropout probability \\(\\theta\\) that determines whether a song remains in the charts.\n\n\n\n# Observation: Genres tend to leave the Top 100 higher than they entered them\n\n\nentrances = []\npeaks = []\nexits = []\n\nfor _, group in d.groupby(\"SongID\"):\n    weeks = group.sort_values(by=\"WeekID\")[\"Week Position\"].values\n    entrances.append(weeks[0])\n    peaks.append(weeks.min())\n    exits.append(weeks[-1])\n\n\nimport numpy as np\n\n\n# from matplotlib.collections import LineCollection\n\n\n_, ax = plt.subplots(figsize=(10,4))\n\nK = len(entrances) + 1\n\nfor a, b, c in zip(entrances[:K], peaks[:K], exits[:K]):\n    if a != b != c: # remove constants\n        ax.plot([0, 1, 2], [a, b, c], c=\"k\", lw=.5, alpha=.01)\n\nplt.xlim(0,2)\nplt.ylim(0,100)\nplt.gca().invert_yaxis() # smaller is better\nplt.savefig(\"img/rise-decline.png\", dpi=600)\nplt.show()\n\n\n\n\n\n\n\n\nOBSERVATION: At least 3 types:\n\nconstants\nlow in, peak, low out\nlow in, peak, mid out\n\nTry to disentangle what causes the difference",
    "crumbs": [
      "DATA ABOUT MUSIC",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Analyzing song survival</span>"
    ]
  },
  {
    "objectID": "03_melody_I.html",
    "href": "03_melody_I.html",
    "title": "14  Melodies in Folk Songs",
    "section": "",
    "text": "14.1 The Essen Folksong Collection\nIn this session, we work with a corpus of melodies, the Essen Folksong Collection (EFC). There are several ways to access this corpus, for example through the interface provided by the Center for Computer Assisted Research in the Humanities (CCARH) at Stanford University: http://essen.themefinder.org/ or via http://kern.ccarh.org/browse?l=essen.\nA more convenient way to work with the pieces is by using the Python library music21. This library was developed and is maintaned my Mike Cuthbert at the MIT and is the most popular library for the computational analysis of symbolic music (i.e. scores). You can find its documentation here: http://web.mit.edu/music21/\nHowever, using music21 requires some training and getting used to its particular API (the way how to interact with its functions). We will not get into too many details here but rather showcase how it can be used for our purposes.\nThe first thing we do is to load the entire EFC and store it in a variable named corpora.\n# load corpus\ncorpora = m21.corpus.getComposer('essenFolksong')\nCalling the variable corpora shows that it consists of a list of file paths. Using the len() function, we can find out how many corpora are stored in the variable corpora.\nlen(corpora)\n\n31\nWe can also directly call the variable corpora to see what it contains:\ncorpora\n\n[WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/altdeu10.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/altdeu20.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad10.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad20.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad30.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad40.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad50.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad60.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad70.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/ballad80.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/boehme10.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/boehme20.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/dva0.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/erk10.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/erk20.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/erk30.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/erk5.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/fink0.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/folkHaydn.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/han1.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/han2.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/irl.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/kinder0.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/lot.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/lux.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/test0.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/test1.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/testd.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/teste.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/variant0.abc'),\n WindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/zuccal0.abc')]\nThe variable corpora is a list of file paths, each of which points to a corpus in this collection. Note that the location depends on the location where music21 is installed. If you would do this on your own computer, you would see different paths. The file names at the end of the file paths indicate what they contain, e.g. altdeu10.abc contains old German folksongs, boehme10.abc contains Czech folksongs, and han1.abc contains Chinese folksongs.\nThe .abc file ending refers to the ABC notation for encoding melodies. You find more information about the ABC encoding here: http://abcnotation.com/\nFor example, a song could be encoded like this:\nexample_song = \"\"\"\nX:1\nT:Speed the Plough\nM:4/4\nC:Trad.\nK:G\n|:GABc dedB|dedB dedB|c2ec B2dB|c2A2 A2BA|\n  GABc dedB|dedB dedB|c2ec B2dB|A2F2 G4:|\n|:g2gf gdBd|g2f2 e2d2|c2ec B2dB|c2A2 A2df|\n  g2gf g2Bd|g2f2 e2d2|c2ec B2dB|A2F2 G4:|\n\"\"\"\nThe tripple quotes (\"\"\") surrounding the ABC notation are used by Python to store multi-line text.\nWhat can we already understand from this encoding?\nmusic21 can load this string and display a graphical output of the score. This is done by a parser. A parser is a program that reads a file and produces a structured output.\nparsed_example_song = m21.converter.parse(example_song)\nWe did not need to give it the entire string again because we have already saved it in the example_song variable. The purpose of variables is that you can refer to them later in your code without explicitly needing to state its value.\nCalling the variable parsed_example_song now, however, does not really help us here…\nparsed_example_song\n\n&lt;music21.stream.Score 0x1898a474340&gt;\nIt returns a somewhat cryptic statement that says that the variable countains a music21.stream.Score object. Understanding the internal organization of music21 goes beyond this class. For us, it is suffient to know that these objects have certain associated functions, called methods, that we can use on them. To look at the score of this example song, we use the method .show().\nparsed_example_song.show()\nVoilà, this is much better! Now, let us compare the score output to the ABC encoding of the song:\nprint(example_song)\n\n\nX:1\nT:Speed the Plough\nM:4/4\nC:Trad.\nK:G\n|:GABc dedB|dedB dedB|c2ec B2dB|c2A2 A2BA|\n  GABc dedB|dedB dedB|c2ec B2dB|A2F2 G4:|\n|:g2gf gdBd|g2f2 e2d2|c2ec B2dB|c2A2 A2df|\n  g2gf g2Bd|g2f2 e2d2|c2ec B2dB|A2F2 G4:|\nNow the ABC notation makes already more sense. T:Speed the Ploug stands for the title, M:4/4 for the meter, and K:G for the key of the song. The ABC documentation tells us that X:1 encodes just a reference number, in case multiple pieces are stored in the same file (as in our case in the variable corpora, remember?). And the lines at the bottom encode the proper melody, where the letters represent note names that are organized into bars with or without repetition signs.\nmusic21 even gives us the option to listen to the song if we path the midi argument to the .show() method:\nparsed_example_song.show(\"midi\")\nNow, what happens if we try to parse one of the corpora in the EFC? We can select a specific corpus by its index in the list. Python starts counting at 0, so the first file in the list corresponds to\ncorpora[0]\n\nWindowsPath('C:/Users/fabianmoss/anaconda3/Lib/site-packages/music21/corpus/essenFolksong/altdeu10.abc')\nAs you can see, this is just the first file path in the variable corpora. Let’s try to parse it!\nfirst_corpus = m21.converter.parse(corpora[0])\nLooking at the new variable first_corpus shows a difference to the example song before; we don’t have a music21.stream.Score object but a music21.stream.Opus object.\nfirst_corpus\n\n&lt;music21.stream.Opus 0x1898b5ff4c0&gt;\nIf we would call the .show() method on first_corpus, we would see the scores of all pieces that are in this particular corpus. But we don’t know how many these are. It there are only three songs, it would not be a problem, but if there were thousands of songs, it could take a very long time to parse and display them all. Fortunately, all pieces in the collection have the X:n line that we saw above, so that we can directly reference them. With which number would we have to replace n if we wanted to look at the 7tst piece? Remember that Python starts counting at 0.\nfirst_corpus[70].show()\nA A B A’\nfirst_corpus[70].show(\"midi\")\nWe have seen that we can select items from lists by indexing them, list[i]. We can get ranges of lists by using the : character. For example, list[:10] shows the first ten elements, list[10:] shows everything after the ninth element, and list[3:6] shows elements 3, 4, and 5 (not 6!) of the list.",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Melodies in Folk Songs</span>"
    ]
  },
  {
    "objectID": "03_melody_I.html#comparing-songs",
    "href": "03_melody_I.html#comparing-songs",
    "title": "14  Melodies in Folk Songs",
    "section": "14.2 Comparing songs",
    "text": "14.2 Comparing songs\nLooking at individual songs is interesting for music analysis but for that the computational approach is not really necessary. We could as easily do the same by just looking at a book of scores. The power of computational methods becomes clearer when we start comparing different songs, potentially in a large number.\nTo facilitate this comparison, we will first load all songs in all corpora of the EFC into a single list, called songs (this might take a couple of minutes).\n\nsongs = [s for i in range(len(corpora)) for s in m21.converter.parse(corpora[i]) ]\n\nThis looks a bit complicated but all it does is to go through all corpora and extract all songs into a new list. The way we did it is called list comprehension in Python. It is not important if you don’t understand this now but feel free to look it up!\nUsing the len() function again, we see how many songs we have in total.\n\nlen(songs)\n\n8514\n\n\nWe can now use the list songs to compare two different songs. Again, we load the 71st song of the first corpus and store it now in a variable german_song, and we load chinese song with index 6200 into the variable chinese_song.\n\ngerman_song = songs[70]\nchinese_song = songs[6200]\n\nIt is easy to display these songs now:\n\ngerman_song.show()\n\n\n\n\n\n\n\n\n\nchinese_song.show()\n\n\n\n\n\n\n\n\n\nchinese_song.show(\"midi\")\n\n\n                \n                \n                \n\n\nAnalysis of songs…",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Melodies in Folk Songs</span>"
    ]
  },
  {
    "objectID": "03_melody_I.html#computational-analysis",
    "href": "03_melody_I.html#computational-analysis",
    "title": "14  Melodies in Folk Songs",
    "section": "14.3 Computational analysis",
    "text": "14.3 Computational analysis\nWe now go on to a computational analysis of these two and all the other songs. Specifically, we wil compare their melodic profiles. To make things a bit simpler, we will just look at the notes.\nA note can be easily represented as a pair of pitch (its height) and its duration. For example, the first note of the Die plappernden Junggesellen could be represented as (D4, 1/4); it is a quarter note on the pitch D4 (the 4 indicates the octave in which the note is).\nAnother way to represent the pitch of notes is using MIDI numbers. MIDI stands for Musical Instrument Digital Interface and was developed for the communication between different electronic instruments such as keyboards. In MIDI, each note is simply associated with a number:\n Image from https://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S2_MIDI.html.\nWe can see that D4 is associated with the number 62. The second note, the G4, is associated with 62+5=67 because G is five semitones above D.\nTo make it easier to work with pieces in this way, we define a function that gives us a list of notes for each piece.\n\ndef notelist(piece):\n    \"\"\"\n    This function takes a song as input and returns a list of (pitch, duration) pairs, \n    where the duration is given in quarter notes.\n    \"\"\"\n    \n    df = pd.DataFrame([ (note.pitch.midi, note.quarterLength) for note in piece.flat.notes ], columns=[\"MIDI Pitch\", \"Duration\"])\n    df[\"Onset\"] = df[\"Duration\"].cumsum()\n    \n    return df\n\nNote that the duration of a note is given in quarter notes, i.e. a quarter note has a duration of 1, a half note has a duration of 2, and an eighth note has a duration of 0.5.\nLet’s display the first phrase (the first eight notes) of the German song:\n\nnotelist(german_song)[:8]\n\n\n\n\n\n\n\n\nMIDI Pitch\nDuration\nOnset\n\n\n\n\n0\n62\n1.0\n1.0\n\n\n1\n67\n2.0\n3.0\n\n\n2\n71\n2.0\n5.0\n\n\n3\n74\n3.0\n8.0\n\n\n4\n72\n1.0\n9.0\n\n\n5\n71\n2.0\n11.0\n\n\n6\n69\n2.0\n13.0\n\n\n7\n67\n2.0\n15.0\n\n\n\n\n\n\n\nNote that we added another column, “Onset”. What does it represent?\nThis allows us now to look at the melodic profile of a particular song.\n\ndef plot_melodic_profile(notelist, ax=None, c=None, mean=False, Z=False, sections=False, standardized=False):\n    \n    if ax == None:\n        ax = plt.gca()\n    \n    if standardized:\n        x = notelist[\"Rel. Onset\"]\n        y = notelist[\"Rel. MIDI Pitch\"]\n    else:\n        x = notelist[\"Onset\"]\n        y = notelist[\"MIDI Pitch\"]\n    \n    ax.step(x,y, color=c)\n    \n    if mean:\n        ax.axhline(y.mean(), color=\"gray\", linestyle=\"--\")\n        \n    if sections:\n        for l in [ x.max() * i for i in [ 1/4, 1/2, 3/4] ]:\n            ax.axvline(l, color=\"gray\", linewidth=1, linestyle=\"--\")\n\n\nplot_melodic_profile(notelist(german_song))\n\n\n\n\n\n\n\n\nLikewise, we can as easily plot the melodic contour of the Chinese song (we will use a different color).\n\nfig, axes = plt.subplots(2,1, figsize=(8,6))\n\nplot_melodic_profile(notelist(german_song), ax=axes[0], mean=True)\nplot_melodic_profile(notelist(chinese_song), ax=axes[1], c=\"firebrick\", mean=True)\n\nplt.tight_layout()\nplt.savefig(\"img/melodic_profiles.png\")\n\n\n\n\n\n\n\n\nThe dashed grey lines in both plots show the average MIDI pitch of the song.\nBut still, it is quite difficult to compare them directly. They differ both with respect to their length (see the numbers on the “Onset” axis) and their pitches (see “MIDI Pitch” axis).\nWe need to transform them in a way that makes them directly comparable. To that end, we define a new function standardize().\n\ndef standardize(notelist):\n    \"\"\"\n    Takes a notelist as input and returns a standardized version.\n    \"\"\"\n    \n    notelist[\"Rel. MIDI Pitch\"] = (notelist[\"MIDI Pitch\"] - notelist[\"MIDI Pitch\"].mean()) / notelist[\"MIDI Pitch\"].std()\n    notelist[\"Rel. Duration\"] = notelist[\"Duration\"] / notelist[\"Duration\"].sum()\n    notelist[\"Rel. Onset\"] = notelist[\"Onset\"] / notelist[\"Onset\"].max()\n    \n    return notelist\n\n\nstandardize(notelist(german_song))[:8]\n\n\n\n\n\n\n\n\nMIDI Pitch\nDuration\nOnset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\n\n\n\n\n0\n62\n1.0\n1.0\n-2.543827\n0.016667\n0.016667\n\n\n1\n67\n2.0\n3.0\n-0.949300\n0.033333\n0.050000\n\n\n2\n71\n2.0\n5.0\n0.326322\n0.033333\n0.083333\n\n\n3\n74\n3.0\n8.0\n1.283038\n0.050000\n0.133333\n\n\n4\n72\n1.0\n9.0\n0.645227\n0.016667\n0.150000\n\n\n5\n71\n2.0\n11.0\n0.326322\n0.033333\n0.183333\n\n\n6\n69\n2.0\n13.0\n-0.311489\n0.033333\n0.216667\n\n\n7\n67\n2.0\n15.0\n-0.949300\n0.033333\n0.250000\n\n\n\n\n\n\n\n\nplot_melodic_profile(standardize(notelist(german_song)), mean=True, sections=True, standardized=True)\nplot_melodic_profile(standardize(notelist(chinese_song)), c=\"firebrick\", standardized=True)\n\n\n\n\n\n\n\n\nStandardizing the songs makes it possible to compare them directly: They have now the same length 1 and their pitches are centered around the mean 0 with a standard deviation of 1.\nHowever, already with two pieces this plot is quite crowded.\n\ndfs = [ ]\n\nfor i, song in enumerate(songs):\n    df = standardize(notelist(song))\n    df[\"Song ID\"] = i\n    dfs.append(df)\n\nbig_df = pd.concat(dfs).reset_index(drop=True)\n\n\nbig_df\n\n\n\n\n\n\n\n\nMIDI Pitch\nDuration\nOnset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\nSong ID\nAvg. MIDI Pitch\nshifted_pitch\n\n\n\n\n0\n67\n2.00\n2.00\n-1.819039\n0.013158\n0.013158\n0\n72\n-5\n\n\n1\n70\n2.00\n4.00\n-0.741977\n0.013158\n0.026316\n0\n72\n-2\n\n\n2\n71\n2.00\n6.00\n-0.382956\n0.013158\n0.039474\n0\n72\n-1\n\n\n3\n72\n2.00\n8.00\n-0.023935\n0.013158\n0.052632\n0\n72\n0\n\n\n4\n72\n2.00\n10.00\n-0.023935\n0.013158\n0.065789\n0\n72\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n450591\n71\n0.25\n28.50\n0.691456\n0.008197\n0.934426\n8513\n68\n3\n\n\n450592\n69\n0.25\n28.75\n0.098779\n0.008197\n0.942623\n8513\n68\n1\n\n\n450593\n73\n0.25\n29.00\n1.284133\n0.008197\n0.950820\n8513\n68\n5\n\n\n450594\n71\n1.00\n30.00\n0.691456\n0.032787\n0.983607\n8513\n68\n3\n\n\n450595\n69\n0.50\n30.50\n0.098779\n0.016393\n1.000000\n8513\n68\n1\n\n\n\n\n450596 rows × 9 columns\n\n\n\n\nbig_df.to_csv(\"data/big_df.csv\") # comma-separated values\n\n\nbig_df = pd.read_csv(\"data/big_df.csv\")",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Melodies in Folk Songs</span>"
    ]
  },
  {
    "objectID": "03_melody_I.html#the-melodic-arc",
    "href": "03_melody_I.html#the-melodic-arc",
    "title": "14  Melodies in Folk Songs",
    "section": "14.4 The melodic arc",
    "text": "14.4 The melodic arc\n\n%%time\n\nfig, ax = plt.subplots(figsize=(12,8))\n\ngrouped = big_df.groupby(\"Song ID\")\n\nfor i, g in grouped:\n    x = g[\"Rel. Onset\"]\n    y = g[\"Rel. MIDI Pitch\"]\n    ax.plot(x,y, lw=.5, c=\"tab:red\", alpha=1/100)\n        \nax.axvline(.25, lw=1, ls=\"--\", c=\"gray\")\nax.axvline(.5, lw=1, ls=\"--\", c=\"gray\")\nax.axvline(.75, lw=1, ls=\"--\", c=\"gray\")\nax.axhline(0, lw=1, ls=\"--\", c=\"gray\")\n\nlowess = sm.nonparametric.lowess\nbig_x = big_df[\"Rel. Onset\"]\nbig_y = big_df[\"Rel. MIDI Pitch\"]\nbig_z = lowess(big_y, big_x, frac=5/100, delta=1/20) # Locally-Weighted Scatterplot Smoothing\nax.plot(big_z[:,0], big_z[:,1], c=\"black\", lw=3)\n\nplt.title(\"Melodic arc\")\nplt.xlabel(\"Relative onset\")\nplt.ylabel(\"Pitch deviation\")\nplt.xticks(np.linspace(0,1,5))\nplt.yticks(np.linspace(-5,5,11))\nplt.xlim(0,1)\n\nplt.tight_layout()\nplt.savefig(\"img/melodic_arc.png\")\nplt.show()\n\n\n\n\n\n\n\n\nWall time: 24.1 s",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Melodies in Folk Songs</span>"
    ]
  },
  {
    "objectID": "03_melody_I.html#intervals",
    "href": "03_melody_I.html#intervals",
    "title": "14  Melodies in Folk Songs",
    "section": "14.5 Intervals",
    "text": "14.5 Intervals\nWe have seen that the melodic arc emerges as a stable shape over the entire EFC, and that sub-phrases of the songs likewise have an arc-like shape. In the remainder of this section, we look at intervals, the distance between two notes.\nLet’s come back to the song Die plappernden Junggesellen\n\ngerman_song.show()\n\n\n\n\n\n\n\n\nWe have already extracted its notes and stored them in a DataFrame:\n\nbig_df[ big_df[\"Song ID\"] == 70].head(8)\n\n\n\n\n\n\n\n\nUnnamed: 0\nUnnamed: 0.1\nMIDI Pitch\nDuration\nOnset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\nSong ID\n\n\n\n\n2969\n2969\n2969\n62\n1.0\n1.0\n-2.543827\n0.016667\n0.016667\n70\n\n\n2970\n2970\n2970\n67\n2.0\n3.0\n-0.949300\n0.033333\n0.050000\n70\n\n\n2971\n2971\n2971\n71\n2.0\n5.0\n0.326322\n0.033333\n0.083333\n70\n\n\n2972\n2972\n2972\n74\n3.0\n8.0\n1.283038\n0.050000\n0.133333\n70\n\n\n2973\n2973\n2973\n72\n1.0\n9.0\n0.645227\n0.016667\n0.150000\n70\n\n\n2974\n2974\n2974\n71\n2.0\n11.0\n0.326322\n0.033333\n0.183333\n70\n\n\n2975\n2975\n2975\n69\n2.0\n13.0\n-0.311489\n0.033333\n0.216667\n70\n\n\n2976\n2976\n2976\n67\n2.0\n15.0\n-0.949300\n0.033333\n0.250000\n70\n\n\n\n\n\n\n\nThe code above reads as “Select all rows in big_df for which the column Song ID is equal to 70”. The .head() method displays the first 5 rows by default but you can specify the number of rows you want to be displayed (here 8).\nFocusing on the “MIDI Pitch” column, the notes in the first phrase have MIDI pitch 62, 67, 71, 74, 72. Since intervals correspond to the difference between notes, the intervals for the beginning of this song are:\n\n+5 (67-62)\n+4 (71-67)\n+3 (74-71)\n-2 (72-74)\n-1 (71-72)\n-2 (69-71)\n-2 (67-69)\n\nThe sequence of intervals in this phrase is thus [+5, +4, +3, -2, -1, -2, -2]. The signs (+ or -) also reflect the arc-like shape of this first phrase, but the sizes of the intervals are not perfecly balanced. Note that -2 (two descending semitones, or one descending whole tone) is the most frequent interval.\n\nall_ints = [ p2 - p1 for i, g in big_df.groupby(\"Song ID\") for p1, p2 in zip(g[\"MIDI Pitch\"], g[\"MIDI Pitch\"][1:]) ]\nmin_int = min(all_ints)\nmax_int = max(all_ints)\n\n\nmin_int, max_int\n\n(-25, 25)\n\n\n\nlen(all_ints)\n\n442082\n\n\n\nints_df = pd.DataFrame(0, index=np.arange(min_int,max_int), columns=np.arange(min_int,max_int+1))\n\nfor i, g in big_df.groupby(\"Song ID\"):\n    intervals = [ p2 - p1 for p1, p2 in zip(g[\"MIDI Pitch\"], g[\"MIDI Pitch\"][1:])]\n        \n    for i1, i2 in zip(intervals, intervals[1:]):\n        ints_df.loc[i1,i2] += 1\n\n\nints_df.loc[-10:10, -10:10]\n\n\n\n\n\n\n\n\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n...\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n-10\n0\n0\n0\n0\n0\n2\n3\n2\n34\n0\n...\n16\n174\n219\n60\n91\n0\n78\n67\n29\n243\n\n\n-9\n0\n0\n0\n6\n0\n3\n0\n39\n10\n31\n...\n5\n159\n39\n40\n313\n2\n30\n0\n111\n27\n\n\n-8\n0\n1\n0\n1\n0\n0\n19\n0\n319\n5\n...\n302\n66\n605\n3\n159\n2\n4\n73\n0\n32\n\n\n-7\n0\n0\n2\n14\n0\n37\n2\n91\n96\n103\n...\n17\n859\n300\n527\n651\n1\n399\n18\n219\n141\n\n\n-6\n0\n0\n0\n0\n0\n0\n12\n11\n8\n21\n...\n274\n7\n132\n25\n10\n19\n6\n3\n3\n4\n\n\n-5\n1\n0\n1\n49\n0\n75\n32\n866\n1361\n25\n...\n230\n1906\n1272\n148\n2131\n22\n252\n87\n368\n160\n\n\n-4\n3\n0\n27\n11\n5\n461\n18\n692\n167\n1099\n...\n129\n2738\n40\n1610\n616\n13\n884\n8\n269\n31\n\n\n-3\n1\n91\n0\n192\n2\n215\n2490\n416\n9478\n134\n...\n2547\n1467\n6274\n109\n1684\n13\n403\n508\n59\n261\n\n\n-2\n67\n14\n260\n858\n132\n1964\n113\n8871\n21285\n13896\n...\n454\n14883\n1115\n2616\n3216\n63\n1681\n88\n537\n407\n\n\n-1\n5\n174\n4\n68\n47\n32\n1679\n91\n13445\n205\n...\n5410\n396\n2878\n58\n477\n44\n23\n241\n3\n52\n\n\n0\n186\n130\n310\n1022\n80\n2270\n1440\n5506\n16887\n4603\n...\n2578\n12142\n4947\n3340\n5203\n30\n1353\n292\n824\n478\n\n\n1\n55\n6\n174\n43\n110\n944\n165\n2564\n501\n3765\n...\n747\n7649\n193\n1470\n132\n26\n172\n9\n59\n2\n\n\n2\n138\n294\n29\n1288\n15\n1361\n3754\n2562\n15795\n254\n...\n8404\n11261\n5249\n86\n1695\n2\n187\n171\n57\n61\n\n\n3\n272\n23\n373\n655\n122\n1755\n24\n5509\n3397\n2400\n...\n295\n4128\n231\n519\n1523\n35\n180\n0\n122\n11\n\n\n4\n5\n164\n3\n130\n11\n51\n1026\n64\n4217\n60\n...\n1133\n67\n3153\n20\n102\n7\n1\n14\n0\n2\n\n\n5\n116\n23\n505\n330\n13\n1996\n82\n2375\n2834\n1868\n...\n102\n2557\n270\n1355\n160\n1\n94\n0\n16\n5\n\n\n6\n2\n4\n1\n4\n23\n1\n14\n18\n44\n34\n...\n42\n8\n20\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n31\n18\n11\n273\n3\n167\n128\n665\n1338\n59\n...\n119\n566\n249\n12\n166\n0\n8\n2\n6\n0\n\n\n8\n47\n1\n88\n7\n19\n149\n4\n370\n33\n384\n...\n23\n159\n0\n23\n6\n0\n1\n0\n0\n0\n\n\n9\n1\n61\n1\n20\n3\n20\n442\n18\n1024\n4\n...\n122\n13\n129\n1\n8\n0\n0\n5\n0\n0\n\n\n10\n267\n0\n56\n23\n28\n58\n0\n517\n163\n295\n...\n3\n78\n0\n4\n5\n0\n0\n0\n0\n0\n\n\n\n\n21 rows × 21 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(ints_df.loc[-12:13,-12:13], cmap=\"coolwarm\", square=True, linewidths=0.01,ax=ax)\nplt.ylabel(\"First interval\")\nplt.xlabel(\"Second interval\")\nplt.show()\n\n\n\n\n\n\n\n\nThe two most common interval pairs are (0,0) and (-2,-2). A much less frequent pair of intervals is (5,0), but this is still much more frequent than, for example, (9,9).\nTo which melodic fragments do these correspond?\n\nbig_df[\"Avg. MIDI Pitch\"] = 0\n\nfor i, group in big_df.groupby(\"Song ID\"):\n    grp_mean_pitch = int(group[\"MIDI Pitch\"].mean())\n    big_df.loc[big_df[\"Song ID\"] == i, \"Avg. MIDI Pitch\"] = grp_mean_pitch\n\n\nbig_df[\"shifted_pitch\"] = big_df[\"MIDI Pitch\"] - big_df[\"Avg. MIDI Pitch\"]\n\n\nbig_df.tail()\n\n\n\n\n\n\n\n\nMIDI Pitch\nDuration\nOnset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\nSong ID\nAvg. MIDI Pitch\nshifted_pitch\n\n\n\n\n450591\n71\n0.25\n28.50\n0.691456\n0.008197\n0.934426\n8513\n68\n3\n\n\n450592\n69\n0.25\n28.75\n0.098779\n0.008197\n0.942623\n8513\n68\n1\n\n\n450593\n73\n0.25\n29.00\n1.284133\n0.008197\n0.950820\n8513\n68\n5\n\n\n450594\n71\n1.00\n30.00\n0.691456\n0.032787\n0.983607\n8513\n68\n3\n\n\n450595\n69\n0.50\n30.50\n0.098779\n0.016393\n1.000000\n8513\n68\n1\n\n\n\n\n\n\n\n\nidx = np.arange(big_df[\"shifted_pitch\"].min(), big_df[\"shifted_pitch\"].max() + 1)\nidx\n\narray([-16, -15, -14, -13, -12, -11, -10,  -9,  -8,  -7,  -6,  -5,  -4,\n        -3,  -2,  -1,   0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n        10,  11,  12,  13,  14,  15,  16,  17])\n\n\n\ntransitions_df = pd.DataFrame(0, index=idx, columns=idx)\ntransitions_df\n\n\n\n\n\n\n\n\n-16\n-15\n-14\n-13\n-12\n-11\n-10\n-9\n-8\n-7\n...\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n-16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-13\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n-1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n13\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n17\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n34 rows × 34 columns\n\n\n\n\n%%time\n\nfor i, group in big_df.groupby(\"Song ID\"):\n    for bg in zip(group[\"shifted_pitch\"], group[\"shifted_pitch\"][1:]):\n        transitions_df.loc[bg[0],bg[1]] +=1\n\nWall time: 1min 29s\n\n\n\nprint(f\"There are {transitions_df.sum().sum()} intervals in total in the corpus.\")\n\nThere are 442082 intervals in total in the corpus.\n\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\ng = sns.heatmap(transitions_df, cmap=\"coolwarm\", linewidths=.01, square=True)\nplt.ylabel(\"First interval\")\nplt.xlabel(\"Second interval\")\nplt.show()",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Melodies in Folk Songs</span>"
    ]
  },
  {
    "objectID": "03_melody_I.html#n-grams",
    "href": "03_melody_I.html#n-grams",
    "title": "14  Melodies in Folk Songs",
    "section": "14.6 n-grams",
    "text": "14.6 n-grams\n\nn-gram viewer [https://www.peachnote.com/#!nt=singleNoteAffine&npq=62+0+1+2] :cite:Viro2011\ntheory on n-grams",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Melodies in Folk Songs</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html",
    "href": "04_jazz_solos.html",
    "title": "15  Solos in the Weimar Jazz Database",
    "section": "",
    "text": "15.1 Melodic arc?\nDoes the melodic arc also appear in the Jazz solos?\ndef notelist(melid):\n    \n    solo = solos[solos[\"melid\"] == melid]\n    \n    solo = solo[[\"pitch\", \"duration\"]]\n    solo[\"onset\"] = solo[\"duration\"].cumsum()\n    return solo\nnotelist(1)\n\n\n\n\n\n\n\n\npitch\nduration\nonset\n\n\n\n\n0\n65.0\n0.138776\n0.138776\n\n\n1\n63.0\n0.171247\n0.310023\n\n\n2\n58.0\n0.081270\n0.391293\n\n\n3\n61.0\n0.235102\n0.626395\n\n\n4\n63.0\n0.130612\n0.757007\n\n\n...\n...\n...\n...\n\n\n525\n66.0\n0.137143\n80.645238\n\n\n526\n65.0\n0.101587\n80.746825\n\n\n527\n63.0\n0.104490\n80.851315\n\n\n528\n62.0\n0.110295\n80.961610\n\n\n529\n70.0\n0.187211\n81.148821\n\n\n\n\n530 rows × 3 columns\ndef plot_melodic_profile(notelist, ax=None, c=None, mean=False, Z=False, sections=False, standardized=False):\n    \n    if ax == None:\n        ax = plt.gca()\n    \n    if standardized:\n        x = notelist[\"Rel. Onset\"]\n        y = notelist[\"Rel. MIDI Pitch\"]\n    else:\n        x = notelist[\"onset\"]\n        y = notelist[\"pitch\"]\n    \n    ax.step(x,y, color=c)\n    \n    if mean:\n        ax.axhline(y.mean(), color=\"gray\", linestyle=\"--\")\n        \n    if sections:\n        for l in [ x.max() * i for i in [ 1/4, 1/2, 3/4] ]:\n            ax.axvline(l, color=\"gray\", linewidth=1, linestyle=\"--\")\nfig, axes = plt.subplots(2,2, figsize=(20,9))\naxes = axes.flatten()\n\nplot_melodic_profile(notelist(1), ax=axes[0], mean=True)\nplot_melodic_profile(notelist(77), ax=axes[1], mean=True)\nplot_melodic_profile(notelist(50), ax=axes[2], mean=True)\nplot_melodic_profile(notelist(233), ax=axes[3], mean=True)\ndef standardize(notelist):\n    \"\"\"\n    Takes a notelist as input and returns a standardized version.\n    \"\"\"\n    \n    notelist[\"Rel. MIDI Pitch\"] = (notelist[\"pitch\"] - notelist[\"pitch\"].mean()) / notelist[\"pitch\"].std()\n    notelist[\"Rel. Duration\"] = notelist[\"duration\"] / notelist[\"duration\"].sum()\n    notelist[\"Rel. Onset\"] = notelist[\"onset\"] / notelist[\"onset\"].max()\n    \n    return notelist\nstandardize(notelist(1))\n\n\n\n\n\n\n\n\npitch\nduration\nonset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\n\n\n\n\n0\n65.0\n0.138776\n0.138776\n-0.460594\n0.001710\n0.001710\n\n\n1\n63.0\n0.171247\n0.310023\n-0.697714\n0.002110\n0.003820\n\n\n2\n58.0\n0.081270\n0.391293\n-1.290513\n0.001001\n0.004822\n\n\n3\n61.0\n0.235102\n0.626395\n-0.934833\n0.002897\n0.007719\n\n\n4\n63.0\n0.130612\n0.757007\n-0.697714\n0.001610\n0.009329\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n525\n66.0\n0.137143\n80.645238\n-0.342034\n0.001690\n0.993794\n\n\n526\n65.0\n0.101587\n80.746825\n-0.460594\n0.001252\n0.995046\n\n\n527\n63.0\n0.104490\n80.851315\n-0.697714\n0.001288\n0.996334\n\n\n528\n62.0\n0.110295\n80.961610\n-0.816274\n0.001359\n0.997693\n\n\n529\n70.0\n0.187211\n81.148821\n0.132205\n0.002307\n1.000000\n\n\n\n\n530 rows × 6 columns\nfig, ax = plt.subplots(figsize=(20,5))\n\nfor i in range(4):\n    plot_melodic_profile(standardize(notelist(i)), \n                         mean=True, \n                         standardized=True)\nplt.xlim(0,1)\nplt.show()\nbig_df = pd.concat([standardize(notelist(i)) for i in range(solos_meta.shape[0])])\nsolos\n\n\n\n\n\n\n\n\neventid\nmelid\nonset\npitch\nduration\nperiod\ndivision\nbar\nbeat\ntatum\n...\nloud_max\nloud_med\nloud_sd\nloud_relpos\nloud_cent\nloud_s2b\nf0_range\nf0_freq_hz\nf0_med_dev\nperformer\n\n\n\n\n0\n1\n1\n10.343492\n65.0\n0.138776\n4\n1\n0\n1\n1\n...\n0.126209\n66.526087\n5.541147\n0.307692\n0.389466\n1.056169\n37.794261\n12.932532\n-0.328442\nArt Pepper\n\n\n1\n2\n1\n10.637642\n63.0\n0.171247\n4\n4\n0\n2\n1\n...\n0.349751\n69.133321\n2.912412\n0.250000\n0.468687\n1.120317\n6.365930\n6.956935\n11.135423\nArt Pepper\n\n\n2\n3\n1\n10.843719\n58.0\n0.081270\n4\n4\n0\n2\n4\n...\n0.094051\n66.352130\n3.564563\n0.428571\n0.531354\n1.310389\n68.010392\nNaN\n32.366787\nArt Pepper\n\n\n3\n4\n1\n10.948209\n61.0\n0.235102\n4\n1\n0\n3\n1\n...\n0.521187\n66.484173\n2.414298\n0.818182\n0.559333\n0.984047\n15.443906\n5.867151\n-3.374696\nArt Pepper\n\n\n4\n5\n1\n11.232653\n63.0\n0.130612\n4\n1\n0\n4\n1\n...\n0.560737\n71.699054\n2.185794\n0.166667\n0.438973\n1.061262\n11.444363\n8.329975\n6.377737\nArt Pepper\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n200804\n200805\n456\n63.135057\n57.0\n0.168345\n4\n2\n53\n4\n2\n...\n1.113380\n72.169552\n6.896394\n0.687500\n0.581956\n1.271747\n191.074095\n10.966972\n-11.891698\nZoot Sims\n\n\n200805\n200806\n456\n63.303401\n55.0\n0.087075\n4\n3\n54\n1\n1\n...\n0.491496\n69.732265\n1.814723\n0.500000\n0.595212\n1.339060\n40.375449\nNaN\n-99.173779\nZoot Sims\n\n\n200806\n200807\n456\n63.390476\n57.0\n0.191565\n4\n3\n54\n1\n2\n...\n1.187058\n76.628621\n2.628726\n0.411765\n0.590950\n1.432802\n104.823845\n11.148561\n-2.911604\nZoot Sims\n\n\n200807\n200808\n456\n63.640091\n59.0\n0.406349\n4\n1\n54\n2\n1\n...\n0.972676\n66.042058\n3.690577\n0.000000\n0.334937\n1.082549\n165.810976\n2.659723\n14.311001\nZoot Sims\n\n\n200808\n200809\n456\n64.058050\n52.0\n1.433832\n4\n2\n54\n3\n2\n...\n0.368321\n58.174931\n9.418678\n0.053030\n0.400571\n1.278890\n66.932198\n2.153916\n-9.381310\nZoot Sims\n\n\n\n\n200809 rows × 27 columns\nbig_df\n\n\n\n\n\n\n\n\npitch\nduration\nonset\nRel. MIDI Pitch\nRel. Duration\nRel. Onset\n\n\n\n\n0\n65.0\n0.138776\n0.138776\n-0.460594\n0.001710\n0.001710\n\n\n1\n63.0\n0.171247\n0.310023\n-0.697714\n0.002110\n0.003820\n\n\n2\n58.0\n0.081270\n0.391293\n-1.290513\n0.001001\n0.004822\n\n\n3\n61.0\n0.235102\n0.626395\n-0.934833\n0.002897\n0.007719\n\n\n4\n63.0\n0.130612\n0.757007\n-0.697714\n0.001610\n0.009329\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n200585\n62.0\n0.870748\n68.588934\n0.014206\n0.012540\n0.987794\n\n\n200586\n57.0\n0.133515\n68.722449\n-0.896471\n0.001923\n0.989717\n\n\n200587\n62.0\n0.139320\n68.861769\n0.014206\n0.002006\n0.991723\n\n\n200588\n61.0\n0.133515\n68.995283\n-0.167930\n0.001923\n0.993646\n\n\n200589\n60.0\n0.441179\n69.436463\n-0.350065\n0.006354\n1.000000\n\n\n\n\n200590 rows × 6 columns\nsolos_meta[\"performer\"].unique()\n\narray(['Art Pepper', 'Benny Carter', 'Benny Goodman', 'Ben Webster',\n       'Bix Beiderbecke', 'Bob Berg', 'Branford Marsalis', 'Buck Clayton',\n       'Cannonball Adderley', 'Charlie Parker', 'Charlie Shavers',\n       'Chet Baker', 'Chris Potter', 'Chu Berry', 'Clifford Brown',\n       'Coleman Hawkins', 'Curtis Fuller', 'David Liebman',\n       'David Murray', 'Dexter Gordon', 'Dickie Wells', 'Dizzy Gillespie',\n       'Don Byas', 'Don Ellis', 'Eric Dolphy', 'Fats Navarro',\n       'Freddie Hubbard', 'George Coleman', 'Gerry Mulligan',\n       'Hank Mobley', 'Harry Edison', 'Henry Allen', 'Herbie Hancock',\n       'J.C. Higginbotham', 'J.J. Johnson', 'Joe Henderson', 'Joe Lovano',\n       'John Abercrombie', 'John Coltrane', 'Johnny Dodds',\n       'Johnny Hodges', 'Joshua Redman', 'Kai Winding', 'Kenny Dorham',\n       'Kenny Garrett', 'Kenny Wheeler', 'Kid Ory', 'Lee Konitz',\n       'Lee Morgan', 'Lester Young', 'Lionel Hampton', 'Louis Armstrong',\n       'Michael Brecker', 'Miles Davis', 'Milt Jackson', 'Nat Adderley',\n       'Ornette Coleman', 'Pat Martino', 'Pat Metheny', 'Paul Desmond',\n       'Pepper Adams', 'Phil Woods', 'Red Garland', 'Rex Stewart',\n       'Roy Eldridge', 'Sidney Bechet', 'Sonny Rollins', 'Sonny Stitt',\n       'Stan Getz', 'Steve Coleman', 'Steve Lacy', 'Steve Turre',\n       'Von Freeman', 'Warne Marsh', 'Wayne Shorter', 'Woody Shaw',\n       'Wynton Marsalis', 'Zoot Sims'], dtype=object)\n%%time\n\nfig, ax = plt.subplots(figsize=(12,8))\n\nartists = [\"Louis Armstrong\"]\n\n# for i, (artist, group) in enumerate(solos.groupby(\"performer\")):\n#     if artist in artists:\n#         for j, group in group.groupby(\"melid\"):\n#             solo = standardize(notelist(j))\n#             x = solo[\"Rel. Onset\"]\n#             y = solo[\"Rel. MIDI Pitch\"]\n#             ax. plot(x,y, lw=.5, c=\"tab:red\", alpha=.5)\n\nfor ID in range(solos_meta.shape[0]):\n    solo = standardize(notelist(ID))\n    x = solo[\"Rel. Onset\"]\n    y = solo[\"Rel. MIDI Pitch\"]\n    ax. plot(x,y, lw=.5, c=\"tab:red\", alpha=.05)\n        \nax.axvline(.25, lw=2, ls=\"--\", c=\"gray\")\nax.axvline(.5, lw=2, ls=\"--\", c=\"gray\")\nax.axvline(.75, lw=2, ls=\"--\", c=\"gray\")\nax.axhline(0, lw=2, ls=\"--\", c=\"gray\")\n\nlowess = sm.nonparametric.lowess\nbig_x = big_df[\"Rel. Onset\"]\nbig_y = big_df[\"Rel. MIDI Pitch\"]\nbig_z = lowess(big_y, big_x, frac=1/10, delta=1/20)\nax.plot(big_z[:,0], big_z[:,1], c=\"black\", lw=3)\n\nplt.title(\"Solo wave\")\nplt.xlabel(\"Relative onset\")\nplt.ylabel(\"Pitch deviation\")\nplt.xticks(np.linspace(0,1,5))\nplt.yticks(np.linspace(-5,5,11))\nplt.xlim(0,1)\n\nplt.tight_layout()\nplt.savefig(\"img/jazz_melodic_arc.png\")\nplt.show()\n\n\n\n\n\n\n\n\nWall time: 7.76 s",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html#pitch-vs-loudness",
    "href": "04_jazz_solos.html#pitch-vs-loudness",
    "title": "15  Solos in the Weimar Jazz Database",
    "section": "15.2 Pitch vs loudness",
    "text": "15.2 Pitch vs loudness\nAbove we have already analyzed some melodic profiles and seen that, on average, the Jazz solos tend not to follow the melodic arch on a global scale. Now, we ask whether the pitch of the notes in the solos are related to another important feature of performance: loudness. The WJazzD contains several measures for loudness (compare the columns in the solos DataFrame). Here, we focus on the “Median loudness” which is stored in the loud_med column.\nLet us look at an example.\n\nexample_solo = solos[ solos[\"melid\"] == 233 ][[\"pitch\", \"loud_med\"]]\n\n\nexample_solo\n\n\n\n\n\n\n\n\npitch\nloud_med\n\n\n\n\n115075\n62.0\n67.082700\n\n\n115076\n65.0\n65.345677\n\n\n115077\n67.0\n66.323539\n\n\n115078\n69.0\n69.204257\n\n\n115079\n62.0\n69.059581\n\n\n...\n...\n...\n\n\n115549\n59.0\n61.083907\n\n\n115550\n57.0\n58.345887\n\n\n115551\n55.0\n65.132786\n\n\n115552\n54.0\n59.595735\n\n\n115553\n53.0\n58.390132\n\n\n\n\n479 rows × 2 columns\n\n\n\nWe can get a visual impression of whether there might be a direct relation between the two features by plotting it and drawing a regression line. For this, the regplot() function of the seaborn library is well-suited.\n\nsns.regplot(data=example_solo, x=\"pitch\", y=\"loud_med\", line_kws={\"color\":\"black\"});\n\n\n\n\n\n\n\n\nThere seems to be no clear relation; no matter how high the pitch, the loudness stays more or less the same. Let’s look at another example!\n\nexample_solo2 = solos[ solos[\"melid\"] == 333 ][[\"pitch\", \"loud_med\"]]\n\nsns.regplot(data=example_solo2, x=\"pitch\", y=\"loud_med\", line_kws={\"color\":\"black\"});\n\n\n\n\n\n\n\n\nIn this case, there is a positive trend. The higher the pitch, the louder the performer plays. Since we have now two different examples - in one case no relation, in the other case a positive correlation - we should now look at whether there is a trend emerging from all solos taken together.",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html#the-rain-cloud-of-jazz-solos",
    "href": "04_jazz_solos.html#the-rain-cloud-of-jazz-solos",
    "title": "15  Solos in the Weimar Jazz Database",
    "section": "15.3 The “rain cloud” of Jazz solos",
    "text": "15.3 The “rain cloud” of Jazz solos\nWe now take all 200’809 notes from all solos and look at the relation between their pitch and their median loudness.\n\nX = solos[[\"pitch\", \"loud_med\"]].values\nx = X[:,0]\ny = X[:,1]\n\nfig, ax = plt.subplots(figsize=(16,9))\nax.scatter(x,y, alpha=0.05)\n\nplt.xlabel(\"Pitch\")\nplt.ylabel(\"Median loudness [dB]\")\nplt.show()\n\n\n\n\n\n\n\n\nThe visual impression is that of a cloud from which rain drops down and forms a puddle. Which trends can we observe?",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "04_jazz_solos.html#comparing-performers",
    "href": "04_jazz_solos.html#comparing-performers",
    "title": "15  Solos in the Weimar Jazz Database",
    "section": "15.4 Comparing performers",
    "text": "15.4 Comparing performers\nTaking all pieces together was not really informative. Maybe a somewhat closer look brings more to the front. Let us some specific performers whose solos we want to compare.\n\nselected_performers = [\"Charlie Parker\", \"Miles Davis\", \"Louis Armstrong\", \"Herbie Hancock\", \"Von Freeman\", \"Red Garland\"]\n\n\ngrouped_df = solos.groupby(\"performer\")\n\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor performer, df in grouped_df:\n    if performer in selected_performers:\n        sns.regplot(\n            data=df, \n            x=\"pitch\", \n            y=\"loud_med\", \n            x_jitter=.1, \n            y_jitter=.1, \n            scatter_kws={\"alpha\":.01, \"color\":\"grey\"}, \n            line_kws={\"lw\":2},\n            label=performer,\n            scatter=False,\n            ax=ax\n        )\n        \nplt.xlabel(\"MIDI Pitch\")\nplt.ylabel(\"Median loudness [dB]\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nObservations:\n\nMost performers increase loudness with increasing pitch.\nCharlie Parker (sax) and Louis Armstrong (t) show very similar patterns but Armstrong is generally higher.\nMiles Davis (t) is similar to the two but plays generally softer than both.\nVon Freeman (sax) strongly and Herbie Hancock (p) weakly decrease loudness with increasing pitch (almost all other performers show positive correlations).\nRed Garland (p) plays generally lower than Herbie Hancock (p) but does show a positive correlation between pitch and loudness (NB: there is only one solo in the database).\n\nDoes this tell us something about performer styles or about instruments?",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Solos in the *Weimar Jazz Database*</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html",
    "href": "05_data-driven_music_history.html",
    "title": "16  Data-Driven Music History",
    "section": "",
    "text": "16.1 Research Questions",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#research-questions",
    "href": "05_data-driven_music_history.html#research-questions",
    "title": "16  Data-Driven Music History",
    "section": "",
    "text": "General: How can we study historical changes quantitatively?\nSpecific: What can we say about the history of tonality based on a dataset of musical pieces?",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#a-bit-of-theory",
    "href": "05_data-driven_music_history.html#a-bit-of-theory",
    "title": "16  Data-Driven Music History",
    "section": "16.2 A bit of theory",
    "text": "16.2 A bit of theory\n\nnote_names = list(\"FCGDAEB\") # diatonic note names in fifths ordering\nnote_names\n\n['F', 'C', 'G', 'D', 'A', 'E', 'B']\n\n\n\naccidentals = [\"bb\", \"b\", \"\", \"#\", \"##\"] # up to two accidentals is suffient here\naccidentals\n\n['bb', 'b', '', '#', '##']\n\n\n\nlof = [ n + a for a in accidentals for n in note_names ] # lof = \"Line of Fifths\"\nprint(lof)\n\n['Fbb', 'Cbb', 'Gbb', 'Dbb', 'Abb', 'Ebb', 'Bbb', 'Fb', 'Cb', 'Gb', 'Db', 'Ab', 'Eb', 'Bb', 'F', 'C', 'G', 'D', 'A', 'E', 'B', 'F#', 'C#', 'G#', 'D#', 'A#', 'E#', 'B#', 'F##', 'C##', 'G##', 'D##', 'A##', 'E##', 'B##']\n\n\n\nlen(lof) # how long is this line-of-fifths segment?\n\n35\n\n\nWe call the elements on the line of fifths tonal pitch-classes",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#data",
    "href": "05_data-driven_music_history.html#data",
    "title": "16  Data-Driven Music History",
    "section": "16.3 Data",
    "text": "16.3 Data\n\n16.3.1 A (kind of) large corpus: TP3C\nHere, we use a dataset that was specifically compiled for this kind of analysis, the Tonal pitch-class counts corpus (TP3C) (Moss, Neuwirth, Rohrmeier, 2020)\n\n2,012 pieces\n75 composers\napprox. spans 600 years of music history\ndoes not contain complete pieces but only counts of tonal pitch-classes\n\n\nimport pandas as pd # to work with tabular data\n\nurl = \"https://raw.githubusercontent.com/DCMLab/TP3C/master/tp3c.tsv\"\ndata = pd.read_table(url)\n\ndata.sample(10)\n\n\n\n\n\n\n\n\ncomposer\ncomposer_first\nwork_group\nwork_catalogue\nopus\nno\nmov\ntitle\ncomposition\npublication\nsource\ndisplay_year\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\n\n\n\n\n1742\nCorelli\nArcangelo\n12 Trio Sonatas\nOp.\n3\n4\n1.0\nNaN\nNaN\n1689.0\nCCARH\n1689.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n33\n75\n47\n70\n107\n108\n56\n7\n8\n24\n3\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n468\nBach\nJohann Sebastian\nInventions and Sinfonias\nBWV\n779\nNaN\nNaN\nNaN\nNaN\n1723.0\nMS\n1723.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n68\n98\n92\n77\n82\n81\n56\n17\n3\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1630\nChopin\nFrédéric\nMazurkas\nOp.\n33\n2\nNaN\nNaN\n1837.0\n1838.0\nCCARH\n1837.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n16\n15\n17\n44\n48\n32\n117\n397\n388\n251\n123\n153\n155\n54\n0\n0\n10\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1604\nJoplin\nScott\nRagtimes\nNaN\nNaN\nNaN\nNaN\nLily Queen\nNaN\n1907.0\nCCARH\n1907.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n23\n19\n51\n180\n353\n252\n134\n188\n162\n75\n55\n14\n17\n17\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n137\nBach\nJohann Sebastian\nWohltemperiertes Klavier II\nBWV\n888\n2\nNaN\nNaN\n1740.0\nNaN\nMS\n1740.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n13\n69\n108\n101\n112\n105\n114\n101\n45\n16\n10\n7\n0\n0\n0\n0\n0\n0\n0\n\n\n1231\nSchubert\nFranz\nDie schöne Müllerin\nD. 795\nNaN\n18\nNaN\nTrockne Blumen\nNaN\n1823.0\nOSLC\n1823.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n14\n86\n34\n65\n214\n309\n117\n57\n115\n60\n20\n0\n16\n0\n0\n0\n0\n0\n0\n0\n\n\n530\nBrahms\nJohannes\n8 Klavierstücke\nOp.\n76\n4\nNaN\nIntermezzo\n1878.0\nNaN\nDCML\n1878.0\n0\n0\n0\n0\n0\n0\n0\n43\n18\n64\n21\n31\n213\n113\n65\n62\n42\n70\n34\n9\n20\n4\n26\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1311\nSchumann\nRobert\nDichterliebe\nOp.\n48\n3\nNaN\nDie Rose, die Lilie, die Taube\n1840.0\nNaN\nOSLC\n1840.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n52\n69\n61\n40\n45\n52\n48\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n338\nBach\nJohann Sebastian\nWohltemperiertes Klavier I\nBWV\n863\n1\nNaN\nNaN\n1722.0\nNaN\nMS\n1722.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n9\n62\n72\n65\n90\n89\n81\n74\n29\n12\n19\n8\n0\n0\n0\n0\n0\n\n\n713\nFauré\nGabriel\nNaN\nOp.\n6\n2\nNaN\nTristesse\nNaN\n1880.0\nMS\n1880.0\n0\n0\n0\n0\n0\n0\n0\n0\n7\n0\n43\n172\n126\n90\n174\n202\n169\n85\n0\n9\n68\n0\n1\n3\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\ndata[\"display_year\"].plot(kind=\"hist\", bins=50, figsize=(15,6)); # historical overview\n\n\n\n\n\n\n\n\n\nit can be seen that there are large gaps and that some historical periods are underrepresented\nhowever, it is not so obvious how to fix that\ndo we want a uniform distribution over time?\ndo we want a “historically accurate” distribution?\ndo we want to remove geographical/gender/class/instrument/etc. biases?\non one hand, balanced datasets are likely not to reflect historical realities\non the other hand, such datasets rather represent the “canon”, that is a contemporary selection of “valuable” compositions that may differ greatly from what was considered relevant at the time\n\n–&gt; There is no unique objective answer to these questions. It is important to be aware of these limitations and take them into account when interpreting the results\nFor this workshop we ignore all the metadata about the pieces (titles, composer names etc.) but only focus on their tonal material. Therefore, we don’t need all the columns of the table.\n\ntpc_counts = data.loc[:, lof] # select all rows (\":\") and the lof columns\ntpc_counts.sample(20)\n\n\n\n\n\n\n\n\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\n\n\n\n\n155\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n32\n149\n224\n179\n194\n212\n191\n161\n52\n22\n28\n13\n1\n0\n0\n0\n0\n\n\n1073\n0\n0\n0\n0\n0\n0\n0\n0\n11\n23\n83\n189\n219\n114\n229\n225\n323\n163\n41\n58\n126\n72\n26\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1975\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n24\n42\n70\n31\n16\n36\n21\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1178\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n84\n101\n77\n111\n149\n110\n29\n9\n26\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n402\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n1\n2\n2\n9\n13\n100\n214\n103\n179\n347\n337\n250\n153\n83\n150\n87\n36\n18\n11\n6\n4\n0\n0\n0\n0\n\n\n98\n0\n0\n0\n0\n2\n0\n4\n2\n6\n41\n118\n144\n114\n164\n241\n340\n142\n44\n78\n60\n29\n8\n1\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n52\n193\n249\n123\n54\n124\n103\n10\n0\n28\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n344\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n2\n6\n38\n133\n76\n16\n21\n85\n27\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1784\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n36\n71\n68\n45\n39\n48\n33\n10\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n407\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n0\n11\n148\n180\n168\n162\n111\n161\n105\n30\n5\n13\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1992\n0\n0\n0\n0\n0\n0\n0\n0\n3\n4\n27\n138\n315\n155\n253\n327\n490\n223\n54\n54\n135\n58\n4\n0\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n543\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n3\n0\n28\n61\n36\n52\n120\n136\n59\n14\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1046\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n7\n11\n15\n4\n20\n32\n17\n14\n4\n3\n14\n3\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1032\n0\n0\n0\n0\n0\n1\n5\n4\n18\n35\n47\n132\n95\n205\n173\n155\n347\n361\n401\n499\n530\n626\n491\n263\n262\n232\n86\n35\n25\n26\n10\n2\n0\n0\n0\n\n\n737\n0\n0\n0\n0\n19\n22\n16\n41\n103\n101\n33\n48\n251\n183\n97\n31\n9\n48\n92\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n186\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n4\n2\n16\n97\n176\n198\n138\n97\n128\n88\n41\n21\n13\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1653\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n0\n16\n123\n237\n223\n172\n98\n112\n87\n29\n2\n9\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1115\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n8\n6\n41\n64\n42\n20\n48\n25\n19\n2\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1886\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n6\n149\n233\n174\n230\n269\n408\n203\n38\n24\n68\n22\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n343\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n19\n53\n102\n146\n102\n292\n549\n729\n483\n235\n377\n285\n109\n52\n9\n6\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\npiece = tpc_counts.iloc[10]\n\nfig, axes = plt.subplots(2, 1, figsize=(20,10))\n\naxes[0].bar(piece.sort_values(ascending=False).index, piece.sort_values(ascending=False))\naxes[0].set_title(\"'without theory'\")\n\naxes[1].bar(piece.index, piece)\naxes[1].set_title(\"'with theory'\")\n\n# plt.savefig(\"img/random_piece.png\")\n# plt.show()\n\nText(0.5, 1.0, \"'with theory'\")\n\n\n\n\n\n\n\n\n\nLet us have an overview of the note counts in these pieces!\nIf we would just look at the raw counts of the tonal pitch-classe, we could not learn much from it. Using a theoretical model (the line of fifths) shows that the notes in pieces are usually come from few adjacent keys (you don’t say!).\nWe probably have very long pieces (sonatas) and very short pieces (songs) in the dataset. Since we don’t want length (or the absolute number of notes in a piece) to have an effect, we rather consider tonal pitch-class distributions instead counts, by normalizing all pieces to sum to one.\n\ntpc_dists = tpc_counts.div(tpc_counts.sum(axis=1), axis=0)\ntpc_dists.sample(20)\n\n\n\n\n\n\n\n\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\n\n\n\n\n606\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.059524\n0.059524\n0.061905\n0.235714\n0.161905\n0.147619\n0.104762\n0.057143\n0.054762\n0.045238\n0.007143\n0.004762\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1844\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.005330\n0.089552\n0.171642\n0.173774\n0.143923\n0.119403\n0.148188\n0.101279\n0.023454\n0.007463\n0.011727\n0.004264\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n251\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000424\n0.027107\n0.044473\n0.035578\n0.077086\n0.159255\n0.213892\n0.154172\n0.058026\n0.085133\n0.087251\n0.043626\n0.008895\n0.002541\n0.002118\n0.000424\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n737\n0.0\n0.0\n0.0\n0.0\n0.017288\n0.020018\n0.014559\n0.037307\n0.093722\n0.091902\n0.030027\n0.043676\n0.228389\n0.166515\n0.088262\n0.028207\n0.008189\n0.043676\n0.083712\n0.003640\n0.000910\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.001466\n0.003908\n0.012702\n0.013679\n0.063019\n0.200293\n0.208109\n0.128481\n0.079140\n0.132389\n0.100147\n0.034196\n0.004885\n0.014656\n0.002931\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n822\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.022222\n0.133333\n0.155556\n0.044444\n0.266667\n0.155556\n0.133333\n0.088889\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n202\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.003268\n0.011438\n0.024510\n0.003268\n0.039216\n0.091503\n0.207516\n0.160131\n0.065359\n0.096405\n0.169935\n0.063725\n0.014706\n0.013072\n0.016340\n0.011438\n0.008170\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n850\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.081545\n0.184549\n0.184549\n0.180258\n0.124464\n0.098712\n0.133047\n0.008584\n0.004292\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1609\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.027883\n0.093790\n0.016477\n0.062104\n0.130545\n0.309252\n0.177440\n0.032953\n0.020279\n0.102662\n0.016477\n0.010139\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n555\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.002088\n0.083507\n0.171190\n0.118998\n0.068894\n0.204593\n0.173278\n0.125261\n0.006263\n0.004175\n0.033403\n0.006263\n0.002088\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n940\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.005703\n0.002852\n0.004753\n0.046578\n0.057034\n0.125475\n0.039924\n0.109316\n0.135932\n0.221483\n0.135932\n0.038023\n0.013308\n0.051331\n0.010456\n0.001901\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1165\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.002660\n0.000000\n0.106383\n0.292553\n0.156915\n0.079787\n0.130319\n0.117021\n0.063830\n0.042553\n0.007979\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1438\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.038251\n0.077869\n0.099727\n0.112022\n0.140710\n0.143443\n0.158470\n0.092896\n0.068306\n0.038251\n0.013661\n0.002732\n0.013661\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1548\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.011111\n0.011111\n0.044444\n0.077778\n0.088889\n0.088889\n0.100000\n0.088889\n0.088889\n0.088889\n0.077778\n0.077778\n0.066667\n0.066667\n0.022222\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1265\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.005924\n0.000000\n0.004739\n0.077014\n0.158768\n0.055687\n0.084123\n0.261848\n0.164692\n0.082938\n0.011848\n0.020142\n0.058057\n0.014218\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1169\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.011472\n0.024857\n0.042065\n0.055449\n0.082218\n0.084130\n0.149140\n0.128107\n0.051625\n0.080306\n0.057361\n0.051625\n0.068834\n0.045889\n0.026769\n0.013384\n0.015296\n0.009560\n0.001912\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1368\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.008929\n0.041667\n0.107143\n0.230655\n0.196429\n0.117560\n0.105655\n0.117560\n0.044643\n0.020833\n0.005952\n0.000000\n0.002976\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1741\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.003036\n0.090081\n0.178138\n0.176113\n0.153846\n0.135628\n0.122470\n0.103239\n0.012146\n0.018219\n0.007085\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1302\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.002725\n0.013624\n0.005450\n0.087193\n0.141689\n0.040872\n0.111717\n0.212534\n0.171662\n0.130790\n0.000000\n0.024523\n0.046322\n0.010899\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n80\n0.0\n0.0\n0.0\n0.0\n0.001965\n0.002947\n0.000000\n0.008841\n0.030452\n0.051081\n0.090373\n0.123772\n0.140472\n0.125737\n0.080550\n0.055992\n0.049116\n0.053045\n0.028487\n0.051081\n0.057957\n0.010806\n0.006876\n0.016699\n0.011788\n0.000000\n0.001965\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nFor further numerical analysis, we extract the data from this table and assign it to a variable X.\n\n# extract values of table to matrix\nX = tpc_dists.values\n\nX.shape # shows (#rows, #columns) of X\n\n(2012, 35)\n\n\nNow, X is a 2012 \\(\\times\\) 35 matrix where the rows represent the pieces and the columns (also called “features” or “dimensions”) represent the relative frequency of tonal pitch-classes.\nThinking in 35 dimensions is quite difficult for most people. Without trying to imagine what this would look like, what can we already say about this data?\nSince each piece is a point in this 35-D space and pieces are represented as vectors, pieces that have similar tonal pitch-class distributions must be close in this space (whatever this looks like).\nWhat groups of pieces that cluster together? Maybe pieces of the same composer are similar to each other? Maybe pieces from a similar time? Maybe pieces for the same instruments?\nIf we find clusters, these would still be in 35-D and thus difficult to interpret. Luckily, there are a range of so-called dimensionality reduction methods that transform the data into lower-dimensional spaces so that we actually can look at them.\nA very common dimensionality reduction method is Principal Components Analysis (PCA).\nThe basic idea of PCA is:\n\nfind dimensions in the data that maximize the variance in this direction\nthese dimensions have to be orthogonal to each other (mutually independent)\nthese dimensions are called the principal components\neach principal component is associated with how much of the data variance it explains\n\n\nimport numpy as np # for numerical computations\nimport sklearn\nfrom sklearn.decomposition import PCA # for dimensionality reduction\n\npca = sklearn.decomposition.PCA(n_components=35) # initialize PCA with 35 dimensions\npca.fit(X) # apply it to the data\nvariance = pca.explained_variance_ratio_ # assign explained variance to variable\n\n\nfig, ax = plt.subplots(figsize=(14,5))\nx = np.arange(35)\nax.plot(x, variance, label=\"relative\", marker=\"o\")\nax.plot(x, variance.cumsum(), label=\"cumulative\", marker=\"o\")\nax.set_xlim(-0.5, 35)\nax.set_ylim(-0.1, 1.1)\nax.set_xlabel(\"Principal Components\")\nax.set_ylabel(\"Explained variance\")\nplt.xticks(np.arange(len(lof)), np.arange(len(lof)) + 1) # because Pyhon starts counting at 0\n\nplt.legend(loc=\"center right\")\nplt.tight_layout()\n# plt.savefig(\"img/explained_variance.png\")\n# plt.show()\n\n\n\n\n\n\n\n\n\nvariance[:5]\n\narray([0.41144591, 0.23410347, 0.09063507, 0.07574242, 0.04436989])\n\n\nThe first principal component explains 41.1% of the variance of the data, the second explains 23.4% and the third 9%. Together, this amounts to 73.6%.\nAlmost three quarters of the variance in the dataset is retained by reducing the dimensionality from 35 to 3 dimensions (8.6%)! If we reduce the data to two dimensions, we still can explain \\(\\approx\\) 65% of the variance.\nThis is great because it means that we can look at the data in 2 or 3 dimensions without loosing too much information.",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#recovering-the-line-of-fifths-from-data",
    "href": "05_data-driven_music_history.html#recovering-the-line-of-fifths-from-data",
    "title": "16  Data-Driven Music History",
    "section": "16.4 Recovering the line of fifths from data",
    "text": "16.4 Recovering the line of fifths from data\n\npca3d = PCA(n_components=3)\npca3d.fit(X)\n\nX_ = pca3d.transform(X)\nX_.shape\n\n(2012, 3)\n\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(6,6))\n\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_[:,0], X_[:,1], X_[:,2], s=50, alpha=.25) # c=cs,\nax.set_xlabel(\"PC 1\", labelpad=30)\nax.set_ylabel(\"PC 2\", labelpad=30)\nax.set_zlabel(\"PC 3\", labelpad=30)\n\nplt.tight_layout()\n# plt.savefig(\"img/3d_scatter.png\")\n# plt.show()\n\n\n\n\n\n\n\n\nEach piece in this plot is represented by a point in 3-D space. But remember that this location represents ~75% of the information contained in the full tonal pitch-class distribution. In 35-D space each dimension corresponded to the relative frequency of a tonal pitch-class in a piece.\n\nWhat do these three dimensions signify?\nHow can we interpret them?\n\nFortunately, we can inspect them individually and try to interpret what we see.\n\nfrom itertools import combinations\n\nfig, axes = plt.subplots(1,3, sharey=True, figsize=(24,8))\n\nfor k, (i, j) in enumerate(combinations(range(3), 2)):\n\n    axes[k].scatter(X_[:,i], X_[:,j], s=50, alpha=.25, edgecolor=None)\n    axes[k].set_xlabel(f\"PC {i+1}\")\n    axes[k].set_ylabel(f\"PC {j+1}\")\n    axes[k].set_aspect(\"equal\")\n\nplt.tight_layout()\n# plt.savefig(\"img/3d_dimension_pairs.png\")\n# plt.show()\n\n\n\n\n\n\n\n\nClearly, looking at two principal components at a time shows that there is some latent structure in the data. How can we understand it better?\nOne way to see whether the pieces are clustered together systematically be coloring them according to some criterion.\nAs always, many different options are available. For the present purpose we will use the most simple summary of the piece: its most frequent note (which is the mode of its pitch-class distribution in statistical terms) and call this note its tonal center.\nThis will also allow to map the tonal pitch-classes on the line of fifths to colors.\n\ntpc_dists[\"tonal_center\"] = tpc_dists.apply(lambda piece: np.argmax(piece[lof].values) - 15, axis=1)\ntpc_dists.sample(10)\n\n\n\n\n\n\n\n\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\ntonal_center\n\n\n\n\n1990\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.002752\n0.004587\n0.015138\n0.088991\n0.138991\n0.086239\n0.117431\n0.145413\n0.195413\n0.094037\n0.015596\n0.021560\n0.055505\n0.016972\n0.001376\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n1\n\n\n364\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.001026\n0.006028\n0.020521\n0.037194\n0.037450\n0.016673\n0.087598\n0.158138\n0.225728\n0.106579\n0.106066\n0.094395\n0.078107\n0.013467\n0.002693\n0.004232\n0.003335\n0.000770\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n1\n\n\n1857\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.007370\n0.031322\n0.103639\n0.081069\n0.051589\n0.111469\n0.177338\n0.224781\n0.087517\n0.038692\n0.030401\n0.037310\n0.017503\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n\n\n1045\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.004902\n0.000000\n0.000000\n0.122549\n0.166667\n0.151961\n0.117647\n0.127451\n0.142157\n0.102941\n0.019608\n0.019608\n0.009804\n0.014706\n0.0\n0.0\n0.0\n0.0\n0.0\n5\n\n\n1246\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.01232\n0.020534\n0.008214\n0.131417\n0.185832\n0.320329\n0.091376\n0.030801\n0.128337\n0.070842\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n-3\n\n\n214\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.001401\n0.022409\n0.082633\n0.122549\n0.086835\n0.147759\n0.169468\n0.156863\n0.111345\n0.032913\n0.020308\n0.037815\n0.007003\n0.000700\n0.0\n0.0\n0.0\n0.0\n0.0\n7\n\n\n167\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000388\n0.028306\n0.053121\n0.065142\n0.099651\n0.153160\n0.186506\n0.141528\n0.075611\n0.089182\n0.072896\n0.031020\n0.001939\n0.000775\n0.000775\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n4\n\n\n570\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.004454\n0.028211\n0.006682\n0.092799\n0.171492\n0.178916\n0.163326\n0.086117\n0.112843\n0.121752\n0.029696\n0.003712\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n3\n\n\n586\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.001506\n0.006274\n0.041405\n0.095609\n0.121957\n0.107152\n0.117440\n0.169887\n0.146048\n0.090088\n0.039649\n0.021330\n0.025094\n0.016562\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n684\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.001412\n0.072034\n0.159605\n0.190678\n0.176554\n0.105932\n0.149718\n0.103107\n0.026836\n0.011299\n0.002825\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n1\n\n\n\n\n\n\n\n\nfrom matplotlib import cm\nfrom matplotlib.colors import Normalize\n\n#normalize item number values to colormap\nnorm = Normalize(vmin=-15, vmax=20)\n\n# cs = [ cm.seismic(norm(c)) for c in data[\"tonal_center\"]]\ncs = [ cm.seismic(norm(c)) for c in tpc_dists[\"tonal_center\"]]\n\n\nfrom itertools import combinations\n\nfig, axes = plt.subplots(1,3, sharey=True, figsize=(24,8))\n\nfor k, (i, j) in enumerate(combinations(range(3), 2)):\n\n    axes[k].scatter(X_[:,i], X_[:,j], s=50, c=[ np.abs(c) for c in cs], edgecolor=None)\n    axes[k].set_xlabel(f\"PC {i}\")\n    axes[k].set_ylabel(f\"PC {j}\")\n    axes[k].set_aspect(\"equal\")\n\nplt.tight_layout()\n# plt.savefig(\"img/3d_dimension_pairs_colored.png\")\n# plt.show()",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#historical-development-of-tonality",
    "href": "05_data-driven_music_history.html#historical-development-of-tonality",
    "title": "16  Data-Driven Music History",
    "section": "16.5 Historical development of tonality",
    "text": "16.5 Historical development of tonality\nThe line of fifths is an important underlying structure for pitch-class distributions in tonal compositions\nBut we have treated all pieces in our dataset as synchronic and have not yet taken their historical location into account.\nLet’s assume the pitch-class content of a piece spreads on the line of fifths from F to A\\(\\sharp\\). This means, its range on the line of fifths is \\(10 - (-1) = 11\\). The piece covers eleven consecutive fifths on the lof.\nWe can generalize this calculation and write a function that calculates the range for each piece in the dataset.\n\ndef lof_range(piece):\n    l = [i for i, v in enumerate(piece) if v!=0]\n    return max(l) - min(l)\n\n\ndata[\"lof_range\"] = data.loc[:, lof].apply(lof_range, axis=1) # create a new column\ndata.sample(20)\n\n\n\n\n\n\n\n\ncomposer\ncomposer_first\nwork_group\nwork_catalogue\nopus\nno\nmov\ntitle\ncomposition\npublication\nsource\ndisplay_year\nFbb\nCbb\nGbb\nDbb\nAbb\nEbb\nBbb\nFb\nCb\nGb\nDb\nAb\nEb\nBb\nF\nC\nG\nD\nA\nE\nB\nF#\nC#\nG#\nD#\nA#\nE#\nB#\nF##\nC##\nG##\nD##\nA##\nE##\nB##\nlof_range\n\n\n\n\n992\nAgricola\nAlexander\nMissa Malheur me bat\nNaN\nNaN\nNaN\nNaN\nGloria\nNaN\n1506.0\nELVIS\n1506.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n218\n386\n327\n274\n333\n351\n298\n5\n3\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n1772\nCorelli\nArcangelo\n12 Trio Sonatas\nOp.\n4\n1\n3.0\nNaN\nNaN\n1694.0\nCCARH\n1694.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n28\n50\n47\n27\n39\n53\n38\n4\n2\n7\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n1199\nScarlatti\nDomenico\nSonata\nK\n64\nNaN\nNaN\nNaN\nNaN\nNaN\nMS\n1721.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n30\n62\n32\n60\n89\n129\n72\n17\n3\n22\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n1361\nScriabin\nAlexander\nPréludes\nOp.\n13\n2\nNaN\n6 Preludes\n1895.0\nNaN\nDCML\n1895.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n6\n78\n61\n27\n40\n91\n70\n46\n4\n3\n12\n16\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n\n\n1490\nTchaikovsky\nPyotr\nThe Seasons\nOp.\n37a\n3\nNaN\nMarch: Song of the Lark\n1876.0\nNaN\nMS\n1876.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n3\n0\n0\n24\n107\n17\n48\n126\n158\n71\n1\n4\n52\n13\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n\n\n387\nChopin\nFrédéric\nPréludes\nOp.\n28\n6\nNaN\n24 Préludes\n1839.0\nNaN\nMS\n1839.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n20\n41\n80\n4\n28\n106\n66\n33\n3\n0\n22\n2\n0\n1\n0\n0\n0\n0\n0\n0\n14\n\n\n290\nAlkan\nCharles Valentin\nPréludes\nOp.\n31\n13\nNaN\nNaN\n1846.0\nNaN\nMS\n1846.0\n0\n0\n0\n0\n1\n7\n22\n3\n108\n201\n263\n110\n69\n228\n150\n10\n1\n20\n6\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n15\n\n\n478\nBach\nJohann Sebastian\nInventions and Sinfonias\nBWV\n789\nNaN\nNaN\nNaN\nNaN\n1723.0\nMS\n1723.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n23\n82\n77\n103\n98\n101\n109\n68\n23\n13\n8\n5\n0\n0\n0\n0\n0\n0\n0\n0\n12\n\n\n1947\nMozart\nWolfgang Amadeus\nSonaten\nKV\n283\n5\n3.0\nNaN\n1774.0\nNaN\nCCARH\n1774.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n9\n29\n183\n367\n317\n245\n184\n269\n199\n70\n33\n32\n38\n1\n0\n0\n0\n0\n0\n0\n0\n0\n14\n\n\n601\nCouperin\nFrançois\nTroisième livre de pièces de Clavecin\nNaN\nNaN\n18\n4.0\nLe petit rien\nNaN\n1722.0\nMS\n1722.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n29\n55\n56\n51\n28\n44\n29\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n8\n\n\n974\nOckeghem\nJeanDe\nMissa Fors Seulement\nNaN\nNaN\nNaN\nNaN\nKyrie\n1497.0\nNaN\nELVIS\n1497.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n25\n131\n124\n106\n158\n167\n104\n60\n0\n5\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n1430\nVictoria\nTomasLuisde\nNaN\nNaN\nNaN\nNaN\nNaN\nSepulto Domino\nNaN\n1585.0\nELVIS\n1585.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n13\n1\n0\n19\n56\n30\n47\n79\n108\n49\n15\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n\n\n528\nBrahms\nJohannes\n8 Klavierstücke\nOp.\n76\n2\nNaN\nCapriccio\n1878.0\nNaN\nDCML\n1878.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n2\n8\n18\n44\n53\n147\n144\n116\n197\n233\n307\n203\n88\n83\n95\n50\n9\n0\n2\n0\n1\n0\n0\n0\n22\n\n\n1765\nCorelli\nArcangelo\n12 Trio Sonatas\nOp.\n3\n8\n4.0\nNaN\nNaN\n1689.0\nCCARH\n1689.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n163\n156\n207\n106\n170\n195\n127\n12\n0\n6\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n\n\n1300\nSchumann\nClara\nSechs Lieder\nOp.\n23\n5\nNaN\nDas ist der Tag, der klingen mag\n1853.0\nNaN\nOSLC\n1853.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n29\n126\n174\n93\n79\n106\n58\n30\n7\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n9\n\n\n204\nLiszt\nFranz\n12 Transcendental Etudes\nS.\n139\n3\nNaN\nPaysage\n1851.0\nNaN\nMS\n1851.0\n0\n0\n0\n0\n0\n10\n2\n3\n10\n36\n109\n66\n72\n168\n519\n243\n124\n173\n279\n115\n42\n13\n20\n14\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n19\n\n\n1858\nGrieg\nEdvard\nLyrical Pieces\nOp.\n65\n6\nNaN\nBryllupsdag\n1896.0\n1897.0\nDCML\n1896.0\n0\n0\n0\n0\n0\n0\n0\n0\n12\n52\n68\n236\n256\n104\n112\n243\n415\n582\n555\n254\n346\n315\n110\n33\n13\n7\n1\n1\n0\n0\n0\n0\n0\n0\n0\n19\n\n\n468\nBach\nJohann Sebastian\nInventions and Sinfonias\nBWV\n779\nNaN\nNaN\nNaN\nNaN\n1723.0\nMS\n1723.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n17\n68\n98\n92\n77\n82\n81\n56\n17\n3\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n\n\n439\nVictoria\nTomasLuisde\nNaN\nNaN\nNaN\nNaN\nNaN\nAleph. Quomodo obscuratum\nNaN\n1585.0\nELVIS\n1585.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n33\n0\n2\n82\n117\n82\n73\n195\n149\n77\n15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n11\n\n\n441\nAlkan\nCharles Valentin\nNaN\nOp.\n25\nNaN\nNaN\nAlleluia\nNaN\n1844.0\nMS\n1844.0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n140\n0\n17\n231\n420\n309\n269\n166\n552\n235\n82\n35\n9\n35\n13\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n14\n\n\n\n\n\n\n\nThis allows us now to take the display_year (composition or publication) and lof_range (range on the line of fifths) features to observe historical changes.\n\nfig, ax = plt.subplots(figsize=(18,9))\nax.scatter(data[\"display_year\"].values, data[\"lof_range\"].values, alpha=.5, s=50)\nax.set_ylim(0,35)\nax.set_xlabel(\"year\")\nax.set_ylabel(\"line-of-fifths range\");\n# plt.savefig(\"img/hist_scatter.png\");\n\n\n\n\n\n\n\n\nWe could try to fit a line to this data to see whether there is a trend (kinda obvious here).\n\ng = sns.lmplot(\n    data=data, \n    x=\"display_year\", \n    y=\"lof_range\", \n    line_kws={\"color\":\"k\"},\n    scatter_kws={\"alpha\":.5},\n#     lowess=True,\n    height=8,\n    aspect=2\n);\n# g.savefig(\"img/hist_scatter_line.png\");\n\n\n\n\n\n\n\n\nBut actually, this is not the best idea. Why should any historical process be linear? More complex models might make more sense.\nA more versatile technique is Locally Weighted Scatterplot Smoothing (LOWESS) that locally fits a polynomial. Using this method, we see that a non-linear process is displayed.\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\nx = data.display_year\ny = data.lof_range\nl = lowess(y,x)\n\nfig, ax = plt.subplots(figsize=(15,10))\n\nax.scatter(x,y, s=50)\nax.plot(l[:,0], l[:,1], c=\"k\")\nax.set_ylabel(\"line-of-fifths range\");\n# plt.savefig(\"img/hist_scatter_lowess.png\")\n# plt.show()",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#if-there-is-time-some-more-advanced-stuff",
    "href": "05_data-driven_music_history.html#if-there-is-time-some-more-advanced-stuff",
    "title": "16  Data-Driven Music History",
    "section": "16.6 If there is time: some more advanced stuff",
    "text": "16.6 If there is time: some more advanced stuff\n\nB = 200\ndelta = 1/10 \n\nfig, ax = plt.subplots(figsize=(16,9))\n\nx = data.display_year\ny = data.lof_range\nl = lowess(y,x, frac=delta)\n\nax.scatter(x,y, s=50, alpha=.25)\n\nfor _ in range(B):\n    resampled = data.sample(data.shape[0], replace=True)\n    \n    xx = resampled.display_year\n    yy = resampled.lof_range\n    ll = lowess(yy,xx, frac=delta)\n    \n    ax.plot(ll[:,0], ll[:,1], c=\"k\", alpha=.05)\n    \nax.plot(l[:,0], l[:,1], c=\"yellow\")\n\n## REGIONS\nfrom matplotlib.patches import Rectangle\n\ntext_kws = {\n    \"rotation\" : 90,\n    \"fontsize\" : 16,\n    \"bbox\" : dict(\n        facecolor=\"white\", \n        boxstyle=\"round\"\n    ),\n    \"horizontalalignment\" : \"center\",\n    \"verticalalignment\" : \"center\"\n}\n\nrect_props = {\n    \"width\" : 40,\n    \"zorder\" : -1,\n    \"alpha\" : 1.\n}\n\nstylecolors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nax.text(1980, 3, \"diatonic\", **text_kws)\nax.axhline(6.5, c=\"gray\", linestyle=\"--\", lw=2) # dia / chrom.\nax.add_patch(Rectangle((1960,0), height=6.5, facecolor=stylecolors[0], **rect_props))\n\nax.text(1980, 9.5, \"chromatic\", **text_kws)\nax.axhline(12.5, c=\"gray\", linestyle=\"--\", lw=2) # chr. / enh.\nax.add_patch(Rectangle((1960,6.5), height=6, facecolor=stylecolors[1], **rect_props))\n\nax.text(1980, 23.5, \"enharmonic\", **text_kws)\nax.add_patch(Rectangle((1960,12.5), height=28, facecolor=stylecolors[2], **rect_props))\n\nax.set_ylim(0,35)\nax.set_xlim(1300,2000)\n\nax.set_ylabel(\"line-of-fifths range\");\n# plt.savefig(\"img/final.png\", dpi=300)\n# plt.show()\n\n\n\n\n\n\n\n\nUsung bootstrap sampling we achieve an estimation of the local varience of the data and thus of the diversity in the note usage of the musical pieces.\nWe also can distinguish three regions in terms of line-of-fifth range: diatonic, chromatic, and enharmonic.\nGrouping the data together in these three regions, we see a clear change from diatonic and chromatic to chromatic and enharmonic pieces over the course of history.\n\nepochs = {\n    \"Renaissance\" : [1300, 1549],\n    \"Baroque\" : [1550, 1649],\n    \"Classical\" : [1650, 1749],\n    \"Early\\nRomantic\" : [1750, 1819],\n    \"Late Romantic/\\nModern\" : [1820, 2000]\n}   \n\nstrata = [\n    \"diatonic\",\n    \"chromatic\",\n    \"enharmonic\"\n]\n\nwidths = data[[\"display_year\", \"lof_range\"]].sort_values(by=\"display_year\").reset_index(drop=True)\n\ndf = pd.concat(\n    [\n        widths[ \n            (widths.display_year &gt;= epochs[e][0]) & (widths.display_year &lt;= epochs[e][1]) \n        ][\"lof_range\"].value_counts(normalize=True).sort_index().groupby( \n            lambda x: strata[0] if x &lt;= 6 else strata[1] if x &lt;= 12 else strata[2]\n        ).sum() for e in epochs\n    ], axis=1, sort=True\n)\n\ndf.columns = epochs.keys()\ndf = df.reindex(strata)\ndf.T.plot(kind=\"bar\", stacked=True, figsize=(12,5))\n# plt.title(\"Epochs\")\nplt.legend(bbox_to_anchor=(1.3,0.75))\nplt.gca().set_xticklabels(epochs.keys(), rotation=\"horizontal\")\nplt.tight_layout()\n# plt.savefig(\"img/epochs_regions.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nRenaissance: largest diatonic proportion overall but mostly chromatic\nBaroque: alost completely chromatic\nClassical: enharmonic proportion increases -&gt; more distant modulations\nThis trend continues through the Romantic eras",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "05_data-driven_music_history.html#summary",
    "href": "05_data-driven_music_history.html#summary",
    "title": "16  Data-Driven Music History",
    "section": "16.7 Summary",
    "text": "16.7 Summary\n\nWe have analyzed a very specific aspect of Western classical music.\nWe have used a large(-ish) corpus to answer our research question.\nWe have operationalized musical pieces as vectors that represent distributions of tonal pitch-classes.\nWe have used the dimensionality-reduction technique Principal Component Analysis (PCA) in order to visually inspect the distribution of the data in 2 and 3 dimensions.\nWe have used music-theoretical domain knowledge to find meaningful structure in this space.\nWe have seen that pieces are largely distributed along the line of fifths.\nWe have used Locally Weighted Scatterplot Smoothing (LOWESS) to estimate the variance in this historical process.\nWe have seen that, historically, composers explore ever larger regions on this line and that the variance also increases.",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data-Driven Music History</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "INTRODUCTION TO DIGITAL MUSICOLOGY",
    "section": "",
    "text": "Home\n\nThis page contains material for the course Introduction to Digital Musicology, held at Julius-Maximilians-Universität, Würzburg (Germany) in Fall 2025.\n\n\n\n\n\n\nWarning\n\n\n\nThese pages are work in progress and will be continuously updated.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to refer to these pages, you can cite them as follows:\nMoss, F. C. (2025). Introduction to Digital Musicology. https://fabianmoss.github.io/intro-digimus",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "01_what_is_dm.html",
    "href": "01_what_is_dm.html",
    "title": "1  What is Digital Musicology?",
    "section": "",
    "text": "1.1 Introduction\nVirtual all aspects of our daily lives are increasingly shaped by data and algorithms. The ‘digital turn’ has also not stopped before academia and science, and most fields are more and more engaging with these new methodologies.\nDigital Musicology (DM) is a term that refers to music research that draws on digital data sandstonsimplesketchslatsolaspacelasuperherunitevapoyetzephyr simplex digital methods to answer its researchquartzrtz printinmonokagithub is difficult, and there is no consensus to date. Moreover, a range of similar terms are frequently being used, for example, Computational Musicology.",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Digital Musicology?</span>"
    ]
  },
  {
    "objectID": "01_what_is_dm.html#overview-of-the-field",
    "href": "01_what_is_dm.html#overview-of-the-field",
    "title": "1  What is Digital Musicology?",
    "section": "1.2 Overview of the field",
    "text": "1.2 Overview of the field\nFaced with the fact that no common definition of these terms exist, Kris Shaffer attempted to describe some of the main aspects associated with Computational Musicology (Schaffer 2016). He names the following subfields, which we will briefly discuss in turn (in an adapted order).\n\n1.2.1 Music Encoding\nAt the beginning of any digital approach to music stands the question how music is to be represented digitally. Music encoding in the narrower sense asks, how symbolic musical notations such as Common Western Music Notation (CWMN) can be translated into a computer-readable form.\n\n\n1.2.2 Corpus Studies\nAlthough corpus studies are considered a modern form of music theory, they can look back an an astonishingly long history. Many consider the first corpus study to be Jeppesen’s study The Style of Palestrina and the Dissonance, where he counts and tabulates occurrences of dissonant vertical intervals in the vocal work of the Renaissance composer (Jeppesen 1927). Further early examples are the dissertations of Budge (1943) and Norman (1945).\nIn a nutshell, corpus attempt to find regularities in large collections of music data (Shanahan, Burgoyne, and Quinn 2022) in order to draw conclusions about the style of a particular composer, period, or genre, for instance. Corpus studies thus naturally make use of digital data and statistical or algorithmic methods for their analysis. Corpus studies often try to generalize music theoretical questions from the analysis of individual pieces to entire corpora. They often report their findings in statistical tables or graphical visualizations.\n\n\n1.2.3 Music Information Retrieval\n\n\n1.2.4 Modeling\n(mention here also other forms of computational modeling and computational musicology)\n\n\n\n\n\n\n\nNote\n\n\n\nThe aspects mentioned above are only part of what people nowadays may understand by Computational Musicology. In that sense, Computational Musicology is part of the larger movement of Computational Science, an endeavour that embeds computational methods and thinking into domain-specific areas of research (Wing 2006). If we have this extended view, we also need to include computational models of music perception and cognition, acoustics, and enthnological and anthropological approaches to music as well. However, this would go beyond the scope of this course.\n\n\nIn this course, we focus on the following aspects of Digital Musicology, each of which forms an integral part of its design:\n\nData about music:\nMusic as data: How can music, an ephemeral expression of human culture, be captured or represented as digital data on a computer? What kinds of decisions do we have to make? Which musical aspects are retained and which are lost? Is music as data translatable?\nWorking with music data:\nCritical Digital Musicology: What kinds of ethical and societal issues do we touch upon when doing Digital Musicology? Which important questions do we need to consider before planning or executing a research study?\n\ne.g. digital vs computational; the latter in 2nd semester\ndigital vs empirical vs quantiative\nhow does DM relate to “traditional” subdivisions of musicology?\n\n\n\n\n\n\n\n\nHomework\n\n\n\n\nRead Schaffer (2016).\nDiscuss with your peers unclear terms and find answers.\nFind aspects of Computational Musicology that you think are underrepresented in Schaffer’s list.\n\n\n\n\n\n\n\nBudge, Helen. 1943. “A Study of Chord Frequencies Based on Music of Representative Composers of the Eighteenth and Nineteenth Centuries.” PhD thesis, Columbia University.\n\n\nJeppesen, Knud. 1927. The Style of Palestrina and the Dissonance. 1st ed. New York: Oxford University Press.\n\n\nNorman, Philip B. 1945. A Quantitative Study of Harmonic Similarities in Certain Specified Works of Bach, Beethoven, and Wagner. C. Fischer, Incorporated.\n\n\nSchaffer, Kris. 2016. “What Is Computational Musicology?” https://medium.com/@krisshaffer/what-is-computational-musicology-f25ee0a65102.\n\n\nShanahan, Daniel, John Ashley Burgoyne, and Ian Quinn, eds. 2022. Oxford Handbook of Music and Corpus Studies. Oxford: Oxford University Press.\n\n\nWing, Jeannette M. 2006. “Computational Thinking.” Commun. ACM 49 (3): 33–35. https://doi.org/10.1145/1118178.1118215.",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Digital Musicology?</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html",
    "href": "02_dm_today.html",
    "title": "2  Digital Musicology today",
    "section": "",
    "text": "2.1 Important institutions and people (also, e.g. NFDI4Culture)",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#important-institutions-and-people-also-e.g.-nfdi4culture",
    "href": "02_dm_today.html#important-institutions-and-people-also-e.g.-nfdi4culture",
    "title": "2  Digital Musicology today",
    "section": "",
    "text": "DCML\nPaderborn\nStanford\nQueen Mary\nLinz",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#current-research-topics",
    "href": "02_dm_today.html#current-research-topics",
    "title": "2  Digital Musicology today",
    "section": "2.2 Current research topics",
    "text": "2.2 Current research topics",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#central-journals-and-conferences",
    "href": "02_dm_today.html#central-journals-and-conferences",
    "title": "2  Digital Musicology today",
    "section": "2.3 Central journals and conferences",
    "text": "2.3 Central journals and conferences\n\n2.3.1 Journals\n\nEMR\nMusic Perception\nMusicae Scientiae\nMusic & Science\nMusic Theory Online\nMusic Theory Spectrum\n\n\n\n2.3.2 Conferences\n\nIMS Digital Musicology\nISMIR\nDLfM\nICMPC\nICCCM\nCMMR\nCHR\nSMC\nMEC\nDH Conference",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "02_dm_today.html#important-tools",
    "href": "02_dm_today.html#important-tools",
    "title": "2  Digital Musicology today",
    "section": "2.4 Important tools",
    "text": "2.4 Important tools\n\nmusic21\nVerovio",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Digital Musicology today</span>"
    ]
  },
  {
    "objectID": "03_history_of_dm.html",
    "href": "03_history_of_dm.html",
    "title": "3  The history of Digital Musicology",
    "section": "",
    "text": "Goal\n\n\n\nKnowing the beginnings and the major stages of DM.",
    "crumbs": [
      "INTRODUCTION",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The history of Digital Musicology</span>"
    ]
  },
  {
    "objectID": "04_RISM.html",
    "href": "04_RISM.html",
    "title": "4  RISM metadata",
    "section": "",
    "text": "Goal\n\n\n\nLearn what metadata are and how to search for music sources on RISM Online.\n\n\n\nWhat is RISM?\nWhat is RISM Online?\n\n\n\n\n\n\n\nExercise\n\n\n\nUnderstand basic SPARQL and design queries via prompting.",
    "crumbs": [
      "DATA ABOUT MUSIC",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>RISM metadata</span>"
    ]
  },
  {
    "objectID": "05_spotify_musicbrainz.html",
    "href": "05_spotify_musicbrainz.html",
    "title": "5  Spotify and MusicBrainz metadata",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand the kind of metadata provided by Spotify vs MusicBrainz.",
    "crumbs": [
      "DATA ABOUT MUSIC",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spotify and MusicBrainz metadata</span>"
    ]
  },
  {
    "objectID": "06_streaming_industry.html",
    "href": "06_streaming_industry.html",
    "title": "6  Music and the streaming industry",
    "section": "",
    "text": "Goal\n\n\n\nGain first insights into the music market and its workings.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWork with sales data.",
    "crumbs": [
      "DATA ABOUT MUSIC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Music and the streaming industry</span>"
    ]
  },
  {
    "objectID": "07_audio.html",
    "href": "07_audio.html",
    "title": "8  Audio",
    "section": "",
    "text": "8.1 pure tones\n\\[x(t) = A \\sin(2\\pi ft + phi)\\]",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "07_audio.html#pure-tones",
    "href": "07_audio.html#pure-tones",
    "title": "8  Audio",
    "section": "",
    "text": "amplidute \\(A\\)\nfrequency \\(f\\)\nlimits of hearing, Audible range and volume\nintervals: octave and fifth\nphase \\(\\phi\\)",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "07_audio.html#harmonics",
    "href": "07_audio.html#harmonics",
    "title": "8  Audio",
    "section": "8.2 Harmonics",
    "text": "8.2 Harmonics\nAdding pure tones and decomposition via Fourier (link to 3b1b)\n\nTimbre\nWaveform to spectrogram\nreading melodies from a spectrogram",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "07_audio.html#digital-audio",
    "href": "07_audio.html#digital-audio",
    "title": "8  Audio",
    "section": "8.3 Digital audio",
    "text": "8.3 Digital audio\n\nSampling\n\n\n\n\n\n\n\nFurther reading\n\n\n\nExcellent introductions can be found in Sethares (2005), Müller (2015), and Eerola (2025).\n\n\n\n\n\n\nEerola, T. 2025. Music and Science: A Guide to Empirical Music Research. SEMPRE Studies in the Psychology of Music. London, UK: Routledge.\n\n\nMüller, Meinard. 2015. Fundamentals of Music Processing: Audio, Analysis, Algorithms, Applications. Springer International Publishing. https://doi.org/10.1007/978-3-319-21945-5.\n\n\nSethares, William A. 2005. Tuning, Timbre, Spectrum, Scale. 2nd ed. London: Springer.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Audio</span>"
    ]
  },
  {
    "objectID": "08_MIDI.html",
    "href": "08_MIDI.html",
    "title": "9  MIDI",
    "section": "",
    "text": "Goal\n\n\n\nBe able to name use cases for MIDI. Translate MIDI numbers to pitches.",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>MIDI</span>"
    ]
  },
  {
    "objectID": "09_MEI_header.html",
    "href": "09_MEI_header.html",
    "title": "10  MEI - header",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand basic XML encoding and the skeleton structure of MEI.\n\n\n\nmei friend",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>MEI - header</span>"
    ]
  },
  {
    "objectID": "10_MEI_body.html",
    "href": "10_MEI_body.html",
    "title": "11  MEI - the body",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand the relation between CWMN and the MEI music element.\n\n\n\nMuseScore export\nmei friend",
    "crumbs": [
      "MUSIC AS DATA",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>MEI - the body</span>"
    ]
  },
  {
    "objectID": "11_dma_harmony.html",
    "href": "11_dma_harmony.html",
    "title": "12  Digital music analysis: harmony",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand what labeling is and why labels can be useful.\n\n\n\nfurther MuseScore practice\nsegmentation and labeling\nCounting chords, finding cadences",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Digital music analysis: harmony</span>"
    ]
  },
  {
    "objectID": "12_dma_melody.html",
    "href": "12_dma_melody.html",
    "title": "13  Digital music analysis: melody",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand how melodic pattern matching works in principle.\n\n\n\nPattern finding in melodies (Non-Western)",
    "crumbs": [
      "WORKING WITH MUSIC DATA",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Digital music analysis: melody</span>"
    ]
  },
  {
    "objectID": "13_copyright.html",
    "href": "13_copyright.html",
    "title": "17  Copyright",
    "section": "",
    "text": "Goal\n\n\n\nKnow a few famous copyright infringement cases and why data analysis is important here.\n\n\n\nPlagiarism cases and copyright",
    "crumbs": [
      "CRITICAL DIGITAL MUSICOLOGY",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Copyright</span>"
    ]
  },
  {
    "objectID": "14_representation.html",
    "href": "14_representation.html",
    "title": "18  Representation and representativeness",
    "section": "",
    "text": "Goal\n\n\n\nUnderstand the difference between representativeness and representation. Obtain a critical understanding of biases relevant for data selection.\n\n\n\nRepresentation and the canon\nRepresenting means modeling means abstraction (what is “music” in “music encoding”?)\nbiases: how to recognize them, how to deal with them, and when biases are a good thing.\nFAIR and CARE",
    "crumbs": [
      "CRITICAL DIGITAL MUSICOLOGY",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Representation and representativeness</span>"
    ]
  },
  {
    "objectID": "15_discussion.html",
    "href": "15_discussion.html",
    "title": "19  Discussion",
    "section": "",
    "text": "Goal",
    "crumbs": [
      "CRITICAL DIGITAL MUSICOLOGY",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "01_exercise.html",
    "href": "01_exercise.html",
    "title": "20  Exercise for Week 1",
    "section": "",
    "text": "In the remainder of the course, we will engage with data and computer code. While this is probably new for most of you, there are luckily many tools that can help us to do so more easily.\n\nCreate a directory/folder named intro-digimus for this course (note: no spaces!). You can use any location on your personal computer or on your J: drive provided by the university.\nAs code editor we will use Microsoft Visual Studio Code. Install the program on your machine.\nOpen the directory you just created and install the Jupyter Extension.\nCreate a new Jupyter notebook named test.ipynb and write the following in the first cell: print(\"Hello world!\"). Congratulations, you wrote your first computer program!",
    "crumbs": [
      "EXERCISES",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Exercise for Week 1</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Budge, Helen. 1943. “A Study of Chord\nFrequencies Based on Music of Representative\nComposers of the Eighteenth and\nNineteenth Centuries.” PhD thesis, Columbia\nUniversity.\n\n\nEerola, T. 2025. Music and Science: A Guide to Empirical Music\nResearch. SEMPRE Studies in the Psychology of Music.\nLondon, UK: Routledge.\n\n\nJeppesen, Knud. 1927. The Style of\nPalestrina and the Dissonance. 1st ed.\nNew York: Oxford University Press.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nMüller, Meinard. 2015. Fundamentals of Music\nProcessing: Audio, Analysis,\nAlgorithms, Applications. Springer\nInternational Publishing. https://doi.org/10.1007/978-3-319-21945-5.\n\n\nNorman, Philip B. 1945. A Quantitative Study of\nHarmonic Similarities in Certain Specified\nWorks of Bach, Beethoven, and\nWagner. C. Fischer, Incorporated.\n\n\nSchaffer, Kris. 2016. “What Is Computational Musicology?”\nhttps://medium.com/@krisshaffer/what-is-computational-musicology-f25ee0a65102.\n\n\nSethares, William A. 2005. Tuning, Timbre,\nSpectrum, Scale. 2nd ed. London:\nSpringer.\n\n\nShanahan, Daniel, John Ashley Burgoyne, and Ian Quinn, eds. 2022.\nOxford Handbook of Music and Corpus\nStudies. Oxford: Oxford University Press.\n\n\nWing, Jeannette M. 2006. “Computational Thinking.”\nCommun. ACM 49 (3): 33–35. https://doi.org/10.1145/1118178.1118215.",
    "crumbs": [
      "References"
    ]
  }
]